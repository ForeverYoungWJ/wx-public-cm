(window["webpackJsonp"] = window["webpackJsonp"] || []).push([[6],{

/***/ "./node_modules/aes-decrypter/es5/aes.js":
/*!***********************************************!*\
  !*** ./node_modules/aes-decrypter/es5/aes.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file aes.js\n *\n * This file contains an adaptation of the AES decryption algorithm\n * from the Standford Javascript Cryptography Library. That work is\n * covered by the following copyright and permissions notice:\n *\n * Copyright 2009-2010 Emily Stark, Mike Hamburg, Dan Boneh.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are\n * met:\n *\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n *\n * 2. Redistributions in binary form must reproduce the above\n *    copyright notice, this list of conditions and the following\n *    disclaimer in the documentation and/or other materials provided\n *    with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR\n * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\n * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * The views and conclusions contained in the software and documentation\n * are those of the authors and should not be interpreted as representing\n * official policies, either expressed or implied, of the authors.\n */\n\n/**\n * Expand the S-box tables.\n *\n * @private\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar precompute = function precompute() {\n  var tables = [[[], [], [], [], []], [[], [], [], [], []]];\n  var encTable = tables[0];\n  var decTable = tables[1];\n  var sbox = encTable[4];\n  var sboxInv = decTable[4];\n  var i = undefined;\n  var x = undefined;\n  var xInv = undefined;\n  var d = [];\n  var th = [];\n  var x2 = undefined;\n  var x4 = undefined;\n  var x8 = undefined;\n  var s = undefined;\n  var tEnc = undefined;\n  var tDec = undefined;\n\n  // Compute double and third tables\n  for (i = 0; i < 256; i++) {\n    th[(d[i] = i << 1 ^ (i >> 7) * 283) ^ i] = i;\n  }\n\n  for (x = xInv = 0; !sbox[x]; x ^= x2 || 1, xInv = th[xInv] || 1) {\n    // Compute sbox\n    s = xInv ^ xInv << 1 ^ xInv << 2 ^ xInv << 3 ^ xInv << 4;\n    s = s >> 8 ^ s & 255 ^ 99;\n    sbox[x] = s;\n    sboxInv[s] = x;\n\n    // Compute MixColumns\n    x8 = d[x4 = d[x2 = d[x]]];\n    tDec = x8 * 0x1010101 ^ x4 * 0x10001 ^ x2 * 0x101 ^ x * 0x1010100;\n    tEnc = d[s] * 0x101 ^ s * 0x1010100;\n\n    for (i = 0; i < 4; i++) {\n      encTable[i][x] = tEnc = tEnc << 24 ^ tEnc >>> 8;\n      decTable[i][s] = tDec = tDec << 24 ^ tDec >>> 8;\n    }\n  }\n\n  // Compactify. Considerable speedup on Firefox.\n  for (i = 0; i < 5; i++) {\n    encTable[i] = encTable[i].slice(0);\n    decTable[i] = decTable[i].slice(0);\n  }\n  return tables;\n};\nvar aesTables = null;\n\n/**\n * Schedule out an AES key for both encryption and decryption. This\n * is a low-level class. Use a cipher mode to do bulk encryption.\n *\n * @class AES\n * @param key {Array} The key as an array of 4, 6 or 8 words.\n */\n\nvar AES = (function () {\n  function AES(key) {\n    _classCallCheck(this, AES);\n\n    /**\n     * The expanded S-box and inverse S-box tables. These will be computed\n     * on the client so that we don't have to send them down the wire.\n     *\n     * There are two tables, _tables[0] is for encryption and\n     * _tables[1] is for decryption.\n     *\n     * The first 4 sub-tables are the expanded S-box with MixColumns. The\n     * last (_tables[01][4]) is the S-box itself.\n     *\n     * @private\n     */\n    // if we have yet to precompute the S-box tables\n    // do so now\n    if (!aesTables) {\n      aesTables = precompute();\n    }\n    // then make a copy of that object for use\n    this._tables = [[aesTables[0][0].slice(), aesTables[0][1].slice(), aesTables[0][2].slice(), aesTables[0][3].slice(), aesTables[0][4].slice()], [aesTables[1][0].slice(), aesTables[1][1].slice(), aesTables[1][2].slice(), aesTables[1][3].slice(), aesTables[1][4].slice()]];\n    var i = undefined;\n    var j = undefined;\n    var tmp = undefined;\n    var encKey = undefined;\n    var decKey = undefined;\n    var sbox = this._tables[0][4];\n    var decTable = this._tables[1];\n    var keyLen = key.length;\n    var rcon = 1;\n\n    if (keyLen !== 4 && keyLen !== 6 && keyLen !== 8) {\n      throw new Error('Invalid aes key size');\n    }\n\n    encKey = key.slice(0);\n    decKey = [];\n    this._key = [encKey, decKey];\n\n    // schedule encryption keys\n    for (i = keyLen; i < 4 * keyLen + 28; i++) {\n      tmp = encKey[i - 1];\n\n      // apply sbox\n      if (i % keyLen === 0 || keyLen === 8 && i % keyLen === 4) {\n        tmp = sbox[tmp >>> 24] << 24 ^ sbox[tmp >> 16 & 255] << 16 ^ sbox[tmp >> 8 & 255] << 8 ^ sbox[tmp & 255];\n\n        // shift rows and add rcon\n        if (i % keyLen === 0) {\n          tmp = tmp << 8 ^ tmp >>> 24 ^ rcon << 24;\n          rcon = rcon << 1 ^ (rcon >> 7) * 283;\n        }\n      }\n\n      encKey[i] = encKey[i - keyLen] ^ tmp;\n    }\n\n    // schedule decryption keys\n    for (j = 0; i; j++, i--) {\n      tmp = encKey[j & 3 ? i : i - 4];\n      if (i <= 4 || j < 4) {\n        decKey[j] = tmp;\n      } else {\n        decKey[j] = decTable[0][sbox[tmp >>> 24]] ^ decTable[1][sbox[tmp >> 16 & 255]] ^ decTable[2][sbox[tmp >> 8 & 255]] ^ decTable[3][sbox[tmp & 255]];\n      }\n    }\n  }\n\n  /**\n   * Decrypt 16 bytes, specified as four 32-bit words.\n   *\n   * @param {Number} encrypted0 the first word to decrypt\n   * @param {Number} encrypted1 the second word to decrypt\n   * @param {Number} encrypted2 the third word to decrypt\n   * @param {Number} encrypted3 the fourth word to decrypt\n   * @param {Int32Array} out the array to write the decrypted words\n   * into\n   * @param {Number} offset the offset into the output array to start\n   * writing results\n   * @return {Array} The plaintext.\n   */\n\n  _createClass(AES, [{\n    key: 'decrypt',\n    value: function decrypt(encrypted0, encrypted1, encrypted2, encrypted3, out, offset) {\n      var key = this._key[1];\n      // state variables a,b,c,d are loaded with pre-whitened data\n      var a = encrypted0 ^ key[0];\n      var b = encrypted3 ^ key[1];\n      var c = encrypted2 ^ key[2];\n      var d = encrypted1 ^ key[3];\n      var a2 = undefined;\n      var b2 = undefined;\n      var c2 = undefined;\n\n      // key.length === 2 ?\n      var nInnerRounds = key.length / 4 - 2;\n      var i = undefined;\n      var kIndex = 4;\n      var table = this._tables[1];\n\n      // load up the tables\n      var table0 = table[0];\n      var table1 = table[1];\n      var table2 = table[2];\n      var table3 = table[3];\n      var sbox = table[4];\n\n      // Inner rounds. Cribbed from OpenSSL.\n      for (i = 0; i < nInnerRounds; i++) {\n        a2 = table0[a >>> 24] ^ table1[b >> 16 & 255] ^ table2[c >> 8 & 255] ^ table3[d & 255] ^ key[kIndex];\n        b2 = table0[b >>> 24] ^ table1[c >> 16 & 255] ^ table2[d >> 8 & 255] ^ table3[a & 255] ^ key[kIndex + 1];\n        c2 = table0[c >>> 24] ^ table1[d >> 16 & 255] ^ table2[a >> 8 & 255] ^ table3[b & 255] ^ key[kIndex + 2];\n        d = table0[d >>> 24] ^ table1[a >> 16 & 255] ^ table2[b >> 8 & 255] ^ table3[c & 255] ^ key[kIndex + 3];\n        kIndex += 4;\n        a = a2;b = b2;c = c2;\n      }\n\n      // Last round.\n      for (i = 0; i < 4; i++) {\n        out[(3 & -i) + offset] = sbox[a >>> 24] << 24 ^ sbox[b >> 16 & 255] << 16 ^ sbox[c >> 8 & 255] << 8 ^ sbox[d & 255] ^ key[kIndex++];\n        a2 = a;a = b;b = c;c = d;d = a2;\n      }\n    }\n  }]);\n\n  return AES;\n})();\n\nexports['default'] = AES;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/aes-decrypter/es5/aes.js?");

/***/ }),

/***/ "./node_modules/aes-decrypter/es5/async-stream.js":
/*!********************************************************!*\
  !*** ./node_modules/aes-decrypter/es5/async-stream.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file async-stream.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _stream = __webpack_require__(/*! ./stream */ \"./node_modules/aes-decrypter/es5/stream.js\");\n\nvar _stream2 = _interopRequireDefault(_stream);\n\n/**\n * A wrapper around the Stream class to use setTiemout\n * and run stream \"jobs\" Asynchronously\n *\n * @class AsyncStream\n * @extends Stream\n */\n\nvar AsyncStream = (function (_Stream) {\n  _inherits(AsyncStream, _Stream);\n\n  function AsyncStream() {\n    _classCallCheck(this, AsyncStream);\n\n    _get(Object.getPrototypeOf(AsyncStream.prototype), 'constructor', this).call(this, _stream2['default']);\n    this.jobs = [];\n    this.delay = 1;\n    this.timeout_ = null;\n  }\n\n  /**\n   * process an async job\n   *\n   * @private\n   */\n\n  _createClass(AsyncStream, [{\n    key: 'processJob_',\n    value: function processJob_() {\n      this.jobs.shift()();\n      if (this.jobs.length) {\n        this.timeout_ = setTimeout(this.processJob_.bind(this), this.delay);\n      } else {\n        this.timeout_ = null;\n      }\n    }\n\n    /**\n     * push a job into the stream\n     *\n     * @param {Function} job the job to push into the stream\n     */\n  }, {\n    key: 'push',\n    value: function push(job) {\n      this.jobs.push(job);\n      if (!this.timeout_) {\n        this.timeout_ = setTimeout(this.processJob_.bind(this), this.delay);\n      }\n    }\n  }]);\n\n  return AsyncStream;\n})(_stream2['default']);\n\nexports['default'] = AsyncStream;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/aes-decrypter/es5/async-stream.js?");

/***/ }),

/***/ "./node_modules/aes-decrypter/es5/decrypter.js":
/*!*****************************************************!*\
  !*** ./node_modules/aes-decrypter/es5/decrypter.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file decrypter.js\n *\n * An asynchronous implementation of AES-128 CBC decryption with\n * PKCS#7 padding.\n */\n\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar _aes = __webpack_require__(/*! ./aes */ \"./node_modules/aes-decrypter/es5/aes.js\");\n\nvar _aes2 = _interopRequireDefault(_aes);\n\nvar _asyncStream = __webpack_require__(/*! ./async-stream */ \"./node_modules/aes-decrypter/es5/async-stream.js\");\n\nvar _asyncStream2 = _interopRequireDefault(_asyncStream);\n\nvar _pkcs7 = __webpack_require__(/*! pkcs7 */ \"./node_modules/pkcs7/lib/pkcs7.js\");\n\n/**\n * Convert network-order (big-endian) bytes into their little-endian\n * representation.\n */\nvar ntoh = function ntoh(word) {\n  return word << 24 | (word & 0xff00) << 8 | (word & 0xff0000) >> 8 | word >>> 24;\n};\n\n/**\n * Decrypt bytes using AES-128 with CBC and PKCS#7 padding.\n *\n * @param {Uint8Array} encrypted the encrypted bytes\n * @param {Uint32Array} key the bytes of the decryption key\n * @param {Uint32Array} initVector the initialization vector (IV) to\n * use for the first round of CBC.\n * @return {Uint8Array} the decrypted bytes\n *\n * @see http://en.wikipedia.org/wiki/Advanced_Encryption_Standard\n * @see http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Cipher_Block_Chaining_.28CBC.29\n * @see https://tools.ietf.org/html/rfc2315\n */\nvar decrypt = function decrypt(encrypted, key, initVector) {\n  // word-level access to the encrypted bytes\n  var encrypted32 = new Int32Array(encrypted.buffer, encrypted.byteOffset, encrypted.byteLength >> 2);\n\n  var decipher = new _aes2['default'](Array.prototype.slice.call(key));\n\n  // byte and word-level access for the decrypted output\n  var decrypted = new Uint8Array(encrypted.byteLength);\n  var decrypted32 = new Int32Array(decrypted.buffer);\n\n  // temporary variables for working with the IV, encrypted, and\n  // decrypted data\n  var init0 = undefined;\n  var init1 = undefined;\n  var init2 = undefined;\n  var init3 = undefined;\n  var encrypted0 = undefined;\n  var encrypted1 = undefined;\n  var encrypted2 = undefined;\n  var encrypted3 = undefined;\n\n  // iteration variable\n  var wordIx = undefined;\n\n  // pull out the words of the IV to ensure we don't modify the\n  // passed-in reference and easier access\n  init0 = initVector[0];\n  init1 = initVector[1];\n  init2 = initVector[2];\n  init3 = initVector[3];\n\n  // decrypt four word sequences, applying cipher-block chaining (CBC)\n  // to each decrypted block\n  for (wordIx = 0; wordIx < encrypted32.length; wordIx += 4) {\n    // convert big-endian (network order) words into little-endian\n    // (javascript order)\n    encrypted0 = ntoh(encrypted32[wordIx]);\n    encrypted1 = ntoh(encrypted32[wordIx + 1]);\n    encrypted2 = ntoh(encrypted32[wordIx + 2]);\n    encrypted3 = ntoh(encrypted32[wordIx + 3]);\n\n    // decrypt the block\n    decipher.decrypt(encrypted0, encrypted1, encrypted2, encrypted3, decrypted32, wordIx);\n\n    // XOR with the IV, and restore network byte-order to obtain the\n    // plaintext\n    decrypted32[wordIx] = ntoh(decrypted32[wordIx] ^ init0);\n    decrypted32[wordIx + 1] = ntoh(decrypted32[wordIx + 1] ^ init1);\n    decrypted32[wordIx + 2] = ntoh(decrypted32[wordIx + 2] ^ init2);\n    decrypted32[wordIx + 3] = ntoh(decrypted32[wordIx + 3] ^ init3);\n\n    // setup the IV for the next round\n    init0 = encrypted0;\n    init1 = encrypted1;\n    init2 = encrypted2;\n    init3 = encrypted3;\n  }\n\n  return decrypted;\n};\n\nexports.decrypt = decrypt;\n/**\n * The `Decrypter` class that manages decryption of AES\n * data through `AsyncStream` objects and the `decrypt`\n * function\n *\n * @param {Uint8Array} encrypted the encrypted bytes\n * @param {Uint32Array} key the bytes of the decryption key\n * @param {Uint32Array} initVector the initialization vector (IV) to\n * @param {Function} done the function to run when done\n * @class Decrypter\n */\n\nvar Decrypter = (function () {\n  function Decrypter(encrypted, key, initVector, done) {\n    _classCallCheck(this, Decrypter);\n\n    var step = Decrypter.STEP;\n    var encrypted32 = new Int32Array(encrypted.buffer);\n    var decrypted = new Uint8Array(encrypted.byteLength);\n    var i = 0;\n\n    this.asyncStream_ = new _asyncStream2['default']();\n\n    // split up the encryption job and do the individual chunks asynchronously\n    this.asyncStream_.push(this.decryptChunk_(encrypted32.subarray(i, i + step), key, initVector, decrypted));\n    for (i = step; i < encrypted32.length; i += step) {\n      initVector = new Uint32Array([ntoh(encrypted32[i - 4]), ntoh(encrypted32[i - 3]), ntoh(encrypted32[i - 2]), ntoh(encrypted32[i - 1])]);\n      this.asyncStream_.push(this.decryptChunk_(encrypted32.subarray(i, i + step), key, initVector, decrypted));\n    }\n    // invoke the done() callback when everything is finished\n    this.asyncStream_.push(function () {\n      // remove pkcs#7 padding from the decrypted bytes\n      done(null, (0, _pkcs7.unpad)(decrypted));\n    });\n  }\n\n  /**\n   * a getter for step the maximum number of bytes to process at one time\n   *\n   * @return {Number} the value of step 32000\n   */\n\n  _createClass(Decrypter, [{\n    key: 'decryptChunk_',\n\n    /**\n     * @private\n     */\n    value: function decryptChunk_(encrypted, key, initVector, decrypted) {\n      return function () {\n        var bytes = decrypt(encrypted, key, initVector);\n\n        decrypted.set(bytes, encrypted.byteOffset);\n      };\n    }\n  }], [{\n    key: 'STEP',\n    get: function get() {\n      // 4 * 8000;\n      return 32000;\n    }\n  }]);\n\n  return Decrypter;\n})();\n\nexports.Decrypter = Decrypter;\nexports['default'] = {\n  Decrypter: Decrypter,\n  decrypt: decrypt\n};\n\n//# sourceURL=webpack:///./node_modules/aes-decrypter/es5/decrypter.js?");

/***/ }),

/***/ "./node_modules/aes-decrypter/es5/index.js":
/*!*************************************************!*\
  !*** ./node_modules/aes-decrypter/es5/index.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file index.js\n *\n * Index module to easily import the primary components of AES-128\n * decryption. Like this:\n *\n * ```js\n * import {Decrypter, decrypt, AsyncStream} from 'aes-decrypter';\n * ```\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _decrypter = __webpack_require__(/*! ./decrypter */ \"./node_modules/aes-decrypter/es5/decrypter.js\");\n\nvar _asyncStream = __webpack_require__(/*! ./async-stream */ \"./node_modules/aes-decrypter/es5/async-stream.js\");\n\nvar _asyncStream2 = _interopRequireDefault(_asyncStream);\n\nexports['default'] = {\n  decrypt: _decrypter.decrypt,\n  Decrypter: _decrypter.Decrypter,\n  AsyncStream: _asyncStream2['default']\n};\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/aes-decrypter/es5/index.js?");

/***/ }),

/***/ "./node_modules/aes-decrypter/es5/stream.js":
/*!**************************************************!*\
  !*** ./node_modules/aes-decrypter/es5/stream.js ***!
  \**************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file stream.js\n */\n/**\n * A lightweight readable stream implemention that handles event dispatching.\n *\n * @class Stream\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar Stream = (function () {\n  function Stream() {\n    _classCallCheck(this, Stream);\n\n    this.listeners = {};\n  }\n\n  /**\n   * Add a listener for a specified event type.\n   *\n   * @param {String} type the event name\n   * @param {Function} listener the callback to be invoked when an event of\n   * the specified type occurs\n   */\n\n  _createClass(Stream, [{\n    key: 'on',\n    value: function on(type, listener) {\n      if (!this.listeners[type]) {\n        this.listeners[type] = [];\n      }\n      this.listeners[type].push(listener);\n    }\n\n    /**\n     * Remove a listener for a specified event type.\n     *\n     * @param {String} type the event name\n     * @param {Function} listener  a function previously registered for this\n     * type of event through `on`\n     * @return {Boolean} if we could turn it off or not\n     */\n  }, {\n    key: 'off',\n    value: function off(type, listener) {\n      var index = undefined;\n\n      if (!this.listeners[type]) {\n        return false;\n      }\n      index = this.listeners[type].indexOf(listener);\n      this.listeners[type].splice(index, 1);\n      return index > -1;\n    }\n\n    /**\n     * Trigger an event of the specified type on this stream. Any additional\n     * arguments to this function are passed as parameters to event listeners.\n     *\n     * @param {String} type the event name\n     */\n  }, {\n    key: 'trigger',\n    value: function trigger(type) {\n      var callbacks = undefined;\n      var i = undefined;\n      var length = undefined;\n      var args = undefined;\n\n      callbacks = this.listeners[type];\n      if (!callbacks) {\n        return;\n      }\n      // Slicing the arguments on every invocation of this method\n      // can add a significant amount of overhead. Avoid the\n      // intermediate object creation for the common case of a\n      // single callback argument\n      if (arguments.length === 2) {\n        length = callbacks.length;\n        for (i = 0; i < length; ++i) {\n          callbacks[i].call(this, arguments[1]);\n        }\n      } else {\n        args = Array.prototype.slice.call(arguments, 1);\n        length = callbacks.length;\n        for (i = 0; i < length; ++i) {\n          callbacks[i].apply(this, args);\n        }\n      }\n    }\n\n    /**\n     * Destroys the stream and cleans up.\n     */\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      this.listeners = {};\n    }\n\n    /**\n     * Forwards all `data` events on this stream to the destination stream. The\n     * destination stream should provide a method `push` to receive the data\n     * events as they arrive.\n     *\n     * @param {Stream} destination the stream that will receive all `data` events\n     * @see http://nodejs.org/api/stream.html#stream_readable_pipe_destination_options\n     */\n  }, {\n    key: 'pipe',\n    value: function pipe(destination) {\n      this.on('data', function (data) {\n        destination.push(data);\n      });\n    }\n  }]);\n\n  return Stream;\n})();\n\nexports['default'] = Stream;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/aes-decrypter/es5/stream.js?");

/***/ }),

/***/ "./node_modules/cache-loader/dist/cjs.js?!./node_modules/babel-loader/lib/index.js!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=script&lang=js&":
/*!********************************************************************************************************************************************************************************************************************************************!*\
  !*** ./node_modules/cache-loader/dist/cjs.js??ref--12-0!./node_modules/babel-loader/lib!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options!./src/views/DKZb.vue?vue&type=script&lang=js& ***!
  \********************************************************************************************************************************************************************************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var videojs_contrib_hls__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! videojs-contrib-hls */ \"./node_modules/videojs-contrib-hls/es5/videojs-contrib-hls.js\");\n/* harmony import */ var videojs_contrib_hls__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(videojs_contrib_hls__WEBPACK_IMPORTED_MODULE_0__);\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n //推流 2020-2-13\n//rtmp://push.legions.tech/live/dangke?txSecret=ca31bc33bda3ff27a4ad9f61c6298736&txTime=5E45727F\n\n/* harmony default export */ __webpack_exports__[\"default\"] = ({\n  name: 'DkZb',\n  //组件\n  components: {},\n  //初始数据\n  data: function data() {\n    return {\n      playerOptions: {\n        //playbackRates: [0.7, 1.0, 1.5, 2.0], //播放速度\n        autoplay: false,\n        //如果true,浏览器准备好时开始回放。\n        muted: false,\n        // 默认情况下将会消除任何音频。\n        loop: false,\n        // 导致视频一结束就重新开始。\n        preload: 'auto',\n        // 建议浏览器在<video>加载元素后是否应该开始下载视频数据。auto浏览器选择最佳行为,立即开始加载视频（如果浏览器支持）\n        language: 'zh-CN',\n        aspectRatio: '16:9',\n        // 将播放器置于流畅模式，并在计算播放器的动态大小时使用该值。值应该代表一个比例 - 用冒号分隔的两个数字（例如\"16:9\"或\"4:3\"）\n        fluid: true,\n        // 当true时，Video.js player将拥有流体大小。换句话说，它将按比例缩放以适应其容器。\n        sources: [],\n        poster: \"\",\n        //你的封面地址 poster.jpg\n        width: document.documentElement.clientWidth,\n        notSupportedMessage: '此视频暂无法播放，请稍后再试' //允许覆盖Video.js无法播放媒体源时显示的默认信息。\n        //  controlBar: {\n        //   timeDivider: true,\n        //   durationDisplay: true,\n        //   remainingTimeDisplay: false,\n        //   fullscreenToggle: true //全屏按钮\n        //  }\n\n      }\n    };\n  },\n  //创建后\n  created: function created() {\n    var Item = {};\n    Item.type = \"application/x-mpegURL\";\n    Item.src = 'http://play.legions.tech/live/dangke.m3u8';\n    this.playerOptions.sources.push(Item);\n  },\n  //调用的方法\n  methods: {}\n});\n\n//# sourceURL=webpack:///./src/views/DKZb.vue?./node_modules/cache-loader/dist/cjs.js??ref--12-0!./node_modules/babel-loader/lib!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options");

/***/ }),

/***/ "./node_modules/cache-loader/dist/cjs.js?{\"cacheDirectory\":\"node_modules/.cache/vue-loader\",\"cacheIdentifier\":\"74741eaf-vue-loader-template\"}!./node_modules/vue-loader/lib/loaders/templateLoader.js?!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=template&id=5211bda9&scoped=true&":
/*!****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************!*\
  !*** ./node_modules/cache-loader/dist/cjs.js?{"cacheDirectory":"node_modules/.cache/vue-loader","cacheIdentifier":"74741eaf-vue-loader-template"}!./node_modules/vue-loader/lib/loaders/templateLoader.js??vue-loader-options!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options!./src/views/DKZb.vue?vue&type=template&id=5211bda9&scoped=true& ***!
  \****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************/
/*! exports provided: render, staticRenderFns */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"render\", function() { return render; });\n/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, \"staticRenderFns\", function() { return staticRenderFns; });\nvar render = function() {\n  var _vm = this\n  var _h = _vm.$createElement\n  var _c = _vm._self._c || _h\n  return _c(\n    \"div\",\n    { staticClass: \"DkZb\" },\n    [\n      _c(\"video-player\", {\n        ref: \"videoPlayer\",\n        staticClass: \"video-player vjs-custom-skin\",\n        attrs: { playsinline: true, options: _vm.playerOptions }\n      })\n    ],\n    1\n  )\n}\nvar staticRenderFns = []\nrender._withStripped = true\n\n\n\n//# sourceURL=webpack:///./src/views/DKZb.vue?./node_modules/cache-loader/dist/cjs.js?%7B%22cacheDirectory%22:%22node_modules/.cache/vue-loader%22,%22cacheIdentifier%22:%2274741eaf-vue-loader-template%22%7D!./node_modules/vue-loader/lib/loaders/templateLoader.js??vue-loader-options!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options");

/***/ }),

/***/ "./node_modules/css-loader/dist/cjs.js?!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src/index.js?!./node_modules/sass-loader/dist/cjs.js?!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true&":
/*!************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************!*\
  !*** ./node_modules/css-loader/dist/cjs.js??ref--8-oneOf-1-1!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src??ref--8-oneOf-1-2!./node_modules/sass-loader/dist/cjs.js??ref--8-oneOf-1-3!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options!./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true& ***!
  \************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("exports = module.exports = __webpack_require__(/*! ../../node_modules/css-loader/dist/runtime/api.js */ \"./node_modules/css-loader/dist/runtime/api.js\")(false);\n// Module\nexports.push([module.i, \".DkZb[data-v-5211bda9] {\\n  height: 100%;\\n  background-color: #fcf8ee;\\n}\", \"\"]);\n\n\n//# sourceURL=webpack:///./src/views/DKZb.vue?./node_modules/css-loader/dist/cjs.js??ref--8-oneOf-1-1!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src??ref--8-oneOf-1-2!./node_modules/sass-loader/dist/cjs.js??ref--8-oneOf-1-3!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options");

/***/ }),

/***/ "./node_modules/m3u8-parser/es5/index.js":
/*!***********************************************!*\
  !*** ./node_modules/m3u8-parser/es5/index.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar _lineStream = __webpack_require__(/*! ./line-stream */ \"./node_modules/m3u8-parser/es5/line-stream.js\");\n\nvar _lineStream2 = _interopRequireDefault(_lineStream);\n\nvar _parseStream = __webpack_require__(/*! ./parse-stream */ \"./node_modules/m3u8-parser/es5/parse-stream.js\");\n\nvar _parseStream2 = _interopRequireDefault(_parseStream);\n\nvar _parser = __webpack_require__(/*! ./parser */ \"./node_modules/m3u8-parser/es5/parser.js\");\n\nvar _parser2 = _interopRequireDefault(_parser);\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nmodule.exports = {\n  LineStream: _lineStream2['default'],\n  ParseStream: _parseStream2['default'],\n  Parser: _parser2['default']\n}; /**\n    * @file m3u8/index.js\n    *\n    * Utilities for parsing M3U8 files. If the entire manifest is available,\n    * `Parser` will create an object representation with enough detail for managing\n    * playback. `ParseStream` and `LineStream` are lower-level parsing primitives\n    * that do not assume the entirety of the manifest is ready and expose a\n    * ReadableStream-like interface.\n    */\n\n//# sourceURL=webpack:///./node_modules/m3u8-parser/es5/index.js?");

/***/ }),

/***/ "./node_modules/m3u8-parser/es5/line-stream.js":
/*!*****************************************************!*\
  !*** ./node_modules/m3u8-parser/es5/line-stream.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (\"value\" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();\n\nvar _stream = __webpack_require__(/*! ./stream */ \"./node_modules/m3u8-parser/es5/stream.js\");\n\nvar _stream2 = _interopRequireDefault(_stream);\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(\"Cannot call a class as a function\"); } }\n\nfunction _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\"); } return call && (typeof call === \"object\" || typeof call === \"function\") ? call : self; }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== \"function\" && superClass !== null) { throw new TypeError(\"Super expression must either be null or a function, not \" + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; } /**\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                * @file m3u8/line-stream.js\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                */\n\n\n/**\n * A stream that buffers string input and generates a `data` event for each\n * line.\n *\n * @class LineStream\n * @extends Stream\n */\nvar LineStream = function (_Stream) {\n  _inherits(LineStream, _Stream);\n\n  function LineStream() {\n    _classCallCheck(this, LineStream);\n\n    var _this = _possibleConstructorReturn(this, (LineStream.__proto__ || Object.getPrototypeOf(LineStream)).call(this));\n\n    _this.buffer = '';\n    return _this;\n  }\n\n  /**\n   * Add new data to be parsed.\n   *\n   * @param {String} data the text to process\n   */\n\n\n  _createClass(LineStream, [{\n    key: 'push',\n    value: function push(data) {\n      var nextNewline = void 0;\n\n      this.buffer += data;\n      nextNewline = this.buffer.indexOf('\\n');\n\n      for (; nextNewline > -1; nextNewline = this.buffer.indexOf('\\n')) {\n        this.trigger('data', this.buffer.substring(0, nextNewline));\n        this.buffer = this.buffer.substring(nextNewline + 1);\n      }\n    }\n  }]);\n\n  return LineStream;\n}(_stream2['default']);\n\nexports['default'] = LineStream;\n\n//# sourceURL=webpack:///./node_modules/m3u8-parser/es5/line-stream.js?");

/***/ }),

/***/ "./node_modules/m3u8-parser/es5/parse-stream.js":
/*!******************************************************!*\
  !*** ./node_modules/m3u8-parser/es5/parse-stream.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _slicedToArray = function () { function sliceIterator(arr, i) { var _arr = []; var _n = true; var _d = false; var _e = undefined; try { for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i[\"return\"]) _i[\"return\"](); } finally { if (_d) throw _e; } } return _arr; } return function (arr, i) { if (Array.isArray(arr)) { return arr; } else if (Symbol.iterator in Object(arr)) { return sliceIterator(arr, i); } else { throw new TypeError(\"Invalid attempt to destructure non-iterable instance\"); } }; }();\n\nvar _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (\"value\" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();\n\nvar _stream = __webpack_require__(/*! ./stream */ \"./node_modules/m3u8-parser/es5/stream.js\");\n\nvar _stream2 = _interopRequireDefault(_stream);\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(\"Cannot call a class as a function\"); } }\n\nfunction _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\"); } return call && (typeof call === \"object\" || typeof call === \"function\") ? call : self; }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== \"function\" && superClass !== null) { throw new TypeError(\"Super expression must either be null or a function, not \" + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; } /**\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                * @file m3u8/parse-stream.js\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                */\n\n\n/**\n * \"forgiving\" attribute list psuedo-grammar:\n * attributes -> keyvalue (',' keyvalue)*\n * keyvalue   -> key '=' value\n * key        -> [^=]*\n * value      -> '\"' [^\"]* '\"' | [^,]*\n */\nvar attributeSeparator = function attributeSeparator() {\n  var key = '[^=]*';\n  var value = '\"[^\"]*\"|[^,]*';\n  var keyvalue = '(?:' + key + ')=(?:' + value + ')';\n\n  return new RegExp('(?:^|,)(' + keyvalue + ')');\n};\n\n/**\n * Parse attributes from a line given the seperator\n *\n * @param {String} attributes the attibute line to parse\n */\nvar parseAttributes = function parseAttributes(attributes) {\n  // split the string using attributes as the separator\n  var attrs = attributes.split(attributeSeparator());\n  var result = {};\n  var i = attrs.length;\n  var attr = void 0;\n\n  while (i--) {\n    // filter out unmatched portions of the string\n    if (attrs[i] === '') {\n      continue;\n    }\n\n    // split the key and value\n    attr = /([^=]*)=(.*)/.exec(attrs[i]).slice(1);\n    // trim whitespace and remove optional quotes around the value\n    attr[0] = attr[0].replace(/^\\s+|\\s+$/g, '');\n    attr[1] = attr[1].replace(/^\\s+|\\s+$/g, '');\n    attr[1] = attr[1].replace(/^['\"](.*)['\"]$/g, '$1');\n    result[attr[0]] = attr[1];\n  }\n  return result;\n};\n\n/**\n * A line-level M3U8 parser event stream. It expects to receive input one\n * line at a time and performs a context-free parse of its contents. A stream\n * interpretation of a manifest can be useful if the manifest is expected to\n * be too large to fit comfortably into memory or the entirety of the input\n * is not immediately available. Otherwise, it's probably much easier to work\n * with a regular `Parser` object.\n *\n * Produces `data` events with an object that captures the parser's\n * interpretation of the input. That object has a property `tag` that is one\n * of `uri`, `comment`, or `tag`. URIs only have a single additional\n * property, `line`, which captures the entirety of the input without\n * interpretation. Comments similarly have a single additional property\n * `text` which is the input without the leading `#`.\n *\n * Tags always have a property `tagType` which is the lower-cased version of\n * the M3U8 directive without the `#EXT` or `#EXT-X-` prefix. For instance,\n * `#EXT-X-MEDIA-SEQUENCE` becomes `media-sequence` when parsed. Unrecognized\n * tags are given the tag type `unknown` and a single additional property\n * `data` with the remainder of the input.\n *\n * @class ParseStream\n * @extends Stream\n */\n\nvar ParseStream = function (_Stream) {\n  _inherits(ParseStream, _Stream);\n\n  function ParseStream() {\n    _classCallCheck(this, ParseStream);\n\n    return _possibleConstructorReturn(this, (ParseStream.__proto__ || Object.getPrototypeOf(ParseStream)).call(this));\n  }\n\n  /**\n   * Parses an additional line of input.\n   *\n   * @param {String} line a single line of an M3U8 file to parse\n   */\n\n\n  _createClass(ParseStream, [{\n    key: 'push',\n    value: function push(line) {\n      var match = void 0;\n      var event = void 0;\n\n      // strip whitespace\n      line = line.replace(/^[\\u0000\\s]+|[\\u0000\\s]+$/g, '');\n      if (line.length === 0) {\n        // ignore empty lines\n        return;\n      }\n\n      // URIs\n      if (line[0] !== '#') {\n        this.trigger('data', {\n          type: 'uri',\n          uri: line\n        });\n        return;\n      }\n\n      // Comments\n      if (line.indexOf('#EXT') !== 0) {\n        this.trigger('data', {\n          type: 'comment',\n          text: line.slice(1)\n        });\n        return;\n      }\n\n      // strip off any carriage returns here so the regex matching\n      // doesn't have to account for them.\n      line = line.replace('\\r', '');\n\n      // Tags\n      match = /^#EXTM3U/.exec(line);\n      if (match) {\n        this.trigger('data', {\n          type: 'tag',\n          tagType: 'm3u'\n        });\n        return;\n      }\n      match = /^#EXTINF:?([0-9\\.]*)?,?(.*)?$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'inf'\n        };\n        if (match[1]) {\n          event.duration = parseFloat(match[1]);\n        }\n        if (match[2]) {\n          event.title = match[2];\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-TARGETDURATION:?([0-9.]*)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'targetduration'\n        };\n        if (match[1]) {\n          event.duration = parseInt(match[1], 10);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#ZEN-TOTAL-DURATION:?([0-9.]*)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'totalduration'\n        };\n        if (match[1]) {\n          event.duration = parseInt(match[1], 10);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-VERSION:?([0-9.]*)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'version'\n        };\n        if (match[1]) {\n          event.version = parseInt(match[1], 10);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-MEDIA-SEQUENCE:?(\\-?[0-9.]*)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'media-sequence'\n        };\n        if (match[1]) {\n          event.number = parseInt(match[1], 10);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-DISCONTINUITY-SEQUENCE:?(\\-?[0-9.]*)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'discontinuity-sequence'\n        };\n        if (match[1]) {\n          event.number = parseInt(match[1], 10);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-PLAYLIST-TYPE:?(.*)?$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'playlist-type'\n        };\n        if (match[1]) {\n          event.playlistType = match[1];\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-BYTERANGE:?([0-9.]*)?@?([0-9.]*)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'byterange'\n        };\n        if (match[1]) {\n          event.length = parseInt(match[1], 10);\n        }\n        if (match[2]) {\n          event.offset = parseInt(match[2], 10);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-ALLOW-CACHE:?(YES|NO)?/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'allow-cache'\n        };\n        if (match[1]) {\n          event.allowed = !/NO/.test(match[1]);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-MAP:?(.*)$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'map'\n        };\n\n        if (match[1]) {\n          var attributes = parseAttributes(match[1]);\n\n          if (attributes.URI) {\n            event.uri = attributes.URI;\n          }\n          if (attributes.BYTERANGE) {\n            var _attributes$BYTERANGE = attributes.BYTERANGE.split('@'),\n                _attributes$BYTERANGE2 = _slicedToArray(_attributes$BYTERANGE, 2),\n                length = _attributes$BYTERANGE2[0],\n                offset = _attributes$BYTERANGE2[1];\n\n            event.byterange = {};\n            if (length) {\n              event.byterange.length = parseInt(length, 10);\n            }\n            if (offset) {\n              event.byterange.offset = parseInt(offset, 10);\n            }\n          }\n        }\n\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-STREAM-INF:?(.*)$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'stream-inf'\n        };\n        if (match[1]) {\n          event.attributes = parseAttributes(match[1]);\n\n          if (event.attributes.RESOLUTION) {\n            var split = event.attributes.RESOLUTION.split('x');\n            var resolution = {};\n\n            if (split[0]) {\n              resolution.width = parseInt(split[0], 10);\n            }\n            if (split[1]) {\n              resolution.height = parseInt(split[1], 10);\n            }\n            event.attributes.RESOLUTION = resolution;\n          }\n          if (event.attributes.BANDWIDTH) {\n            event.attributes.BANDWIDTH = parseInt(event.attributes.BANDWIDTH, 10);\n          }\n          if (event.attributes['PROGRAM-ID']) {\n            event.attributes['PROGRAM-ID'] = parseInt(event.attributes['PROGRAM-ID'], 10);\n          }\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-MEDIA:?(.*)$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'media'\n        };\n        if (match[1]) {\n          event.attributes = parseAttributes(match[1]);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-ENDLIST/.exec(line);\n      if (match) {\n        this.trigger('data', {\n          type: 'tag',\n          tagType: 'endlist'\n        });\n        return;\n      }\n      match = /^#EXT-X-DISCONTINUITY/.exec(line);\n      if (match) {\n        this.trigger('data', {\n          type: 'tag',\n          tagType: 'discontinuity'\n        });\n        return;\n      }\n      match = /^#EXT-X-PROGRAM-DATE-TIME:?(.*)$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'program-date-time'\n        };\n        if (match[1]) {\n          event.dateTimeString = match[1];\n          event.dateTimeObject = new Date(match[1]);\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-KEY:?(.*)$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'key'\n        };\n        if (match[1]) {\n          event.attributes = parseAttributes(match[1]);\n          // parse the IV string into a Uint32Array\n          if (event.attributes.IV) {\n            if (event.attributes.IV.substring(0, 2).toLowerCase() === '0x') {\n              event.attributes.IV = event.attributes.IV.substring(2);\n            }\n\n            event.attributes.IV = event.attributes.IV.match(/.{8}/g);\n            event.attributes.IV[0] = parseInt(event.attributes.IV[0], 16);\n            event.attributes.IV[1] = parseInt(event.attributes.IV[1], 16);\n            event.attributes.IV[2] = parseInt(event.attributes.IV[2], 16);\n            event.attributes.IV[3] = parseInt(event.attributes.IV[3], 16);\n            event.attributes.IV = new Uint32Array(event.attributes.IV);\n          }\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-CUE-OUT-CONT:?(.*)?$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'cue-out-cont'\n        };\n        if (match[1]) {\n          event.data = match[1];\n        } else {\n          event.data = '';\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-CUE-OUT:?(.*)?$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'cue-out'\n        };\n        if (match[1]) {\n          event.data = match[1];\n        } else {\n          event.data = '';\n        }\n        this.trigger('data', event);\n        return;\n      }\n      match = /^#EXT-X-CUE-IN:?(.*)?$/.exec(line);\n      if (match) {\n        event = {\n          type: 'tag',\n          tagType: 'cue-in'\n        };\n        if (match[1]) {\n          event.data = match[1];\n        } else {\n          event.data = '';\n        }\n        this.trigger('data', event);\n        return;\n      }\n\n      // unknown tag type\n      this.trigger('data', {\n        type: 'tag',\n        data: line.slice(4)\n      });\n    }\n  }]);\n\n  return ParseStream;\n}(_stream2['default']);\n\nexports['default'] = ParseStream;\n\n//# sourceURL=webpack:///./node_modules/m3u8-parser/es5/parse-stream.js?");

/***/ }),

/***/ "./node_modules/m3u8-parser/es5/parser.js":
/*!************************************************!*\
  !*** ./node_modules/m3u8-parser/es5/parser.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; };\n\nvar _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (\"value\" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();\n\nvar _stream = __webpack_require__(/*! ./stream */ \"./node_modules/m3u8-parser/es5/stream.js\");\n\nvar _stream2 = _interopRequireDefault(_stream);\n\nvar _lineStream = __webpack_require__(/*! ./line-stream */ \"./node_modules/m3u8-parser/es5/line-stream.js\");\n\nvar _lineStream2 = _interopRequireDefault(_lineStream);\n\nvar _parseStream = __webpack_require__(/*! ./parse-stream */ \"./node_modules/m3u8-parser/es5/parse-stream.js\");\n\nvar _parseStream2 = _interopRequireDefault(_parseStream);\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(\"Cannot call a class as a function\"); } }\n\nfunction _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\"); } return call && (typeof call === \"object\" || typeof call === \"function\") ? call : self; }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== \"function\" && superClass !== null) { throw new TypeError(\"Super expression must either be null or a function, not \" + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; } /**\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                * @file m3u8/parser.js\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                */\n\n\n/**\n * A parser for M3U8 files. The current interpretation of the input is\n * exposed as a property `manifest` on parser objects. It's just two lines to\n * create and parse a manifest once you have the contents available as a string:\n *\n * ```js\n * var parser = new m3u8.Parser();\n * parser.push(xhr.responseText);\n * ```\n *\n * New input can later be applied to update the manifest object by calling\n * `push` again.\n *\n * The parser attempts to create a usable manifest object even if the\n * underlying input is somewhat nonsensical. It emits `info` and `warning`\n * events during the parse if it encounters input that seems invalid or\n * requires some property of the manifest object to be defaulted.\n *\n * @class Parser\n * @extends Stream\n */\nvar Parser = function (_Stream) {\n  _inherits(Parser, _Stream);\n\n  function Parser() {\n    _classCallCheck(this, Parser);\n\n    var _this = _possibleConstructorReturn(this, (Parser.__proto__ || Object.getPrototypeOf(Parser)).call(this));\n\n    _this.lineStream = new _lineStream2['default']();\n    _this.parseStream = new _parseStream2['default']();\n    _this.lineStream.pipe(_this.parseStream);\n    /* eslint-disable consistent-this */\n    var self = _this;\n    /* eslint-enable consistent-this */\n    var uris = [];\n    var currentUri = {};\n    // if specified, the active EXT-X-MAP definition\n    var currentMap = void 0;\n    // if specified, the active decryption key\n    var _key = void 0;\n    var noop = function noop() {};\n    var defaultMediaGroups = {\n      'AUDIO': {},\n      'VIDEO': {},\n      'CLOSED-CAPTIONS': {},\n      'SUBTITLES': {}\n    };\n    // group segments into numbered timelines delineated by discontinuities\n    var currentTimeline = 0;\n\n    // the manifest is empty until the parse stream begins delivering data\n    _this.manifest = {\n      allowCache: true,\n      discontinuityStarts: [],\n      segments: []\n    };\n\n    // update the manifest with the m3u8 entry from the parse stream\n    _this.parseStream.on('data', function (entry) {\n      var mediaGroup = void 0;\n      var rendition = void 0;\n\n      ({\n        tag: function tag() {\n          // switch based on the tag type\n          (({\n            'allow-cache': function allowCache() {\n              this.manifest.allowCache = entry.allowed;\n              if (!('allowed' in entry)) {\n                this.trigger('info', {\n                  message: 'defaulting allowCache to YES'\n                });\n                this.manifest.allowCache = true;\n              }\n            },\n            byterange: function byterange() {\n              var byterange = {};\n\n              if ('length' in entry) {\n                currentUri.byterange = byterange;\n                byterange.length = entry.length;\n\n                if (!('offset' in entry)) {\n                  this.trigger('info', {\n                    message: 'defaulting offset to zero'\n                  });\n                  entry.offset = 0;\n                }\n              }\n              if ('offset' in entry) {\n                currentUri.byterange = byterange;\n                byterange.offset = entry.offset;\n              }\n            },\n            endlist: function endlist() {\n              this.manifest.endList = true;\n            },\n            inf: function inf() {\n              if (!('mediaSequence' in this.manifest)) {\n                this.manifest.mediaSequence = 0;\n                this.trigger('info', {\n                  message: 'defaulting media sequence to zero'\n                });\n              }\n              if (!('discontinuitySequence' in this.manifest)) {\n                this.manifest.discontinuitySequence = 0;\n                this.trigger('info', {\n                  message: 'defaulting discontinuity sequence to zero'\n                });\n              }\n              if (entry.duration > 0) {\n                currentUri.duration = entry.duration;\n              }\n\n              if (entry.duration === 0) {\n                currentUri.duration = 0.01;\n                this.trigger('info', {\n                  message: 'updating zero segment duration to a small value'\n                });\n              }\n\n              this.manifest.segments = uris;\n            },\n            key: function key() {\n              if (!entry.attributes) {\n                this.trigger('warn', {\n                  message: 'ignoring key declaration without attribute list'\n                });\n                return;\n              }\n              // clear the active encryption key\n              if (entry.attributes.METHOD === 'NONE') {\n                _key = null;\n                return;\n              }\n              if (!entry.attributes.URI) {\n                this.trigger('warn', {\n                  message: 'ignoring key declaration without URI'\n                });\n                return;\n              }\n              if (!entry.attributes.METHOD) {\n                this.trigger('warn', {\n                  message: 'defaulting key method to AES-128'\n                });\n              }\n\n              // setup an encryption key for upcoming segments\n              _key = {\n                method: entry.attributes.METHOD || 'AES-128',\n                uri: entry.attributes.URI\n              };\n\n              if (typeof entry.attributes.IV !== 'undefined') {\n                _key.iv = entry.attributes.IV;\n              }\n            },\n            'media-sequence': function mediaSequence() {\n              if (!isFinite(entry.number)) {\n                this.trigger('warn', {\n                  message: 'ignoring invalid media sequence: ' + entry.number\n                });\n                return;\n              }\n              this.manifest.mediaSequence = entry.number;\n            },\n            'discontinuity-sequence': function discontinuitySequence() {\n              if (!isFinite(entry.number)) {\n                this.trigger('warn', {\n                  message: 'ignoring invalid discontinuity sequence: ' + entry.number\n                });\n                return;\n              }\n              this.manifest.discontinuitySequence = entry.number;\n              currentTimeline = entry.number;\n            },\n            'playlist-type': function playlistType() {\n              if (!/VOD|EVENT/.test(entry.playlistType)) {\n                this.trigger('warn', {\n                  message: 'ignoring unknown playlist type: ' + entry.playlist\n                });\n                return;\n              }\n              this.manifest.playlistType = entry.playlistType;\n            },\n            map: function map() {\n              currentMap = {};\n              if (entry.uri) {\n                currentMap.uri = entry.uri;\n              }\n              if (entry.byterange) {\n                currentMap.byterange = entry.byterange;\n              }\n            },\n            'stream-inf': function streamInf() {\n              this.manifest.playlists = uris;\n              this.manifest.mediaGroups = this.manifest.mediaGroups || defaultMediaGroups;\n\n              if (!entry.attributes) {\n                this.trigger('warn', {\n                  message: 'ignoring empty stream-inf attributes'\n                });\n                return;\n              }\n\n              if (!currentUri.attributes) {\n                currentUri.attributes = {};\n              }\n              _extends(currentUri.attributes, entry.attributes);\n            },\n            media: function media() {\n              this.manifest.mediaGroups = this.manifest.mediaGroups || defaultMediaGroups;\n\n              if (!(entry.attributes && entry.attributes.TYPE && entry.attributes['GROUP-ID'] && entry.attributes.NAME)) {\n                this.trigger('warn', {\n                  message: 'ignoring incomplete or missing media group'\n                });\n                return;\n              }\n\n              // find the media group, creating defaults as necessary\n              var mediaGroupType = this.manifest.mediaGroups[entry.attributes.TYPE];\n\n              mediaGroupType[entry.attributes['GROUP-ID']] = mediaGroupType[entry.attributes['GROUP-ID']] || {};\n              mediaGroup = mediaGroupType[entry.attributes['GROUP-ID']];\n\n              // collect the rendition metadata\n              rendition = {\n                'default': /yes/i.test(entry.attributes.DEFAULT)\n              };\n              if (rendition['default']) {\n                rendition.autoselect = true;\n              } else {\n                rendition.autoselect = /yes/i.test(entry.attributes.AUTOSELECT);\n              }\n              if (entry.attributes.LANGUAGE) {\n                rendition.language = entry.attributes.LANGUAGE;\n              }\n              if (entry.attributes.URI) {\n                rendition.uri = entry.attributes.URI;\n              }\n              if (entry.attributes['INSTREAM-ID']) {\n                rendition.instreamId = entry.attributes['INSTREAM-ID'];\n              }\n              if (entry.attributes.CHARACTERISTICS) {\n                rendition.characteristics = entry.attributes.CHARACTERISTICS;\n              }\n              if (entry.attributes.FORCED) {\n                rendition.forced = /yes/i.test(entry.attributes.FORCED);\n              }\n\n              // insert the new rendition\n              mediaGroup[entry.attributes.NAME] = rendition;\n            },\n            discontinuity: function discontinuity() {\n              currentTimeline += 1;\n              currentUri.discontinuity = true;\n              this.manifest.discontinuityStarts.push(uris.length);\n            },\n            'program-date-time': function programDateTime() {\n              this.manifest.dateTimeString = entry.dateTimeString;\n              this.manifest.dateTimeObject = entry.dateTimeObject;\n            },\n            targetduration: function targetduration() {\n              if (!isFinite(entry.duration) || entry.duration < 0) {\n                this.trigger('warn', {\n                  message: 'ignoring invalid target duration: ' + entry.duration\n                });\n                return;\n              }\n              this.manifest.targetDuration = entry.duration;\n            },\n            totalduration: function totalduration() {\n              if (!isFinite(entry.duration) || entry.duration < 0) {\n                this.trigger('warn', {\n                  message: 'ignoring invalid total duration: ' + entry.duration\n                });\n                return;\n              }\n              this.manifest.totalDuration = entry.duration;\n            },\n            'cue-out': function cueOut() {\n              currentUri.cueOut = entry.data;\n            },\n            'cue-out-cont': function cueOutCont() {\n              currentUri.cueOutCont = entry.data;\n            },\n            'cue-in': function cueIn() {\n              currentUri.cueIn = entry.data;\n            }\n          })[entry.tagType] || noop).call(self);\n        },\n        uri: function uri() {\n          currentUri.uri = entry.uri;\n          uris.push(currentUri);\n\n          // if no explicit duration was declared, use the target duration\n          if (this.manifest.targetDuration && !('duration' in currentUri)) {\n            this.trigger('warn', {\n              message: 'defaulting segment duration to the target duration'\n            });\n            currentUri.duration = this.manifest.targetDuration;\n          }\n          // annotate with encryption information, if necessary\n          if (_key) {\n            currentUri.key = _key;\n          }\n          currentUri.timeline = currentTimeline;\n          // annotate with initialization segment information, if necessary\n          if (currentMap) {\n            currentUri.map = currentMap;\n          }\n\n          // prepare for the next URI\n          currentUri = {};\n        },\n        comment: function comment() {\n          // comments are not important for playback\n        }\n      })[entry.type].call(self);\n    });\n\n    return _this;\n  }\n\n  /**\n   * Parse the input string and update the manifest object.\n   *\n   * @param {String} chunk a potentially incomplete portion of the manifest\n   */\n\n\n  _createClass(Parser, [{\n    key: 'push',\n    value: function push(chunk) {\n      this.lineStream.push(chunk);\n    }\n\n    /**\n     * Flush any remaining input. This can be handy if the last line of an M3U8\n     * manifest did not contain a trailing newline but the file has been\n     * completely received.\n     */\n\n  }, {\n    key: 'end',\n    value: function end() {\n      // flush any buffered input\n      this.lineStream.push('\\n');\n    }\n  }]);\n\n  return Parser;\n}(_stream2['default']);\n\nexports['default'] = Parser;\n\n//# sourceURL=webpack:///./node_modules/m3u8-parser/es5/parser.js?");

/***/ }),

/***/ "./node_modules/m3u8-parser/es5/stream.js":
/*!************************************************!*\
  !*** ./node_modules/m3u8-parser/es5/stream.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _createClass = function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (\"value\" in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(\"Cannot call a class as a function\"); } }\n\n/**\n * @file stream.js\n */\n/**\n * A lightweight readable stream implemention that handles event dispatching.\n *\n * @class Stream\n */\nvar Stream = function () {\n  function Stream() {\n    _classCallCheck(this, Stream);\n\n    this.listeners = {};\n  }\n\n  /**\n   * Add a listener for a specified event type.\n   *\n   * @param {String} type the event name\n   * @param {Function} listener the callback to be invoked when an event of\n   * the specified type occurs\n   */\n\n\n  _createClass(Stream, [{\n    key: 'on',\n    value: function on(type, listener) {\n      if (!this.listeners[type]) {\n        this.listeners[type] = [];\n      }\n      this.listeners[type].push(listener);\n    }\n\n    /**\n     * Remove a listener for a specified event type.\n     *\n     * @param {String} type the event name\n     * @param {Function} listener  a function previously registered for this\n     * type of event through `on`\n     * @return {Boolean} if we could turn it off or not\n     */\n\n  }, {\n    key: 'off',\n    value: function off(type, listener) {\n      if (!this.listeners[type]) {\n        return false;\n      }\n\n      var index = this.listeners[type].indexOf(listener);\n\n      this.listeners[type].splice(index, 1);\n      return index > -1;\n    }\n\n    /**\n     * Trigger an event of the specified type on this stream. Any additional\n     * arguments to this function are passed as parameters to event listeners.\n     *\n     * @param {String} type the event name\n     */\n\n  }, {\n    key: 'trigger',\n    value: function trigger(type) {\n      var callbacks = this.listeners[type];\n      var i = void 0;\n      var length = void 0;\n      var args = void 0;\n\n      if (!callbacks) {\n        return;\n      }\n      // Slicing the arguments on every invocation of this method\n      // can add a significant amount of overhead. Avoid the\n      // intermediate object creation for the common case of a\n      // single callback argument\n      if (arguments.length === 2) {\n        length = callbacks.length;\n        for (i = 0; i < length; ++i) {\n          callbacks[i].call(this, arguments[1]);\n        }\n      } else {\n        args = Array.prototype.slice.call(arguments, 1);\n        length = callbacks.length;\n        for (i = 0; i < length; ++i) {\n          callbacks[i].apply(this, args);\n        }\n      }\n    }\n\n    /**\n     * Destroys the stream and cleans up.\n     */\n\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      this.listeners = {};\n    }\n    /**\n     * Forwards all `data` events on this stream to the destination stream. The\n     * destination stream should provide a method `push` to receive the data\n     * events as they arrive.\n     *\n     * @param {Stream} destination the stream that will receive all `data` events\n     * @see http://nodejs.org/api/stream.html#stream_readable_pipe_destination_options\n     */\n\n  }, {\n    key: 'pipe',\n    value: function pipe(destination) {\n      this.on('data', function (data) {\n        destination.push(data);\n      });\n    }\n  }]);\n\n  return Stream;\n}();\n\nexports['default'] = Stream;\n\n//# sourceURL=webpack:///./node_modules/m3u8-parser/es5/stream.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/aac/index.js":
/*!**********************************************!*\
  !*** ./node_modules/mux.js/lib/aac/index.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2016 Brightcove\n * All rights reserved.\n *\n * A stream-based aac to mp4 converter. This utility can be used to\n * deliver mp4s to a SourceBuffer on platforms that support native\n * Media Source Extensions.\n */\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\");\n\n// Constants\nvar AacStream;\n\n/**\n * Splits an incoming stream of binary data into ADTS and ID3 Frames.\n */\n\nAacStream = function() {\n  var\n    everything = new Uint8Array(),\n    timeStamp = 0;\n\n  AacStream.prototype.init.call(this);\n\n  this.setTimestamp = function(timestamp) {\n    timeStamp = timestamp;\n  };\n\n  this.parseId3TagSize = function(header, byteIndex) {\n    var\n      returnSize = (header[byteIndex + 6] << 21) |\n                   (header[byteIndex + 7] << 14) |\n                   (header[byteIndex + 8] << 7) |\n                   (header[byteIndex + 9]),\n      flags = header[byteIndex + 5],\n      footerPresent = (flags & 16) >> 4;\n\n    if (footerPresent) {\n      return returnSize + 20;\n    }\n    return returnSize + 10;\n  };\n\n  this.parseAdtsSize = function(header, byteIndex) {\n    var\n      lowThree = (header[byteIndex + 5] & 0xE0) >> 5,\n      middle = header[byteIndex + 4] << 3,\n      highTwo = header[byteIndex + 3] & 0x3 << 11;\n\n    return (highTwo | middle) | lowThree;\n  };\n\n  this.push = function(bytes) {\n    var\n      frameSize = 0,\n      byteIndex = 0,\n      bytesLeft,\n      chunk,\n      packet,\n      tempLength;\n\n    // If there are bytes remaining from the last segment, prepend them to the\n    // bytes that were pushed in\n    if (everything.length) {\n      tempLength = everything.length;\n      everything = new Uint8Array(bytes.byteLength + tempLength);\n      everything.set(everything.subarray(0, tempLength));\n      everything.set(bytes, tempLength);\n    } else {\n      everything = bytes;\n    }\n\n    while (everything.length - byteIndex >= 3) {\n      if ((everything[byteIndex] === 'I'.charCodeAt(0)) &&\n          (everything[byteIndex + 1] === 'D'.charCodeAt(0)) &&\n          (everything[byteIndex + 2] === '3'.charCodeAt(0))) {\n\n        // Exit early because we don't have enough to parse\n        // the ID3 tag header\n        if (everything.length - byteIndex < 10) {\n          break;\n        }\n\n        // check framesize\n        frameSize = this.parseId3TagSize(everything, byteIndex);\n\n        // Exit early if we don't have enough in the buffer\n        // to emit a full packet\n        if (frameSize > everything.length) {\n          break;\n        }\n        chunk = {\n          type: 'timed-metadata',\n          data: everything.subarray(byteIndex, byteIndex + frameSize)\n        };\n        this.trigger('data', chunk);\n        byteIndex += frameSize;\n        continue;\n      } else if ((everything[byteIndex] & 0xff === 0xff) &&\n                 ((everything[byteIndex + 1] & 0xf0) === 0xf0)) {\n\n        // Exit early because we don't have enough to parse\n        // the ADTS frame header\n        if (everything.length - byteIndex < 7) {\n          break;\n        }\n\n        frameSize = this.parseAdtsSize(everything, byteIndex);\n\n        // Exit early if we don't have enough in the buffer\n        // to emit a full packet\n        if (frameSize > everything.length) {\n          break;\n        }\n\n        packet = {\n          type: 'audio',\n          data: everything.subarray(byteIndex, byteIndex + frameSize),\n          pts: timeStamp,\n          dts: timeStamp\n        };\n        this.trigger('data', packet);\n        byteIndex += frameSize;\n        continue;\n      }\n      byteIndex++;\n    }\n    bytesLeft = everything.length - byteIndex;\n\n    if (bytesLeft > 0) {\n      everything = everything.subarray(byteIndex);\n    } else {\n      everything = new Uint8Array();\n    }\n  };\n};\n\nAacStream.prototype = new Stream();\n\nmodule.exports = AacStream;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/aac/index.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/aac/probe.js":
/*!**********************************************!*\
  !*** ./node_modules/mux.js/lib/aac/probe.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2016 Brightcove\n * All rights reserved.\n *\n * Utilities to detect basic properties and metadata about Aac data.\n */\n\n\nvar ADTS_SAMPLING_FREQUENCIES = [\n  96000,\n  88200,\n  64000,\n  48000,\n  44100,\n  32000,\n  24000,\n  22050,\n  16000,\n  12000,\n  11025,\n  8000,\n  7350\n];\n\nvar parseSyncSafeInteger = function(data) {\n  return (data[0] << 21) |\n          (data[1] << 14) |\n          (data[2] << 7) |\n          (data[3]);\n};\n\n// return a percent-encoded representation of the specified byte range\n// @see http://en.wikipedia.org/wiki/Percent-encoding\nvar percentEncode = function(bytes, start, end) {\n  var i, result = '';\n  for (i = start; i < end; i++) {\n    result += '%' + ('00' + bytes[i].toString(16)).slice(-2);\n  }\n  return result;\n};\n\n// return the string representation of the specified byte range,\n// interpreted as ISO-8859-1.\nvar parseIso88591 = function(bytes, start, end) {\n  return unescape(percentEncode(bytes, start, end)); // jshint ignore:line\n};\n\nvar parseId3TagSize = function(header, byteIndex) {\n  var\n    returnSize = (header[byteIndex + 6] << 21) |\n                 (header[byteIndex + 7] << 14) |\n                 (header[byteIndex + 8] << 7) |\n                 (header[byteIndex + 9]),\n    flags = header[byteIndex + 5],\n    footerPresent = (flags & 16) >> 4;\n\n  if (footerPresent) {\n    return returnSize + 20;\n  }\n  return returnSize + 10;\n};\n\nvar parseAdtsSize = function(header, byteIndex) {\n  var\n    lowThree = (header[byteIndex + 5] & 0xE0) >> 5,\n    middle = header[byteIndex + 4] << 3,\n    highTwo = header[byteIndex + 3] & 0x3 << 11;\n\n  return (highTwo | middle) | lowThree;\n};\n\nvar parseType = function(header, byteIndex) {\n  if ((header[byteIndex] === 'I'.charCodeAt(0)) &&\n      (header[byteIndex + 1] === 'D'.charCodeAt(0)) &&\n      (header[byteIndex + 2] === '3'.charCodeAt(0))) {\n    return 'timed-metadata';\n  } else if ((header[byteIndex] & 0xff === 0xff) &&\n             ((header[byteIndex + 1] & 0xf0) === 0xf0)) {\n    return 'audio';\n  }\n  return null;\n};\n\nvar parseSampleRate = function(packet) {\n  var i = 0;\n\n  while (i + 5 < packet.length) {\n    if (packet[i] !== 0xFF || (packet[i + 1] & 0xF6) !== 0xF0) {\n      // If a valid header was not found,  jump one forward and attempt to\n      // find a valid ADTS header starting at the next byte\n      i++;\n      continue;\n    }\n    return ADTS_SAMPLING_FREQUENCIES[(packet[i + 2] & 0x3c) >>> 2];\n  }\n\n  return null;\n};\n\nvar parseAacTimestamp = function(packet) {\n  var frameStart, frameSize, frame, frameHeader;\n\n  // find the start of the first frame and the end of the tag\n  frameStart = 10;\n  if (packet[5] & 0x40) {\n    // advance the frame start past the extended header\n    frameStart += 4; // header size field\n    frameStart += parseSyncSafeInteger(packet.subarray(10, 14));\n  }\n\n  // parse one or more ID3 frames\n  // http://id3.org/id3v2.3.0#ID3v2_frame_overview\n  do {\n    // determine the number of bytes in this frame\n    frameSize = parseSyncSafeInteger(packet.subarray(frameStart + 4, frameStart + 8));\n    if (frameSize < 1) {\n      return null;\n    }\n    frameHeader = String.fromCharCode(packet[frameStart],\n                                      packet[frameStart + 1],\n                                      packet[frameStart + 2],\n                                      packet[frameStart + 3]);\n\n    if (frameHeader === 'PRIV') {\n      frame = packet.subarray(frameStart + 10, frameStart + frameSize + 10);\n\n      for (var i = 0; i < frame.byteLength; i++) {\n        if (frame[i] === 0) {\n          var owner = parseIso88591(frame, 0, i);\n          if (owner === 'com.apple.streaming.transportStreamTimestamp') {\n            var d = frame.subarray(i + 1);\n            var size = ((d[3] & 0x01)  << 30) |\n                       (d[4]  << 22) |\n                       (d[5] << 14) |\n                       (d[6] << 6) |\n                       (d[7] >>> 2);\n            size *= 4;\n            size += d[7] & 0x03;\n\n            return size;\n          }\n          break;\n        }\n      }\n    }\n\n    frameStart += 10; // advance past the frame header\n    frameStart += frameSize; // advance past the frame body\n  } while (frameStart < packet.byteLength);\n  return null;\n};\n\nmodule.exports = {\n  parseId3TagSize: parseId3TagSize,\n  parseAdtsSize: parseAdtsSize,\n  parseType: parseType,\n  parseSampleRate: parseSampleRate,\n  parseAacTimestamp: parseAacTimestamp\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/aac/probe.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/codecs/adts.js":
/*!************************************************!*\
  !*** ./node_modules/mux.js/lib/codecs/adts.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\");\n\nvar AdtsStream;\n\nvar\n  ADTS_SAMPLING_FREQUENCIES = [\n    96000,\n    88200,\n    64000,\n    48000,\n    44100,\n    32000,\n    24000,\n    22050,\n    16000,\n    12000,\n    11025,\n    8000,\n    7350\n  ];\n\n/*\n * Accepts a ElementaryStream and emits data events with parsed\n * AAC Audio Frames of the individual packets. Input audio in ADTS\n * format is unpacked and re-emitted as AAC frames.\n *\n * @see http://wiki.multimedia.cx/index.php?title=ADTS\n * @see http://wiki.multimedia.cx/?title=Understanding_AAC\n */\nAdtsStream = function() {\n  var buffer;\n\n  AdtsStream.prototype.init.call(this);\n\n  this.push = function(packet) {\n    var\n      i = 0,\n      frameNum = 0,\n      frameLength,\n      protectionSkipBytes,\n      frameEnd,\n      oldBuffer,\n      sampleCount,\n      adtsFrameDuration;\n\n    if (packet.type !== 'audio') {\n      // ignore non-audio data\n      return;\n    }\n\n    // Prepend any data in the buffer to the input data so that we can parse\n    // aac frames the cross a PES packet boundary\n    if (buffer) {\n      oldBuffer = buffer;\n      buffer = new Uint8Array(oldBuffer.byteLength + packet.data.byteLength);\n      buffer.set(oldBuffer);\n      buffer.set(packet.data, oldBuffer.byteLength);\n    } else {\n      buffer = packet.data;\n    }\n\n    // unpack any ADTS frames which have been fully received\n    // for details on the ADTS header, see http://wiki.multimedia.cx/index.php?title=ADTS\n    while (i + 5 < buffer.length) {\n\n      // Loook for the start of an ADTS header..\n      if (buffer[i] !== 0xFF || (buffer[i + 1] & 0xF6) !== 0xF0) {\n        // If a valid header was not found,  jump one forward and attempt to\n        // find a valid ADTS header starting at the next byte\n        i++;\n        continue;\n      }\n\n      // The protection skip bit tells us if we have 2 bytes of CRC data at the\n      // end of the ADTS header\n      protectionSkipBytes = (~buffer[i + 1] & 0x01) * 2;\n\n      // Frame length is a 13 bit integer starting 16 bits from the\n      // end of the sync sequence\n      frameLength = ((buffer[i + 3] & 0x03) << 11) |\n        (buffer[i + 4] << 3) |\n        ((buffer[i + 5] & 0xe0) >> 5);\n\n      sampleCount = ((buffer[i + 6] & 0x03) + 1) * 1024;\n      adtsFrameDuration = (sampleCount * 90000) /\n        ADTS_SAMPLING_FREQUENCIES[(buffer[i + 2] & 0x3c) >>> 2];\n\n      frameEnd = i + frameLength;\n\n      // If we don't have enough data to actually finish this ADTS frame, return\n      // and wait for more data\n      if (buffer.byteLength < frameEnd) {\n        return;\n      }\n\n      // Otherwise, deliver the complete AAC frame\n      this.trigger('data', {\n        pts: packet.pts + (frameNum * adtsFrameDuration),\n        dts: packet.dts + (frameNum * adtsFrameDuration),\n        sampleCount: sampleCount,\n        audioobjecttype: ((buffer[i + 2] >>> 6) & 0x03) + 1,\n        channelcount: ((buffer[i + 2] & 1) << 2) |\n          ((buffer[i + 3] & 0xc0) >>> 6),\n        samplerate: ADTS_SAMPLING_FREQUENCIES[(buffer[i + 2] & 0x3c) >>> 2],\n        samplingfrequencyindex: (buffer[i + 2] & 0x3c) >>> 2,\n        // assume ISO/IEC 14496-12 AudioSampleEntry default of 16\n        samplesize: 16,\n        data: buffer.subarray(i + 7 + protectionSkipBytes, frameEnd)\n      });\n\n      // If the buffer is empty, clear it and return\n      if (buffer.byteLength === frameEnd) {\n        buffer = undefined;\n        return;\n      }\n\n      frameNum++;\n\n      // Remove the finished frame from the buffer and start the process again\n      buffer = buffer.subarray(frameEnd);\n    }\n  };\n  this.flush = function() {\n    this.trigger('done');\n  };\n};\n\nAdtsStream.prototype = new Stream();\n\nmodule.exports = AdtsStream;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/codecs/adts.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/codecs/h264.js":
/*!************************************************!*\
  !*** ./node_modules/mux.js/lib/codecs/h264.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\");\nvar ExpGolomb = __webpack_require__(/*! ../utils/exp-golomb.js */ \"./node_modules/mux.js/lib/utils/exp-golomb.js\");\n\nvar H264Stream, NalByteStream;\nvar PROFILES_WITH_OPTIONAL_SPS_DATA;\n\n/**\n * Accepts a NAL unit byte stream and unpacks the embedded NAL units.\n */\nNalByteStream = function() {\n  var\n    syncPoint = 0,\n    i,\n    buffer;\n  NalByteStream.prototype.init.call(this);\n\n  this.push = function(data) {\n    var swapBuffer;\n\n    if (!buffer) {\n      buffer = data.data;\n    } else {\n      swapBuffer = new Uint8Array(buffer.byteLength + data.data.byteLength);\n      swapBuffer.set(buffer);\n      swapBuffer.set(data.data, buffer.byteLength);\n      buffer = swapBuffer;\n    }\n\n    // Rec. ITU-T H.264, Annex B\n    // scan for NAL unit boundaries\n\n    // a match looks like this:\n    // 0 0 1 .. NAL .. 0 0 1\n    // ^ sync point        ^ i\n    // or this:\n    // 0 0 1 .. NAL .. 0 0 0\n    // ^ sync point        ^ i\n\n    // advance the sync point to a NAL start, if necessary\n    for (; syncPoint < buffer.byteLength - 3; syncPoint++) {\n      if (buffer[syncPoint + 2] === 1) {\n        // the sync point is properly aligned\n        i = syncPoint + 5;\n        break;\n      }\n    }\n\n    while (i < buffer.byteLength) {\n      // look at the current byte to determine if we've hit the end of\n      // a NAL unit boundary\n      switch (buffer[i]) {\n      case 0:\n        // skip past non-sync sequences\n        if (buffer[i - 1] !== 0) {\n          i += 2;\n          break;\n        } else if (buffer[i - 2] !== 0) {\n          i++;\n          break;\n        }\n\n        // deliver the NAL unit if it isn't empty\n        if (syncPoint + 3 !== i - 2) {\n          this.trigger('data', buffer.subarray(syncPoint + 3, i - 2));\n        }\n\n        // drop trailing zeroes\n        do {\n          i++;\n        } while (buffer[i] !== 1 && i < buffer.length);\n        syncPoint = i - 2;\n        i += 3;\n        break;\n      case 1:\n        // skip past non-sync sequences\n        if (buffer[i - 1] !== 0 ||\n            buffer[i - 2] !== 0) {\n          i += 3;\n          break;\n        }\n\n        // deliver the NAL unit\n        this.trigger('data', buffer.subarray(syncPoint + 3, i - 2));\n        syncPoint = i - 2;\n        i += 3;\n        break;\n      default:\n        // the current byte isn't a one or zero, so it cannot be part\n        // of a sync sequence\n        i += 3;\n        break;\n      }\n    }\n    // filter out the NAL units that were delivered\n    buffer = buffer.subarray(syncPoint);\n    i -= syncPoint;\n    syncPoint = 0;\n  };\n\n  this.flush = function() {\n    // deliver the last buffered NAL unit\n    if (buffer && buffer.byteLength > 3) {\n      this.trigger('data', buffer.subarray(syncPoint + 3));\n    }\n    // reset the stream state\n    buffer = null;\n    syncPoint = 0;\n    this.trigger('done');\n  };\n};\nNalByteStream.prototype = new Stream();\n\n// values of profile_idc that indicate additional fields are included in the SPS\n// see Recommendation ITU-T H.264 (4/2013),\n// 7.3.2.1.1 Sequence parameter set data syntax\nPROFILES_WITH_OPTIONAL_SPS_DATA = {\n  100: true,\n  110: true,\n  122: true,\n  244: true,\n  44: true,\n  83: true,\n  86: true,\n  118: true,\n  128: true,\n  138: true,\n  139: true,\n  134: true\n};\n\n/**\n * Accepts input from a ElementaryStream and produces H.264 NAL unit data\n * events.\n */\nH264Stream = function() {\n  var\n    nalByteStream = new NalByteStream(),\n    self,\n    trackId,\n    currentPts,\n    currentDts,\n\n    discardEmulationPreventionBytes,\n    readSequenceParameterSet,\n    skipScalingList;\n\n  H264Stream.prototype.init.call(this);\n  self = this;\n\n  this.push = function(packet) {\n    if (packet.type !== 'video') {\n      return;\n    }\n    trackId = packet.trackId;\n    currentPts = packet.pts;\n    currentDts = packet.dts;\n\n    nalByteStream.push(packet);\n  };\n\n  nalByteStream.on('data', function(data) {\n    var\n      event = {\n        trackId: trackId,\n        pts: currentPts,\n        dts: currentDts,\n        data: data\n      };\n\n    switch (data[0] & 0x1f) {\n    case 0x05:\n      event.nalUnitType = 'slice_layer_without_partitioning_rbsp_idr';\n      break;\n    case 0x06:\n      event.nalUnitType = 'sei_rbsp';\n      event.escapedRBSP = discardEmulationPreventionBytes(data.subarray(1));\n      break;\n    case 0x07:\n      event.nalUnitType = 'seq_parameter_set_rbsp';\n      event.escapedRBSP = discardEmulationPreventionBytes(data.subarray(1));\n      event.config = readSequenceParameterSet(event.escapedRBSP);\n      break;\n    case 0x08:\n      event.nalUnitType = 'pic_parameter_set_rbsp';\n      break;\n    case 0x09:\n      event.nalUnitType = 'access_unit_delimiter_rbsp';\n      break;\n\n    default:\n      break;\n    }\n    self.trigger('data', event);\n  });\n  nalByteStream.on('done', function() {\n    self.trigger('done');\n  });\n\n  this.flush = function() {\n    nalByteStream.flush();\n  };\n\n  /**\n   * Advance the ExpGolomb decoder past a scaling list. The scaling\n   * list is optionally transmitted as part of a sequence parameter\n   * set and is not relevant to transmuxing.\n   * @param count {number} the number of entries in this scaling list\n   * @param expGolombDecoder {object} an ExpGolomb pointed to the\n   * start of a scaling list\n   * @see Recommendation ITU-T H.264, Section 7.3.2.1.1.1\n   */\n  skipScalingList = function(count, expGolombDecoder) {\n    var\n      lastScale = 8,\n      nextScale = 8,\n      j,\n      deltaScale;\n\n    for (j = 0; j < count; j++) {\n      if (nextScale !== 0) {\n        deltaScale = expGolombDecoder.readExpGolomb();\n        nextScale = (lastScale + deltaScale + 256) % 256;\n      }\n\n      lastScale = (nextScale === 0) ? lastScale : nextScale;\n    }\n  };\n\n  /**\n   * Expunge any \"Emulation Prevention\" bytes from a \"Raw Byte\n   * Sequence Payload\"\n   * @param data {Uint8Array} the bytes of a RBSP from a NAL\n   * unit\n   * @return {Uint8Array} the RBSP without any Emulation\n   * Prevention Bytes\n   */\n  discardEmulationPreventionBytes = function(data) {\n    var\n      length = data.byteLength,\n      emulationPreventionBytesPositions = [],\n      i = 1,\n      newLength, newData;\n\n    // Find all `Emulation Prevention Bytes`\n    while (i < length - 2) {\n      if (data[i] === 0 && data[i + 1] === 0 && data[i + 2] === 0x03) {\n        emulationPreventionBytesPositions.push(i + 2);\n        i += 2;\n      } else {\n        i++;\n      }\n    }\n\n    // If no Emulation Prevention Bytes were found just return the original\n    // array\n    if (emulationPreventionBytesPositions.length === 0) {\n      return data;\n    }\n\n    // Create a new array to hold the NAL unit data\n    newLength = length - emulationPreventionBytesPositions.length;\n    newData = new Uint8Array(newLength);\n    var sourceIndex = 0;\n\n    for (i = 0; i < newLength; sourceIndex++, i++) {\n      if (sourceIndex === emulationPreventionBytesPositions[0]) {\n        // Skip this byte\n        sourceIndex++;\n        // Remove this position index\n        emulationPreventionBytesPositions.shift();\n      }\n      newData[i] = data[sourceIndex];\n    }\n\n    return newData;\n  };\n\n  /**\n   * Read a sequence parameter set and return some interesting video\n   * properties. A sequence parameter set is the H264 metadata that\n   * describes the properties of upcoming video frames.\n   * @param data {Uint8Array} the bytes of a sequence parameter set\n   * @return {object} an object with configuration parsed from the\n   * sequence parameter set, including the dimensions of the\n   * associated video frames.\n   */\n  readSequenceParameterSet = function(data) {\n    var\n      frameCropLeftOffset = 0,\n      frameCropRightOffset = 0,\n      frameCropTopOffset = 0,\n      frameCropBottomOffset = 0,\n      sarScale = 1,\n      expGolombDecoder, profileIdc, levelIdc, profileCompatibility,\n      chromaFormatIdc, picOrderCntType,\n      numRefFramesInPicOrderCntCycle, picWidthInMbsMinus1,\n      picHeightInMapUnitsMinus1,\n      frameMbsOnlyFlag,\n      scalingListCount,\n      sarRatio,\n      aspectRatioIdc,\n      i;\n\n    expGolombDecoder = new ExpGolomb(data);\n    profileIdc = expGolombDecoder.readUnsignedByte(); // profile_idc\n    profileCompatibility = expGolombDecoder.readUnsignedByte(); // constraint_set[0-5]_flag\n    levelIdc = expGolombDecoder.readUnsignedByte(); // level_idc u(8)\n    expGolombDecoder.skipUnsignedExpGolomb(); // seq_parameter_set_id\n\n    // some profiles have more optional data we don't need\n    if (PROFILES_WITH_OPTIONAL_SPS_DATA[profileIdc]) {\n      chromaFormatIdc = expGolombDecoder.readUnsignedExpGolomb();\n      if (chromaFormatIdc === 3) {\n        expGolombDecoder.skipBits(1); // separate_colour_plane_flag\n      }\n      expGolombDecoder.skipUnsignedExpGolomb(); // bit_depth_luma_minus8\n      expGolombDecoder.skipUnsignedExpGolomb(); // bit_depth_chroma_minus8\n      expGolombDecoder.skipBits(1); // qpprime_y_zero_transform_bypass_flag\n      if (expGolombDecoder.readBoolean()) { // seq_scaling_matrix_present_flag\n        scalingListCount = (chromaFormatIdc !== 3) ? 8 : 12;\n        for (i = 0; i < scalingListCount; i++) {\n          if (expGolombDecoder.readBoolean()) { // seq_scaling_list_present_flag[ i ]\n            if (i < 6) {\n              skipScalingList(16, expGolombDecoder);\n            } else {\n              skipScalingList(64, expGolombDecoder);\n            }\n          }\n        }\n      }\n    }\n\n    expGolombDecoder.skipUnsignedExpGolomb(); // log2_max_frame_num_minus4\n    picOrderCntType = expGolombDecoder.readUnsignedExpGolomb();\n\n    if (picOrderCntType === 0) {\n      expGolombDecoder.readUnsignedExpGolomb(); // log2_max_pic_order_cnt_lsb_minus4\n    } else if (picOrderCntType === 1) {\n      expGolombDecoder.skipBits(1); // delta_pic_order_always_zero_flag\n      expGolombDecoder.skipExpGolomb(); // offset_for_non_ref_pic\n      expGolombDecoder.skipExpGolomb(); // offset_for_top_to_bottom_field\n      numRefFramesInPicOrderCntCycle = expGolombDecoder.readUnsignedExpGolomb();\n      for (i = 0; i < numRefFramesInPicOrderCntCycle; i++) {\n        expGolombDecoder.skipExpGolomb(); // offset_for_ref_frame[ i ]\n      }\n    }\n\n    expGolombDecoder.skipUnsignedExpGolomb(); // max_num_ref_frames\n    expGolombDecoder.skipBits(1); // gaps_in_frame_num_value_allowed_flag\n\n    picWidthInMbsMinus1 = expGolombDecoder.readUnsignedExpGolomb();\n    picHeightInMapUnitsMinus1 = expGolombDecoder.readUnsignedExpGolomb();\n\n    frameMbsOnlyFlag = expGolombDecoder.readBits(1);\n    if (frameMbsOnlyFlag === 0) {\n      expGolombDecoder.skipBits(1); // mb_adaptive_frame_field_flag\n    }\n\n    expGolombDecoder.skipBits(1); // direct_8x8_inference_flag\n    if (expGolombDecoder.readBoolean()) { // frame_cropping_flag\n      frameCropLeftOffset = expGolombDecoder.readUnsignedExpGolomb();\n      frameCropRightOffset = expGolombDecoder.readUnsignedExpGolomb();\n      frameCropTopOffset = expGolombDecoder.readUnsignedExpGolomb();\n      frameCropBottomOffset = expGolombDecoder.readUnsignedExpGolomb();\n    }\n    if (expGolombDecoder.readBoolean()) {\n      // vui_parameters_present_flag\n      if (expGolombDecoder.readBoolean()) {\n        // aspect_ratio_info_present_flag\n        aspectRatioIdc = expGolombDecoder.readUnsignedByte();\n        switch (aspectRatioIdc) {\n          case 1: sarRatio = [1, 1]; break;\n          case 2: sarRatio = [12, 11]; break;\n          case 3: sarRatio = [10, 11]; break;\n          case 4: sarRatio = [16, 11]; break;\n          case 5: sarRatio = [40, 33]; break;\n          case 6: sarRatio = [24, 11]; break;\n          case 7: sarRatio = [20, 11]; break;\n          case 8: sarRatio = [32, 11]; break;\n          case 9: sarRatio = [80, 33]; break;\n          case 10: sarRatio = [18, 11]; break;\n          case 11: sarRatio = [15, 11]; break;\n          case 12: sarRatio = [64, 33]; break;\n          case 13: sarRatio = [160, 99]; break;\n          case 14: sarRatio = [4, 3]; break;\n          case 15: sarRatio = [3, 2]; break;\n          case 16: sarRatio = [2, 1]; break;\n          case 255: {\n            sarRatio = [expGolombDecoder.readUnsignedByte() << 8 |\n                        expGolombDecoder.readUnsignedByte(),\n                        expGolombDecoder.readUnsignedByte() << 8 |\n                        expGolombDecoder.readUnsignedByte() ];\n            break;\n          }\n        }\n        if (sarRatio) {\n          sarScale = sarRatio[0] / sarRatio[1];\n        }\n      }\n    }\n    return {\n      profileIdc: profileIdc,\n      levelIdc: levelIdc,\n      profileCompatibility: profileCompatibility,\n      width: Math.ceil((((picWidthInMbsMinus1 + 1) * 16) - frameCropLeftOffset * 2 - frameCropRightOffset * 2) * sarScale),\n      height: ((2 - frameMbsOnlyFlag) * (picHeightInMapUnitsMinus1 + 1) * 16) - (frameCropTopOffset * 2) - (frameCropBottomOffset * 2)\n    };\n  };\n\n};\nH264Stream.prototype = new Stream();\n\nmodule.exports = {\n  H264Stream: H264Stream,\n  NalByteStream: NalByteStream\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/codecs/h264.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/data/silence.js":
/*!*************************************************!*\
  !*** ./node_modules/mux.js/lib/data/silence.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("var highPrefix = [33, 16, 5, 32, 164, 27];\nvar lowPrefix = [33, 65, 108, 84, 1, 2, 4, 8, 168, 2, 4, 8, 17, 191, 252];\nvar zeroFill = function(count) {\n  var a = [];\n  while (count--) {\n    a.push(0);\n  }\n  return a;\n};\n\nvar makeTable = function(metaTable) {\n  return Object.keys(metaTable).reduce(function(obj, key) {\n    obj[key] = new Uint8Array(metaTable[key].reduce(function(arr, part) {\n      return arr.concat(part);\n    }, []));\n    return obj;\n  }, {});\n};\n\n// Frames-of-silence to use for filling in missing AAC frames\nvar coneOfSilence = {\n  96000: [highPrefix, [227, 64], zeroFill(154), [56]],\n  88200: [highPrefix, [231], zeroFill(170), [56]],\n  64000: [highPrefix, [248, 192], zeroFill(240), [56]],\n  48000: [highPrefix, [255, 192], zeroFill(268), [55, 148, 128], zeroFill(54), [112]],\n  44100: [highPrefix, [255, 192], zeroFill(268), [55, 163, 128], zeroFill(84), [112]],\n  32000: [highPrefix, [255, 192], zeroFill(268), [55, 234], zeroFill(226), [112]],\n  24000: [highPrefix, [255, 192], zeroFill(268), [55, 255, 128], zeroFill(268), [111, 112], zeroFill(126), [224]],\n  16000: [highPrefix, [255, 192], zeroFill(268), [55, 255, 128], zeroFill(268), [111, 255], zeroFill(269), [223, 108], zeroFill(195), [1, 192]],\n  12000: [lowPrefix, zeroFill(268), [3, 127, 248], zeroFill(268), [6, 255, 240], zeroFill(268), [13, 255, 224], zeroFill(268), [27, 253, 128], zeroFill(259), [56]],\n  11025: [lowPrefix, zeroFill(268), [3, 127, 248], zeroFill(268), [6, 255, 240], zeroFill(268), [13, 255, 224], zeroFill(268), [27, 255, 192], zeroFill(268), [55, 175, 128], zeroFill(108), [112]],\n  8000: [lowPrefix, zeroFill(268), [3, 121, 16], zeroFill(47), [7]]\n};\n\nmodule.exports = makeTable(coneOfSilence);\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/data/silence.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/flv/coalesce-stream.js":
/*!********************************************************!*\
  !*** ./node_modules/mux.js/lib/flv/coalesce-stream.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\");\n\n/**\n * The final stage of the transmuxer that emits the flv tags\n * for audio, video, and metadata. Also tranlates in time and\n * outputs caption data and id3 cues.\n */\nvar CoalesceStream = function(options) {\n  // Number of Tracks per output segment\n  // If greater than 1, we combine multiple\n  // tracks into a single segment\n  this.numberOfTracks = 0;\n  this.metadataStream = options.metadataStream;\n\n  this.videoTags = [];\n  this.audioTags = [];\n  this.videoTrack = null;\n  this.audioTrack = null;\n  this.pendingCaptions = [];\n  this.pendingMetadata = [];\n  this.pendingTracks = 0;\n  this.processedTracks = 0;\n\n  CoalesceStream.prototype.init.call(this);\n\n  // Take output from multiple\n  this.push = function(output) {\n    // buffer incoming captions until the associated video segment\n    // finishes\n    if (output.text) {\n      return this.pendingCaptions.push(output);\n    }\n    // buffer incoming id3 tags until the final flush\n    if (output.frames) {\n      return this.pendingMetadata.push(output);\n    }\n\n    if (output.track.type === 'video') {\n      this.videoTrack = output.track;\n      this.videoTags = output.tags;\n      this.pendingTracks++;\n    }\n    if (output.track.type === 'audio') {\n      this.audioTrack = output.track;\n      this.audioTags = output.tags;\n      this.pendingTracks++;\n    }\n  };\n};\n\nCoalesceStream.prototype = new Stream();\nCoalesceStream.prototype.flush = function(flushSource) {\n  var\n    id3,\n    caption,\n    i,\n    timelineStartPts,\n    event = {\n      tags: {},\n      captions: [],\n      captionStreams: {},\n      metadata: []\n    };\n\n  if (this.pendingTracks < this.numberOfTracks) {\n    if (flushSource !== 'VideoSegmentStream' &&\n        flushSource !== 'AudioSegmentStream') {\n      // Return because we haven't received a flush from a data-generating\n      // portion of the segment (meaning that we have only recieved meta-data\n      // or captions.)\n      return;\n    } else if (this.pendingTracks === 0) {\n      // In the case where we receive a flush without any data having been\n      // received we consider it an emitted track for the purposes of coalescing\n      // `done` events.\n      // We do this for the case where there is an audio and video track in the\n      // segment but no audio data. (seen in several playlists with alternate\n      // audio tracks and no audio present in the main TS segments.)\n      this.processedTracks++;\n\n      if (this.processedTracks < this.numberOfTracks) {\n        return;\n      }\n    }\n  }\n\n  this.processedTracks += this.pendingTracks;\n  this.pendingTracks = 0;\n\n  if (this.processedTracks < this.numberOfTracks) {\n    return;\n  }\n\n  if (this.videoTrack) {\n    timelineStartPts = this.videoTrack.timelineStartInfo.pts;\n  } else if (this.audioTrack) {\n    timelineStartPts = this.audioTrack.timelineStartInfo.pts;\n  }\n\n  event.tags.videoTags = this.videoTags;\n  event.tags.audioTags = this.audioTags;\n\n  // Translate caption PTS times into second offsets into the\n  // video timeline for the segment, and add track info\n  for (i = 0; i < this.pendingCaptions.length; i++) {\n    caption = this.pendingCaptions[i];\n    caption.startTime = caption.startPts - timelineStartPts;\n    caption.startTime /= 90e3;\n    caption.endTime = caption.endPts - timelineStartPts;\n    caption.endTime /= 90e3;\n    event.captionStreams[caption.stream] = true;\n    event.captions.push(caption);\n  }\n\n  // Translate ID3 frame PTS times into second offsets into the\n  // video timeline for the segment\n  for (i = 0; i < this.pendingMetadata.length; i++) {\n    id3 = this.pendingMetadata[i];\n    id3.cueTime = id3.pts - timelineStartPts;\n    id3.cueTime /= 90e3;\n    event.metadata.push(id3);\n  }\n  // We add this to every single emitted segment even though we only need\n  // it for the first\n  event.metadata.dispatchType = this.metadataStream.dispatchType;\n\n  // Reset stream state\n  this.videoTrack = null;\n  this.audioTrack = null;\n  this.videoTags = [];\n  this.audioTags = [];\n  this.pendingCaptions.length = 0;\n  this.pendingMetadata.length = 0;\n  this.pendingTracks = 0;\n  this.processedTracks = 0;\n\n  // Emit the final segment\n  this.trigger('data', event);\n\n  this.trigger('done');\n};\n\nmodule.exports = CoalesceStream;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/flv/coalesce-stream.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/flv/flv-header.js":
/*!***************************************************!*\
  !*** ./node_modules/mux.js/lib/flv/flv-header.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar FlvTag = __webpack_require__(/*! ./flv-tag.js */ \"./node_modules/mux.js/lib/flv/flv-tag.js\");\n\n// For information on the FLV format, see\n// http://download.macromedia.com/f4v/video_file_format_spec_v10_1.pdf.\n// Technically, this function returns the header and a metadata FLV tag\n// if duration is greater than zero\n// duration in seconds\n// @return {object} the bytes of the FLV header as a Uint8Array\nvar getFlvHeader = function(duration, audio, video) { // :ByteArray {\n  var\n    headBytes = new Uint8Array(3 + 1 + 1 + 4),\n    head = new DataView(headBytes.buffer),\n    metadata,\n    result,\n    metadataLength;\n\n  // default arguments\n  duration = duration || 0;\n  audio = audio === undefined ? true : audio;\n  video = video === undefined ? true : video;\n\n  // signature\n  head.setUint8(0, 0x46); // 'F'\n  head.setUint8(1, 0x4c); // 'L'\n  head.setUint8(2, 0x56); // 'V'\n\n  // version\n  head.setUint8(3, 0x01);\n\n  // flags\n  head.setUint8(4, (audio ? 0x04 : 0x00) | (video ? 0x01 : 0x00));\n\n  // data offset, should be 9 for FLV v1\n  head.setUint32(5, headBytes.byteLength);\n\n  // init the first FLV tag\n  if (duration <= 0) {\n    // no duration available so just write the first field of the first\n    // FLV tag\n    result = new Uint8Array(headBytes.byteLength + 4);\n    result.set(headBytes);\n    result.set([0, 0, 0, 0], headBytes.byteLength);\n    return result;\n  }\n\n  // write out the duration metadata tag\n  metadata = new FlvTag(FlvTag.METADATA_TAG);\n  metadata.pts = metadata.dts = 0;\n  metadata.writeMetaDataDouble('duration', duration);\n  metadataLength = metadata.finalize().length;\n  result = new Uint8Array(headBytes.byteLength + metadataLength);\n  result.set(headBytes);\n  result.set(head.byteLength, metadataLength);\n\n  return result;\n};\n\nmodule.exports = getFlvHeader;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/flv/flv-header.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/flv/flv-tag.js":
/*!************************************************!*\
  !*** ./node_modules/mux.js/lib/flv/flv-tag.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * An object that stores the bytes of an FLV tag and methods for\n * querying and manipulating that data.\n * @see http://download.macromedia.com/f4v/video_file_format_spec_v10_1.pdf\n */\n\n\nvar FlvTag;\n\n// (type:uint, extraData:Boolean = false) extends ByteArray\nFlvTag = function(type, extraData) {\n  var\n    // Counter if this is a metadata tag, nal start marker if this is a video\n    // tag. unused if this is an audio tag\n    adHoc = 0, // :uint\n\n    // The default size is 16kb but this is not enough to hold iframe\n    // data and the resizing algorithm costs a bit so we create a larger\n    // starting buffer for video tags\n    bufferStartSize = 16384,\n\n    // checks whether the FLV tag has enough capacity to accept the proposed\n    // write and re-allocates the internal buffers if necessary\n    prepareWrite = function(flv, count) {\n      var\n        bytes,\n        minLength = flv.position + count;\n      if (minLength < flv.bytes.byteLength) {\n        // there's enough capacity so do nothing\n        return;\n      }\n\n      // allocate a new buffer and copy over the data that will not be modified\n      bytes = new Uint8Array(minLength * 2);\n      bytes.set(flv.bytes.subarray(0, flv.position), 0);\n      flv.bytes = bytes;\n      flv.view = new DataView(flv.bytes.buffer);\n    },\n\n    // commonly used metadata properties\n    widthBytes = FlvTag.widthBytes || new Uint8Array('width'.length),\n    heightBytes = FlvTag.heightBytes || new Uint8Array('height'.length),\n    videocodecidBytes = FlvTag.videocodecidBytes || new Uint8Array('videocodecid'.length),\n    i;\n\n  if (!FlvTag.widthBytes) {\n    // calculating the bytes of common metadata names ahead of time makes the\n    // corresponding writes faster because we don't have to loop over the\n    // characters\n    // re-test with test/perf.html if you're planning on changing this\n    for (i = 0; i < 'width'.length; i++) {\n      widthBytes[i] = 'width'.charCodeAt(i);\n    }\n    for (i = 0; i < 'height'.length; i++) {\n      heightBytes[i] = 'height'.charCodeAt(i);\n    }\n    for (i = 0; i < 'videocodecid'.length; i++) {\n      videocodecidBytes[i] = 'videocodecid'.charCodeAt(i);\n    }\n\n    FlvTag.widthBytes = widthBytes;\n    FlvTag.heightBytes = heightBytes;\n    FlvTag.videocodecidBytes = videocodecidBytes;\n  }\n\n  this.keyFrame = false; // :Boolean\n\n  switch (type) {\n  case FlvTag.VIDEO_TAG:\n    this.length = 16;\n    // Start the buffer at 256k\n    bufferStartSize *= 6;\n    break;\n  case FlvTag.AUDIO_TAG:\n    this.length = 13;\n    this.keyFrame = true;\n    break;\n  case FlvTag.METADATA_TAG:\n    this.length = 29;\n    this.keyFrame = true;\n    break;\n  default:\n    throw new Error('Unknown FLV tag type');\n  }\n\n  this.bytes = new Uint8Array(bufferStartSize);\n  this.view = new DataView(this.bytes.buffer);\n  this.bytes[0] = type;\n  this.position = this.length;\n  this.keyFrame = extraData; // Defaults to false\n\n  // presentation timestamp\n  this.pts = 0;\n  // decoder timestamp\n  this.dts = 0;\n\n  // ByteArray#writeBytes(bytes:ByteArray, offset:uint = 0, length:uint = 0)\n  this.writeBytes = function(bytes, offset, length) {\n    var\n      start = offset || 0,\n      end;\n    length = length || bytes.byteLength;\n    end = start + length;\n\n    prepareWrite(this, length);\n    this.bytes.set(bytes.subarray(start, end), this.position);\n\n    this.position += length;\n    this.length = Math.max(this.length, this.position);\n  };\n\n  // ByteArray#writeByte(value:int):void\n  this.writeByte = function(byte) {\n    prepareWrite(this, 1);\n    this.bytes[this.position] = byte;\n    this.position++;\n    this.length = Math.max(this.length, this.position);\n  };\n\n  // ByteArray#writeShort(value:int):void\n  this.writeShort = function(short) {\n    prepareWrite(this, 2);\n    this.view.setUint16(this.position, short);\n    this.position += 2;\n    this.length = Math.max(this.length, this.position);\n  };\n\n  // Negative index into array\n  // (pos:uint):int\n  this.negIndex = function(pos) {\n    return this.bytes[this.length - pos];\n  };\n\n  // The functions below ONLY work when this[0] == VIDEO_TAG.\n  // We are not going to check for that because we dont want the overhead\n  // (nal:ByteArray = null):int\n  this.nalUnitSize = function() {\n    if (adHoc === 0) {\n      return 0;\n    }\n\n    return this.length - (adHoc + 4);\n  };\n\n  this.startNalUnit = function() {\n    // remember position and add 4 bytes\n    if (adHoc > 0) {\n      throw new Error('Attempted to create new NAL wihout closing the old one');\n    }\n\n    // reserve 4 bytes for nal unit size\n    adHoc = this.length;\n    this.length += 4;\n    this.position = this.length;\n  };\n\n  // (nal:ByteArray = null):void\n  this.endNalUnit = function(nalContainer) {\n    var\n      nalStart, // :uint\n      nalLength; // :uint\n\n    // Rewind to the marker and write the size\n    if (this.length === adHoc + 4) {\n      // we started a nal unit, but didnt write one, so roll back the 4 byte size value\n      this.length -= 4;\n    } else if (adHoc > 0) {\n      nalStart = adHoc + 4;\n      nalLength = this.length - nalStart;\n\n      this.position = adHoc;\n      this.view.setUint32(this.position, nalLength);\n      this.position = this.length;\n\n      if (nalContainer) {\n        // Add the tag to the NAL unit\n        nalContainer.push(this.bytes.subarray(nalStart, nalStart + nalLength));\n      }\n    }\n\n    adHoc = 0;\n  };\n\n  /**\n   * Write out a 64-bit floating point valued metadata property. This method is\n   * called frequently during a typical parse and needs to be fast.\n   */\n  // (key:String, val:Number):void\n  this.writeMetaDataDouble = function(key, val) {\n    var i;\n    prepareWrite(this, 2 + key.length + 9);\n\n    // write size of property name\n    this.view.setUint16(this.position, key.length);\n    this.position += 2;\n\n    // this next part looks terrible but it improves parser throughput by\n    // 10kB/s in my testing\n\n    // write property name\n    if (key === 'width') {\n      this.bytes.set(widthBytes, this.position);\n      this.position += 5;\n    } else if (key === 'height') {\n      this.bytes.set(heightBytes, this.position);\n      this.position += 6;\n    } else if (key === 'videocodecid') {\n      this.bytes.set(videocodecidBytes, this.position);\n      this.position += 12;\n    } else {\n      for (i = 0; i < key.length; i++) {\n        this.bytes[this.position] = key.charCodeAt(i);\n        this.position++;\n      }\n    }\n\n    // skip null byte\n    this.position++;\n\n    // write property value\n    this.view.setFloat64(this.position, val);\n    this.position += 8;\n\n    // update flv tag length\n    this.length = Math.max(this.length, this.position);\n    ++adHoc;\n  };\n\n  // (key:String, val:Boolean):void\n  this.writeMetaDataBoolean = function(key, val) {\n    var i;\n    prepareWrite(this, 2);\n    this.view.setUint16(this.position, key.length);\n    this.position += 2;\n    for (i = 0; i < key.length; i++) {\n      // if key.charCodeAt(i) >= 255, handle error\n      prepareWrite(this, 1);\n      this.bytes[this.position] = key.charCodeAt(i);\n      this.position++;\n    }\n    prepareWrite(this, 2);\n    this.view.setUint8(this.position, 0x01);\n    this.position++;\n    this.view.setUint8(this.position, val ? 0x01 : 0x00);\n    this.position++;\n    this.length = Math.max(this.length, this.position);\n    ++adHoc;\n  };\n\n  // ():ByteArray\n  this.finalize = function() {\n    var\n      dtsDelta, // :int\n      len; // :int\n\n    switch (this.bytes[0]) {\n      // Video Data\n    case FlvTag.VIDEO_TAG:\n       // We only support AVC, 1 = key frame (for AVC, a seekable\n       // frame), 2 = inter frame (for AVC, a non-seekable frame)\n      this.bytes[11] = ((this.keyFrame || extraData) ? 0x10 : 0x20) | 0x07;\n      this.bytes[12] = extraData ?  0x00 : 0x01;\n\n      dtsDelta = this.pts - this.dts;\n      this.bytes[13] = (dtsDelta & 0x00FF0000) >>> 16;\n      this.bytes[14] = (dtsDelta & 0x0000FF00) >>>  8;\n      this.bytes[15] = (dtsDelta & 0x000000FF) >>>  0;\n      break;\n\n    case FlvTag.AUDIO_TAG:\n      this.bytes[11] = 0xAF; // 44 kHz, 16-bit stereo\n      this.bytes[12] = extraData ? 0x00 : 0x01;\n      break;\n\n    case FlvTag.METADATA_TAG:\n      this.position = 11;\n      this.view.setUint8(this.position, 0x02); // String type\n      this.position++;\n      this.view.setUint16(this.position, 0x0A); // 10 Bytes\n      this.position += 2;\n      // set \"onMetaData\"\n      this.bytes.set([0x6f, 0x6e, 0x4d, 0x65,\n                      0x74, 0x61, 0x44, 0x61,\n                      0x74, 0x61], this.position);\n      this.position += 10;\n      this.bytes[this.position] = 0x08; // Array type\n      this.position++;\n      this.view.setUint32(this.position, adHoc);\n      this.position = this.length;\n      this.bytes.set([0, 0, 9], this.position);\n      this.position += 3; // End Data Tag\n      this.length = this.position;\n      break;\n    }\n\n    len = this.length - 11;\n\n    // write the DataSize field\n    this.bytes[ 1] = (len & 0x00FF0000) >>> 16;\n    this.bytes[ 2] = (len & 0x0000FF00) >>>  8;\n    this.bytes[ 3] = (len & 0x000000FF) >>>  0;\n    // write the Timestamp\n    this.bytes[ 4] = (this.dts & 0x00FF0000) >>> 16;\n    this.bytes[ 5] = (this.dts & 0x0000FF00) >>>  8;\n    this.bytes[ 6] = (this.dts & 0x000000FF) >>>  0;\n    this.bytes[ 7] = (this.dts & 0xFF000000) >>> 24;\n    // write the StreamID\n    this.bytes[ 8] = 0;\n    this.bytes[ 9] = 0;\n    this.bytes[10] = 0;\n\n    // Sometimes we're at the end of the view and have one slot to write a\n    // uint32, so, prepareWrite of count 4, since, view is uint8\n    prepareWrite(this, 4);\n    this.view.setUint32(this.length, this.length);\n    this.length += 4;\n    this.position += 4;\n\n    // trim down the byte buffer to what is actually being used\n    this.bytes = this.bytes.subarray(0, this.length);\n    this.frameTime = FlvTag.frameTime(this.bytes);\n    // if bytes.bytelength isn't equal to this.length, handle error\n    return this;\n  };\n};\n\nFlvTag.AUDIO_TAG = 0x08; // == 8, :uint\nFlvTag.VIDEO_TAG = 0x09; // == 9, :uint\nFlvTag.METADATA_TAG = 0x12; // == 18, :uint\n\n// (tag:ByteArray):Boolean {\nFlvTag.isAudioFrame = function(tag) {\n  return FlvTag.AUDIO_TAG === tag[0];\n};\n\n// (tag:ByteArray):Boolean {\nFlvTag.isVideoFrame = function(tag) {\n  return FlvTag.VIDEO_TAG === tag[0];\n};\n\n// (tag:ByteArray):Boolean {\nFlvTag.isMetaData = function(tag) {\n  return FlvTag.METADATA_TAG === tag[0];\n};\n\n// (tag:ByteArray):Boolean {\nFlvTag.isKeyFrame = function(tag) {\n  if (FlvTag.isVideoFrame(tag)) {\n    return tag[11] === 0x17;\n  }\n\n  if (FlvTag.isAudioFrame(tag)) {\n    return true;\n  }\n\n  if (FlvTag.isMetaData(tag)) {\n    return true;\n  }\n\n  return false;\n};\n\n// (tag:ByteArray):uint {\nFlvTag.frameTime = function(tag) {\n  var pts = tag[ 4] << 16; // :uint\n  pts |= tag[ 5] <<  8;\n  pts |= tag[ 6] <<  0;\n  pts |= tag[ 7] << 24;\n  return pts;\n};\n\nmodule.exports = FlvTag;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/flv/flv-tag.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/flv/index.js":
/*!**********************************************!*\
  !*** ./node_modules/mux.js/lib/flv/index.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("module.exports = {\n  tag: __webpack_require__(/*! ./flv-tag */ \"./node_modules/mux.js/lib/flv/flv-tag.js\"),\n  Transmuxer: __webpack_require__(/*! ./transmuxer */ \"./node_modules/mux.js/lib/flv/transmuxer.js\"),\n  getFlvHeader: __webpack_require__(/*! ./flv-header */ \"./node_modules/mux.js/lib/flv/flv-header.js\")\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/flv/index.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/flv/tag-list.js":
/*!*************************************************!*\
  !*** ./node_modules/mux.js/lib/flv/tag-list.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar TagList = function() {\n  var self = this;\n\n  this.list = [];\n\n  this.push = function(tag) {\n    this.list.push({\n      bytes: tag.bytes,\n      dts: tag.dts,\n      pts: tag.pts,\n      keyFrame: tag.keyFrame,\n      metaDataTag: tag.metaDataTag\n    });\n  };\n\n  Object.defineProperty(this, 'length', {\n    get: function() {\n      return self.list.length;\n    }\n  });\n};\n\nmodule.exports = TagList;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/flv/tag-list.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/flv/transmuxer.js":
/*!***************************************************!*\
  !*** ./node_modules/mux.js/lib/flv/transmuxer.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\");\nvar FlvTag = __webpack_require__(/*! ./flv-tag.js */ \"./node_modules/mux.js/lib/flv/flv-tag.js\");\nvar m2ts = __webpack_require__(/*! ../m2ts/m2ts.js */ \"./node_modules/mux.js/lib/m2ts/m2ts.js\");\nvar AdtsStream = __webpack_require__(/*! ../codecs/adts.js */ \"./node_modules/mux.js/lib/codecs/adts.js\");\nvar H264Stream = __webpack_require__(/*! ../codecs/h264 */ \"./node_modules/mux.js/lib/codecs/h264.js\").H264Stream;\nvar CoalesceStream = __webpack_require__(/*! ./coalesce-stream.js */ \"./node_modules/mux.js/lib/flv/coalesce-stream.js\");\nvar TagList = __webpack_require__(/*! ./tag-list.js */ \"./node_modules/mux.js/lib/flv/tag-list.js\");\n\nvar\n  Transmuxer,\n  VideoSegmentStream,\n  AudioSegmentStream,\n  collectTimelineInfo,\n  metaDataTag,\n  extraDataTag;\n\n/**\n * Store information about the start and end of the tracka and the\n * duration for each frame/sample we process in order to calculate\n * the baseMediaDecodeTime\n */\ncollectTimelineInfo = function(track, data) {\n  if (typeof data.pts === 'number') {\n    if (track.timelineStartInfo.pts === undefined) {\n      track.timelineStartInfo.pts = data.pts;\n    } else {\n      track.timelineStartInfo.pts =\n        Math.min(track.timelineStartInfo.pts, data.pts);\n    }\n  }\n\n  if (typeof data.dts === 'number') {\n    if (track.timelineStartInfo.dts === undefined) {\n      track.timelineStartInfo.dts = data.dts;\n    } else {\n      track.timelineStartInfo.dts =\n        Math.min(track.timelineStartInfo.dts, data.dts);\n    }\n  }\n};\n\nmetaDataTag = function(track, pts) {\n  var\n    tag = new FlvTag(FlvTag.METADATA_TAG); // :FlvTag\n\n  tag.dts = pts;\n  tag.pts = pts;\n\n  tag.writeMetaDataDouble('videocodecid', 7);\n  tag.writeMetaDataDouble('width', track.width);\n  tag.writeMetaDataDouble('height', track.height);\n\n  return tag;\n};\n\nextraDataTag = function(track, pts) {\n  var\n    i,\n    tag = new FlvTag(FlvTag.VIDEO_TAG, true);\n\n  tag.dts = pts;\n  tag.pts = pts;\n\n  tag.writeByte(0x01);// version\n  tag.writeByte(track.profileIdc);// profile\n  tag.writeByte(track.profileCompatibility);// compatibility\n  tag.writeByte(track.levelIdc);// level\n  tag.writeByte(0xFC | 0x03); // reserved (6 bits), NULA length size - 1 (2 bits)\n  tag.writeByte(0xE0 | 0x01); // reserved (3 bits), num of SPS (5 bits)\n  tag.writeShort(track.sps[0].length); // data of SPS\n  tag.writeBytes(track.sps[0]); // SPS\n\n  tag.writeByte(track.pps.length); // num of PPS (will there ever be more that 1 PPS?)\n  for (i = 0; i < track.pps.length; ++i) {\n    tag.writeShort(track.pps[i].length); // 2 bytes for length of PPS\n    tag.writeBytes(track.pps[i]); // data of PPS\n  }\n\n  return tag;\n};\n\n/**\n * Constructs a single-track, media segment from AAC data\n * events. The output of this stream can be fed to flash.\n */\nAudioSegmentStream = function(track) {\n  var\n    adtsFrames = [],\n    videoKeyFrames = [],\n    oldExtraData;\n\n  AudioSegmentStream.prototype.init.call(this);\n\n  this.push = function(data) {\n    collectTimelineInfo(track, data);\n\n    if (track) {\n      track.audioobjecttype = data.audioobjecttype;\n      track.channelcount = data.channelcount;\n      track.samplerate = data.samplerate;\n      track.samplingfrequencyindex = data.samplingfrequencyindex;\n      track.samplesize = data.samplesize;\n      track.extraData = (track.audioobjecttype << 11) |\n                        (track.samplingfrequencyindex << 7) |\n                        (track.channelcount << 3);\n    }\n\n    data.pts = Math.round(data.pts / 90);\n    data.dts = Math.round(data.dts / 90);\n\n    // buffer audio data until end() is called\n    adtsFrames.push(data);\n  };\n\n  this.flush = function() {\n    var currentFrame, adtsFrame, lastMetaPts, tags = new TagList();\n    // return early if no audio data has been observed\n    if (adtsFrames.length === 0) {\n      this.trigger('done', 'AudioSegmentStream');\n      return;\n    }\n\n    lastMetaPts = -Infinity;\n\n    while (adtsFrames.length) {\n      currentFrame = adtsFrames.shift();\n\n      // write out a metadata frame at every video key frame\n      if (videoKeyFrames.length && currentFrame.pts >= videoKeyFrames[0]) {\n        lastMetaPts = videoKeyFrames.shift();\n        this.writeMetaDataTags(tags, lastMetaPts);\n      }\n\n      // also write out metadata tags every 1 second so that the decoder\n      // is re-initialized quickly after seeking into a different\n      // audio configuration.\n      if (track.extraData !== oldExtraData || currentFrame.pts - lastMetaPts >= 1000) {\n        this.writeMetaDataTags(tags, currentFrame.pts);\n        oldExtraData = track.extraData;\n        lastMetaPts = currentFrame.pts;\n      }\n\n      adtsFrame = new FlvTag(FlvTag.AUDIO_TAG);\n      adtsFrame.pts = currentFrame.pts;\n      adtsFrame.dts = currentFrame.dts;\n\n      adtsFrame.writeBytes(currentFrame.data);\n\n      tags.push(adtsFrame.finalize());\n    }\n\n    videoKeyFrames.length = 0;\n    oldExtraData = null;\n    this.trigger('data', {track: track, tags: tags.list});\n\n    this.trigger('done', 'AudioSegmentStream');\n  };\n\n  this.writeMetaDataTags = function(tags, pts) {\n    var adtsFrame;\n\n    adtsFrame = new FlvTag(FlvTag.METADATA_TAG);\n    // For audio, DTS is always the same as PTS. We want to set the DTS\n    // however so we can compare with video DTS to determine approximate\n    // packet order\n    adtsFrame.pts = pts;\n    adtsFrame.dts = pts;\n\n    // AAC is always 10\n    adtsFrame.writeMetaDataDouble('audiocodecid', 10);\n    adtsFrame.writeMetaDataBoolean('stereo', track.channelcount === 2);\n    adtsFrame.writeMetaDataDouble('audiosamplerate', track.samplerate);\n    // Is AAC always 16 bit?\n    adtsFrame.writeMetaDataDouble('audiosamplesize', 16);\n\n    tags.push(adtsFrame.finalize());\n\n    adtsFrame = new FlvTag(FlvTag.AUDIO_TAG, true);\n    // For audio, DTS is always the same as PTS. We want to set the DTS\n    // however so we can compare with video DTS to determine approximate\n    // packet order\n    adtsFrame.pts = pts;\n    adtsFrame.dts = pts;\n\n    adtsFrame.view.setUint16(adtsFrame.position, track.extraData);\n    adtsFrame.position += 2;\n    adtsFrame.length = Math.max(adtsFrame.length, adtsFrame.position);\n\n    tags.push(adtsFrame.finalize());\n  };\n\n  this.onVideoKeyFrame = function(pts) {\n    videoKeyFrames.push(pts);\n  };\n};\nAudioSegmentStream.prototype = new Stream();\n\n/**\n * Store FlvTags for the h264 stream\n * @param track {object} track metadata configuration\n */\nVideoSegmentStream = function(track) {\n  var\n    nalUnits = [],\n    config,\n    h264Frame;\n  VideoSegmentStream.prototype.init.call(this);\n\n  this.finishFrame = function(tags, frame) {\n    if (!frame) {\n      return;\n    }\n    // Check if keyframe and the length of tags.\n    // This makes sure we write metadata on the first frame of a segment.\n    if (config && track && track.newMetadata &&\n        (frame.keyFrame || tags.length === 0)) {\n      // Push extra data on every IDR frame in case we did a stream change + seek\n      var metaTag = metaDataTag(config, frame.dts).finalize();\n      var extraTag = extraDataTag(track, frame.dts).finalize();\n\n      metaTag.metaDataTag = extraTag.metaDataTag = true;\n\n      tags.push(metaTag);\n      tags.push(extraTag);\n      track.newMetadata = false;\n\n      this.trigger('keyframe', frame.dts);\n    }\n\n    frame.endNalUnit();\n    tags.push(frame.finalize());\n    h264Frame = null;\n  };\n\n  this.push = function(data) {\n    collectTimelineInfo(track, data);\n\n    data.pts = Math.round(data.pts / 90);\n    data.dts = Math.round(data.dts / 90);\n\n    // buffer video until flush() is called\n    nalUnits.push(data);\n  };\n\n  this.flush = function() {\n    var\n      currentNal,\n      tags = new TagList();\n\n    // Throw away nalUnits at the start of the byte stream until we find\n    // the first AUD\n    while (nalUnits.length) {\n      if (nalUnits[0].nalUnitType === 'access_unit_delimiter_rbsp') {\n        break;\n      }\n      nalUnits.shift();\n    }\n\n    // return early if no video data has been observed\n    if (nalUnits.length === 0) {\n      this.trigger('done', 'VideoSegmentStream');\n      return;\n    }\n\n    while (nalUnits.length) {\n      currentNal = nalUnits.shift();\n\n      // record the track config\n      if (currentNal.nalUnitType === 'seq_parameter_set_rbsp') {\n        track.newMetadata = true;\n        config = currentNal.config;\n        track.width = config.width;\n        track.height = config.height;\n        track.sps = [currentNal.data];\n        track.profileIdc = config.profileIdc;\n        track.levelIdc = config.levelIdc;\n        track.profileCompatibility = config.profileCompatibility;\n        h264Frame.endNalUnit();\n      } else if (currentNal.nalUnitType === 'pic_parameter_set_rbsp') {\n        track.newMetadata = true;\n        track.pps = [currentNal.data];\n        h264Frame.endNalUnit();\n      } else if (currentNal.nalUnitType === 'access_unit_delimiter_rbsp') {\n        if (h264Frame) {\n          this.finishFrame(tags, h264Frame);\n        }\n        h264Frame = new FlvTag(FlvTag.VIDEO_TAG);\n        h264Frame.pts = currentNal.pts;\n        h264Frame.dts = currentNal.dts;\n      } else {\n        if (currentNal.nalUnitType === 'slice_layer_without_partitioning_rbsp_idr') {\n          // the current sample is a key frame\n          h264Frame.keyFrame = true;\n        }\n        h264Frame.endNalUnit();\n      }\n      h264Frame.startNalUnit();\n      h264Frame.writeBytes(currentNal.data);\n    }\n    if (h264Frame) {\n      this.finishFrame(tags, h264Frame);\n    }\n\n    this.trigger('data', {track: track, tags: tags.list});\n\n    // Continue with the flush process now\n    this.trigger('done', 'VideoSegmentStream');\n  };\n};\n\nVideoSegmentStream.prototype = new Stream();\n\n/**\n * An object that incrementally transmuxes MPEG2 Trasport Stream\n * chunks into an FLV.\n */\nTransmuxer = function(options) {\n  var\n    self = this,\n\n    packetStream, parseStream, elementaryStream,\n    videoTimestampRolloverStream, audioTimestampRolloverStream,\n    timedMetadataTimestampRolloverStream,\n    adtsStream, h264Stream,\n    videoSegmentStream, audioSegmentStream, captionStream,\n    coalesceStream;\n\n  Transmuxer.prototype.init.call(this);\n\n  options = options || {};\n\n  // expose the metadata stream\n  this.metadataStream = new m2ts.MetadataStream();\n\n  options.metadataStream = this.metadataStream;\n\n  // set up the parsing pipeline\n  packetStream = new m2ts.TransportPacketStream();\n  parseStream = new m2ts.TransportParseStream();\n  elementaryStream = new m2ts.ElementaryStream();\n  videoTimestampRolloverStream = new m2ts.TimestampRolloverStream('video');\n  audioTimestampRolloverStream = new m2ts.TimestampRolloverStream('audio');\n  timedMetadataTimestampRolloverStream = new m2ts.TimestampRolloverStream('timed-metadata');\n\n  adtsStream = new AdtsStream();\n  h264Stream = new H264Stream();\n  coalesceStream = new CoalesceStream(options);\n\n  // disassemble MPEG2-TS packets into elementary streams\n  packetStream\n    .pipe(parseStream)\n    .pipe(elementaryStream);\n\n  // !!THIS ORDER IS IMPORTANT!!\n  // demux the streams\n  elementaryStream\n    .pipe(videoTimestampRolloverStream)\n    .pipe(h264Stream);\n  elementaryStream\n    .pipe(audioTimestampRolloverStream)\n    .pipe(adtsStream);\n\n  elementaryStream\n    .pipe(timedMetadataTimestampRolloverStream)\n    .pipe(this.metadataStream)\n    .pipe(coalesceStream);\n  // if CEA-708 parsing is available, hook up a caption stream\n  captionStream = new m2ts.CaptionStream();\n  h264Stream.pipe(captionStream)\n    .pipe(coalesceStream);\n\n  // hook up the segment streams once track metadata is delivered\n  elementaryStream.on('data', function(data) {\n    var i, videoTrack, audioTrack;\n\n    if (data.type === 'metadata') {\n      i = data.tracks.length;\n\n      // scan the tracks listed in the metadata\n      while (i--) {\n        if (data.tracks[i].type === 'video') {\n          videoTrack = data.tracks[i];\n        } else if (data.tracks[i].type === 'audio') {\n          audioTrack = data.tracks[i];\n        }\n      }\n\n      // hook up the video segment stream to the first track with h264 data\n      if (videoTrack && !videoSegmentStream) {\n        coalesceStream.numberOfTracks++;\n        videoSegmentStream = new VideoSegmentStream(videoTrack);\n\n        // Set up the final part of the video pipeline\n        h264Stream\n          .pipe(videoSegmentStream)\n          .pipe(coalesceStream);\n      }\n\n      if (audioTrack && !audioSegmentStream) {\n        // hook up the audio segment stream to the first track with aac data\n        coalesceStream.numberOfTracks++;\n        audioSegmentStream = new AudioSegmentStream(audioTrack);\n\n        // Set up the final part of the audio pipeline\n        adtsStream\n          .pipe(audioSegmentStream)\n          .pipe(coalesceStream);\n\n        if (videoSegmentStream) {\n          videoSegmentStream.on('keyframe', audioSegmentStream.onVideoKeyFrame);\n        }\n      }\n    }\n  });\n\n  // feed incoming data to the front of the parsing pipeline\n  this.push = function(data) {\n    packetStream.push(data);\n  };\n\n  // flush any buffered data\n  this.flush = function() {\n    // Start at the top of the pipeline and flush all pending work\n    packetStream.flush();\n  };\n\n  // Caption data has to be reset when seeking outside buffered range\n  this.resetCaptions = function() {\n    captionStream.reset();\n  };\n\n  // Re-emit any data coming from the coalesce stream to the outside world\n  coalesceStream.on('data', function(event) {\n    self.trigger('data', event);\n  });\n\n  // Let the consumer know we have finished flushing the entire pipeline\n  coalesceStream.on('done', function() {\n    self.trigger('done');\n  });\n};\nTransmuxer.prototype = new Stream();\n\n// forward compatibility\nmodule.exports = Transmuxer;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/flv/transmuxer.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/m2ts/caption-stream.js":
/*!********************************************************!*\
  !*** ./node_modules/mux.js/lib/m2ts/caption-stream.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2015 Brightcove\n * All rights reserved.\n *\n * Reads in-band caption information from a video elementary\n * stream. Captions must follow the CEA-708 standard for injection\n * into an MPEG-2 transport streams.\n * @see https://en.wikipedia.org/wiki/CEA-708\n * @see https://www.gpo.gov/fdsys/pkg/CFR-2007-title47-vol1/pdf/CFR-2007-title47-vol1-sec15-119.pdf\n */\n\n\n\n// -----------------\n// Link To Transport\n// -----------------\n\n// Supplemental enhancement information (SEI) NAL units have a\n// payload type field to indicate how they are to be\n// interpreted. CEAS-708 caption content is always transmitted with\n// payload type 0x04.\nvar USER_DATA_REGISTERED_ITU_T_T35 = 4,\n    RBSP_TRAILING_BITS = 128,\n    Stream = __webpack_require__(/*! ../utils/stream */ \"./node_modules/mux.js/lib/utils/stream.js\");\n\n/**\n  * Parse a supplemental enhancement information (SEI) NAL unit.\n  * Stops parsing once a message of type ITU T T35 has been found.\n  *\n  * @param bytes {Uint8Array} the bytes of a SEI NAL unit\n  * @return {object} the parsed SEI payload\n  * @see Rec. ITU-T H.264, 7.3.2.3.1\n  */\nvar parseSei = function(bytes) {\n  var\n    i = 0,\n    result = {\n      payloadType: -1,\n      payloadSize: 0\n    },\n    payloadType = 0,\n    payloadSize = 0;\n\n  // go through the sei_rbsp parsing each each individual sei_message\n  while (i < bytes.byteLength) {\n    // stop once we have hit the end of the sei_rbsp\n    if (bytes[i] === RBSP_TRAILING_BITS) {\n      break;\n    }\n\n    // Parse payload type\n    while (bytes[i] === 0xFF) {\n      payloadType += 255;\n      i++;\n    }\n    payloadType += bytes[i++];\n\n    // Parse payload size\n    while (bytes[i] === 0xFF) {\n      payloadSize += 255;\n      i++;\n    }\n    payloadSize += bytes[i++];\n\n    // this sei_message is a 608/708 caption so save it and break\n    // there can only ever be one caption message in a frame's sei\n    if (!result.payload && payloadType === USER_DATA_REGISTERED_ITU_T_T35) {\n      result.payloadType = payloadType;\n      result.payloadSize = payloadSize;\n      result.payload = bytes.subarray(i, i + payloadSize);\n      break;\n    }\n\n    // skip the payload and parse the next message\n    i += payloadSize;\n    payloadType = 0;\n    payloadSize = 0;\n  }\n\n  return result;\n};\n\n// see ANSI/SCTE 128-1 (2013), section 8.1\nvar parseUserData = function(sei) {\n  // itu_t_t35_contry_code must be 181 (United States) for\n  // captions\n  if (sei.payload[0] !== 181) {\n    return null;\n  }\n\n  // itu_t_t35_provider_code should be 49 (ATSC) for captions\n  if (((sei.payload[1] << 8) | sei.payload[2]) !== 49) {\n    return null;\n  }\n\n  // the user_identifier should be \"GA94\" to indicate ATSC1 data\n  if (String.fromCharCode(sei.payload[3],\n                          sei.payload[4],\n                          sei.payload[5],\n                          sei.payload[6]) !== 'GA94') {\n    return null;\n  }\n\n  // finally, user_data_type_code should be 0x03 for caption data\n  if (sei.payload[7] !== 0x03) {\n    return null;\n  }\n\n  // return the user_data_type_structure and strip the trailing\n  // marker bits\n  return sei.payload.subarray(8, sei.payload.length - 1);\n};\n\n// see CEA-708-D, section 4.4\nvar parseCaptionPackets = function(pts, userData) {\n  var results = [], i, count, offset, data;\n\n  // if this is just filler, return immediately\n  if (!(userData[0] & 0x40)) {\n    return results;\n  }\n\n  // parse out the cc_data_1 and cc_data_2 fields\n  count = userData[0] & 0x1f;\n  for (i = 0; i < count; i++) {\n    offset = i * 3;\n    data = {\n      type: userData[offset + 2] & 0x03,\n      pts: pts\n    };\n\n    // capture cc data when cc_valid is 1\n    if (userData[offset + 2] & 0x04) {\n      data.ccData = (userData[offset + 3] << 8) | userData[offset + 4];\n      results.push(data);\n    }\n  }\n  return results;\n};\n\nvar CaptionStream = function() {\n\n  CaptionStream.prototype.init.call(this);\n\n  this.captionPackets_ = [];\n\n  this.ccStreams_ = [\n    new Cea608Stream(0, 0), // eslint-disable-line no-use-before-define\n    new Cea608Stream(0, 1), // eslint-disable-line no-use-before-define\n    new Cea608Stream(1, 0), // eslint-disable-line no-use-before-define\n    new Cea608Stream(1, 1) // eslint-disable-line no-use-before-define\n  ];\n\n  this.reset();\n\n  // forward data and done events from CCs to this CaptionStream\n  this.ccStreams_.forEach(function(cc) {\n    cc.on('data', this.trigger.bind(this, 'data'));\n    cc.on('done', this.trigger.bind(this, 'done'));\n  }, this);\n\n};\n\nCaptionStream.prototype = new Stream();\nCaptionStream.prototype.push = function(event) {\n  var sei, userData;\n\n  // only examine SEI NALs\n  if (event.nalUnitType !== 'sei_rbsp') {\n    return;\n  }\n\n  // parse the sei\n  sei = parseSei(event.escapedRBSP);\n\n  // ignore everything but user_data_registered_itu_t_t35\n  if (sei.payloadType !== USER_DATA_REGISTERED_ITU_T_T35) {\n    return;\n  }\n\n  // parse out the user data payload\n  userData = parseUserData(sei);\n\n  // ignore unrecognized userData\n  if (!userData) {\n    return;\n  }\n\n  // Sometimes, the same segment # will be downloaded twice. To stop the\n  // caption data from being processed twice, we track the latest dts we've\n  // received and ignore everything with a dts before that. However, since\n  // data for a specific dts can be split across 2 packets on either side of\n  // a segment boundary, we need to make sure we *don't* ignore the second\n  // dts packet we receive that has dts === this.latestDts_. And thus, the\n  // ignoreNextEqualDts_ flag was born.\n  if (event.dts < this.latestDts_) {\n    // We've started getting older data, so set the flag.\n    this.ignoreNextEqualDts_ = true;\n    return;\n  } else if ((event.dts === this.latestDts_) && (this.ignoreNextEqualDts_)) {\n    // We've received the last duplicate packet, time to start processing again\n    this.ignoreNextEqualDts_ = false;\n    return;\n  }\n\n  // parse out CC data packets and save them for later\n  this.captionPackets_ = this.captionPackets_.concat(parseCaptionPackets(event.pts, userData));\n  this.latestDts_ = event.dts;\n};\n\nCaptionStream.prototype.flush = function() {\n  // make sure we actually parsed captions before proceeding\n  if (!this.captionPackets_.length) {\n    this.ccStreams_.forEach(function(cc) {\n      cc.flush();\n    }, this);\n    return;\n  }\n\n  // In Chrome, the Array#sort function is not stable so add a\n  // presortIndex that we can use to ensure we get a stable-sort\n  this.captionPackets_.forEach(function(elem, idx) {\n    elem.presortIndex = idx;\n  });\n\n  // sort caption byte-pairs based on their PTS values\n  this.captionPackets_.sort(function(a, b) {\n    if (a.pts === b.pts) {\n      return a.presortIndex - b.presortIndex;\n    }\n    return a.pts - b.pts;\n  });\n\n  this.captionPackets_.forEach(function(packet) {\n    if (packet.type < 2) {\n      // Dispatch packet to the right Cea608Stream\n      this.dispatchCea608Packet(packet);\n    }\n    // this is where an 'else' would go for a dispatching packets\n    // to a theoretical Cea708Stream that handles SERVICEn data\n  }, this);\n\n  this.captionPackets_.length = 0;\n  this.ccStreams_.forEach(function(cc) {\n    cc.flush();\n  }, this);\n  return;\n};\n\nCaptionStream.prototype.reset = function() {\n  this.latestDts_ = null;\n  this.ignoreNextEqualDts_ = false;\n  this.activeCea608Channel_ = [null, null];\n  this.ccStreams_.forEach(function(ccStream) {\n    ccStream.reset();\n  });\n};\n\nCaptionStream.prototype.dispatchCea608Packet = function(packet) {\n  // NOTE: packet.type is the CEA608 field\n  if (this.setsChannel1Active(packet)) {\n    this.activeCea608Channel_[packet.type] = 0;\n  } else if (this.setsChannel2Active(packet)) {\n    this.activeCea608Channel_[packet.type] = 1;\n  }\n  if (this.activeCea608Channel_[packet.type] === null) {\n    // If we haven't received anything to set the active channel, discard the\n    // data; we don't want jumbled captions\n    return;\n  }\n  this.ccStreams_[(packet.type << 1) + this.activeCea608Channel_[packet.type]].push(packet);\n};\n\nCaptionStream.prototype.setsChannel1Active = function(packet) {\n  return ((packet.ccData & 0x7800) === 0x1000);\n};\nCaptionStream.prototype.setsChannel2Active = function(packet) {\n  return ((packet.ccData & 0x7800) === 0x1800);\n};\n\n// ----------------------\n// Session to Application\n// ----------------------\n\nvar CHARACTER_TRANSLATION = {\n  0x2a: 0xe1,     // á\n  0x5c: 0xe9,     // é\n  0x5e: 0xed,     // í\n  0x5f: 0xf3,     // ó\n  0x60: 0xfa,     // ú\n  0x7b: 0xe7,     // ç\n  0x7c: 0xf7,     // ÷\n  0x7d: 0xd1,     // Ñ\n  0x7e: 0xf1,     // ñ\n  0x7f: 0x2588,   // █\n  0x0130: 0xae,   // ®\n  0x0131: 0xb0,   // °\n  0x0132: 0xbd,   // ½\n  0x0133: 0xbf,   // ¿\n  0x0134: 0x2122, // ™\n  0x0135: 0xa2,   // ¢\n  0x0136: 0xa3,   // £\n  0x0137: 0x266a, // ♪\n  0x0138: 0xe0,   // à\n  0x0139: 0xa0,   //\n  0x013a: 0xe8,   // è\n  0x013b: 0xe2,   // â\n  0x013c: 0xea,   // ê\n  0x013d: 0xee,   // î\n  0x013e: 0xf4,   // ô\n  0x013f: 0xfb,   // û\n  0x0220: 0xc1,   // Á\n  0x0221: 0xc9,   // É\n  0x0222: 0xd3,   // Ó\n  0x0223: 0xda,   // Ú\n  0x0224: 0xdc,   // Ü\n  0x0225: 0xfc,   // ü\n  0x0226: 0x2018, // ‘\n  0x0227: 0xa1,   // ¡\n  0x0228: 0x2a,   // *\n  0x0229: 0x27,   // '\n  0x022a: 0x2014, // —\n  0x022b: 0xa9,   // ©\n  0x022c: 0x2120, // ℠\n  0x022d: 0x2022, // •\n  0x022e: 0x201c, // “\n  0x022f: 0x201d, // ”\n  0x0230: 0xc0,   // À\n  0x0231: 0xc2,   // Â\n  0x0232: 0xc7,   // Ç\n  0x0233: 0xc8,   // È\n  0x0234: 0xca,   // Ê\n  0x0235: 0xcb,   // Ë\n  0x0236: 0xeb,   // ë\n  0x0237: 0xce,   // Î\n  0x0238: 0xcf,   // Ï\n  0x0239: 0xef,   // ï\n  0x023a: 0xd4,   // Ô\n  0x023b: 0xd9,   // Ù\n  0x023c: 0xf9,   // ù\n  0x023d: 0xdb,   // Û\n  0x023e: 0xab,   // «\n  0x023f: 0xbb,   // »\n  0x0320: 0xc3,   // Ã\n  0x0321: 0xe3,   // ã\n  0x0322: 0xcd,   // Í\n  0x0323: 0xcc,   // Ì\n  0x0324: 0xec,   // ì\n  0x0325: 0xd2,   // Ò\n  0x0326: 0xf2,   // ò\n  0x0327: 0xd5,   // Õ\n  0x0328: 0xf5,   // õ\n  0x0329: 0x7b,   // {\n  0x032a: 0x7d,   // }\n  0x032b: 0x5c,   // \\\n  0x032c: 0x5e,   // ^\n  0x032d: 0x5f,   // _\n  0x032e: 0x7c,   // |\n  0x032f: 0x7e,   // ~\n  0x0330: 0xc4,   // Ä\n  0x0331: 0xe4,   // ä\n  0x0332: 0xd6,   // Ö\n  0x0333: 0xf6,   // ö\n  0x0334: 0xdf,   // ß\n  0x0335: 0xa5,   // ¥\n  0x0336: 0xa4,   // ¤\n  0x0337: 0x2502, // │\n  0x0338: 0xc5,   // Å\n  0x0339: 0xe5,   // å\n  0x033a: 0xd8,   // Ø\n  0x033b: 0xf8,   // ø\n  0x033c: 0x250c, // ┌\n  0x033d: 0x2510, // ┐\n  0x033e: 0x2514, // └\n  0x033f: 0x2518  // ┘\n};\n\nvar getCharFromCode = function(code) {\n  if (code === null) {\n    return '';\n  }\n  code = CHARACTER_TRANSLATION[code] || code;\n  return String.fromCharCode(code);\n};\n\n// the index of the last row in a CEA-608 display buffer\nvar BOTTOM_ROW = 14;\n\n// This array is used for mapping PACs -> row #, since there's no way of\n// getting it through bit logic.\nvar ROWS = [0x1100, 0x1120, 0x1200, 0x1220, 0x1500, 0x1520, 0x1600, 0x1620,\n            0x1700, 0x1720, 0x1000, 0x1300, 0x1320, 0x1400, 0x1420];\n\n// CEA-608 captions are rendered onto a 34x15 matrix of character\n// cells. The \"bottom\" row is the last element in the outer array.\nvar createDisplayBuffer = function() {\n  var result = [], i = BOTTOM_ROW + 1;\n  while (i--) {\n    result.push('');\n  }\n  return result;\n};\n\nvar Cea608Stream = function(field, dataChannel) {\n  Cea608Stream.prototype.init.call(this);\n\n  this.field_ = field || 0;\n  this.dataChannel_ = dataChannel || 0;\n\n  this.name_ = 'CC' + (((this.field_ << 1) | this.dataChannel_) + 1);\n\n  this.setConstants();\n  this.reset();\n\n  this.push = function(packet) {\n    var data, swap, char0, char1, text;\n    // remove the parity bits\n    data = packet.ccData & 0x7f7f;\n\n    // ignore duplicate control codes; the spec demands they're sent twice\n    if (data === this.lastControlCode_) {\n      this.lastControlCode_ = null;\n      return;\n    }\n\n    // Store control codes\n    if ((data & 0xf000) === 0x1000) {\n      this.lastControlCode_ = data;\n    } else if (data !== this.PADDING_) {\n      this.lastControlCode_ = null;\n    }\n\n    char0 = data >>> 8;\n    char1 = data & 0xff;\n\n    if (data === this.PADDING_) {\n      return;\n\n    } else if (data === this.RESUME_CAPTION_LOADING_) {\n      this.mode_ = 'popOn';\n\n    } else if (data === this.END_OF_CAPTION_) {\n      this.clearFormatting(packet.pts);\n      // if a caption was being displayed, it's gone now\n      this.flushDisplayed(packet.pts);\n\n      // flip memory\n      swap = this.displayed_;\n      this.displayed_ = this.nonDisplayed_;\n      this.nonDisplayed_ = swap;\n\n      // start measuring the time to display the caption\n      this.startPts_ = packet.pts;\n\n    } else if (data === this.ROLL_UP_2_ROWS_) {\n      this.topRow_ = BOTTOM_ROW - 1;\n      this.mode_ = 'rollUp';\n    } else if (data === this.ROLL_UP_3_ROWS_) {\n      this.topRow_ = BOTTOM_ROW - 2;\n      this.mode_ = 'rollUp';\n    } else if (data === this.ROLL_UP_4_ROWS_) {\n      this.topRow_ = BOTTOM_ROW - 3;\n      this.mode_ = 'rollUp';\n    } else if (data === this.CARRIAGE_RETURN_) {\n      this.clearFormatting(packet.pts);\n      this.flushDisplayed(packet.pts);\n      this.shiftRowsUp_();\n      this.startPts_ = packet.pts;\n\n    } else if (data === this.BACKSPACE_) {\n      if (this.mode_ === 'popOn') {\n        this.nonDisplayed_[BOTTOM_ROW] = this.nonDisplayed_[BOTTOM_ROW].slice(0, -1);\n      } else {\n        this.displayed_[BOTTOM_ROW] = this.displayed_[BOTTOM_ROW].slice(0, -1);\n      }\n    } else if (data === this.ERASE_DISPLAYED_MEMORY_) {\n      this.flushDisplayed(packet.pts);\n      this.displayed_ = createDisplayBuffer();\n    } else if (data === this.ERASE_NON_DISPLAYED_MEMORY_) {\n      this.nonDisplayed_ = createDisplayBuffer();\n\n    } else if (data === this.RESUME_DIRECT_CAPTIONING_) {\n      this.mode_ = 'paintOn';\n\n    // Append special characters to caption text\n    } else if (this.isSpecialCharacter(char0, char1)) {\n      // Bitmask char0 so that we can apply character transformations\n      // regardless of field and data channel.\n      // Then byte-shift to the left and OR with char1 so we can pass the\n      // entire character code to `getCharFromCode`.\n      char0 = (char0 & 0x03) << 8;\n      text = getCharFromCode(char0 | char1);\n      this[this.mode_](packet.pts, text);\n      this.column_++;\n\n    // Append extended characters to caption text\n    } else if (this.isExtCharacter(char0, char1)) {\n      // Extended characters always follow their \"non-extended\" equivalents.\n      // IE if a \"è\" is desired, you'll always receive \"eè\"; non-compliant\n      // decoders are supposed to drop the \"è\", while compliant decoders\n      // backspace the \"e\" and insert \"è\".\n\n      // Delete the previous character\n      if (this.mode_ === 'popOn') {\n        this.nonDisplayed_[this.row_] = this.nonDisplayed_[this.row_].slice(0, -1);\n      } else {\n        this.displayed_[BOTTOM_ROW] = this.displayed_[BOTTOM_ROW].slice(0, -1);\n      }\n\n      // Bitmask char0 so that we can apply character transformations\n      // regardless of field and data channel.\n      // Then byte-shift to the left and OR with char1 so we can pass the\n      // entire character code to `getCharFromCode`.\n      char0 = (char0 & 0x03) << 8;\n      text = getCharFromCode(char0 | char1);\n      this[this.mode_](packet.pts, text);\n      this.column_++;\n\n    // Process mid-row codes\n    } else if (this.isMidRowCode(char0, char1)) {\n      // Attributes are not additive, so clear all formatting\n      this.clearFormatting(packet.pts);\n\n      // According to the standard, mid-row codes\n      // should be replaced with spaces, so add one now\n      this[this.mode_](packet.pts, ' ');\n      this.column_++;\n\n      if ((char1 & 0xe) === 0xe) {\n        this.addFormatting(packet.pts, ['i']);\n      }\n\n      if ((char1 & 0x1) === 0x1) {\n        this.addFormatting(packet.pts, ['u']);\n      }\n\n    // Detect offset control codes and adjust cursor\n    } else if (this.isOffsetControlCode(char0, char1)) {\n      // Cursor position is set by indent PAC (see below) in 4-column\n      // increments, with an additional offset code of 1-3 to reach any\n      // of the 32 columns specified by CEA-608. So all we need to do\n      // here is increment the column cursor by the given offset.\n      this.column_ += (char1 & 0x03);\n\n    // Detect PACs (Preamble Address Codes)\n    } else if (this.isPAC(char0, char1)) {\n\n      // There's no logic for PAC -> row mapping, so we have to just\n      // find the row code in an array and use its index :(\n      var row = ROWS.indexOf(data & 0x1f20);\n\n      if (row !== this.row_) {\n        // formatting is only persistent for current row\n        this.clearFormatting(packet.pts);\n        this.row_ = row;\n      }\n      // All PACs can apply underline, so detect and apply\n      // (All odd-numbered second bytes set underline)\n      if ((char1 & 0x1) && (this.formatting_.indexOf('u') === -1)) {\n          this.addFormatting(packet.pts, ['u']);\n      }\n\n      if ((data & 0x10) === 0x10) {\n        // We've got an indent level code. Each successive even number\n        // increments the column cursor by 4, so we can get the desired\n        // column position by bit-shifting to the right (to get n/2)\n        // and multiplying by 4.\n        this.column_ = ((data & 0xe) >> 1) * 4;\n      }\n\n      if (this.isColorPAC(char1)) {\n        // it's a color code, though we only support white, which\n        // can be either normal or italicized. white italics can be\n        // either 0x4e or 0x6e depending on the row, so we just\n        // bitwise-and with 0xe to see if italics should be turned on\n        if ((char1 & 0xe) === 0xe) {\n          this.addFormatting(packet.pts, ['i']);\n        }\n      }\n\n    // We have a normal character in char0, and possibly one in char1\n    } else if (this.isNormalChar(char0)) {\n      if (char1 === 0x00) {\n        char1 = null;\n      }\n      text = getCharFromCode(char0);\n      text += getCharFromCode(char1);\n      this[this.mode_](packet.pts, text);\n      this.column_ += text.length;\n\n    } // finish data processing\n\n  };\n};\nCea608Stream.prototype = new Stream();\n// Trigger a cue point that captures the current state of the\n// display buffer\nCea608Stream.prototype.flushDisplayed = function(pts) {\n  var content = this.displayed_\n    // remove spaces from the start and end of the string\n    .map(function(row) {\n      return row.trim();\n    })\n    // combine all text rows to display in one cue\n    .join('\\n')\n    // and remove blank rows from the start and end, but not the middle\n    .replace(/^\\n+|\\n+$/g, '');\n\n  if (content.length) {\n    this.trigger('data', {\n      startPts: this.startPts_,\n      endPts: pts,\n      text: content,\n      stream: this.name_\n    });\n  }\n};\n\n/**\n * Zero out the data, used for startup and on seek\n */\nCea608Stream.prototype.reset = function() {\n  this.mode_ = 'popOn';\n  // When in roll-up mode, the index of the last row that will\n  // actually display captions. If a caption is shifted to a row\n  // with a lower index than this, it is cleared from the display\n  // buffer\n  this.topRow_ = 0;\n  this.startPts_ = 0;\n  this.displayed_ = createDisplayBuffer();\n  this.nonDisplayed_ = createDisplayBuffer();\n  this.lastControlCode_ = null;\n\n  // Track row and column for proper line-breaking and spacing\n  this.column_ = 0;\n  this.row_ = BOTTOM_ROW;\n\n  // This variable holds currently-applied formatting\n  this.formatting_ = [];\n};\n\n/**\n * Sets up control code and related constants for this instance\n */\nCea608Stream.prototype.setConstants = function() {\n  // The following attributes have these uses:\n  // ext_ :    char0 for mid-row codes, and the base for extended\n  //           chars (ext_+0, ext_+1, and ext_+2 are char0s for\n  //           extended codes)\n  // control_: char0 for control codes, except byte-shifted to the\n  //           left so that we can do this.control_ | CONTROL_CODE\n  // offset_:  char0 for tab offset codes\n  //\n  // It's also worth noting that control codes, and _only_ control codes,\n  // differ between field 1 and field2. Field 2 control codes are always\n  // their field 1 value plus 1. That's why there's the \"| field\" on the\n  // control value.\n  if (this.dataChannel_ === 0) {\n    this.BASE_     = 0x10;\n    this.EXT_      = 0x11;\n    this.CONTROL_  = (0x14 | this.field_) << 8;\n    this.OFFSET_   = 0x17;\n  } else if (this.dataChannel_ === 1) {\n    this.BASE_     = 0x18;\n    this.EXT_      = 0x19;\n    this.CONTROL_  = (0x1c | this.field_) << 8;\n    this.OFFSET_   = 0x1f;\n  }\n\n  // Constants for the LSByte command codes recognized by Cea608Stream. This\n  // list is not exhaustive. For a more comprehensive listing and semantics see\n  // http://www.gpo.gov/fdsys/pkg/CFR-2010-title47-vol1/pdf/CFR-2010-title47-vol1-sec15-119.pdf\n  // Padding\n  this.PADDING_                    = 0x0000;\n  // Pop-on Mode\n  this.RESUME_CAPTION_LOADING_     = this.CONTROL_ | 0x20;\n  this.END_OF_CAPTION_             = this.CONTROL_ | 0x2f;\n  // Roll-up Mode\n  this.ROLL_UP_2_ROWS_             = this.CONTROL_ | 0x25;\n  this.ROLL_UP_3_ROWS_             = this.CONTROL_ | 0x26;\n  this.ROLL_UP_4_ROWS_             = this.CONTROL_ | 0x27;\n  this.CARRIAGE_RETURN_            = this.CONTROL_ | 0x2d;\n  // paint-on mode (not supported)\n  this.RESUME_DIRECT_CAPTIONING_   = this.CONTROL_ | 0x29;\n  // Erasure\n  this.BACKSPACE_                  = this.CONTROL_ | 0x21;\n  this.ERASE_DISPLAYED_MEMORY_     = this.CONTROL_ | 0x2c;\n  this.ERASE_NON_DISPLAYED_MEMORY_ = this.CONTROL_ | 0x2e;\n};\n\n/**\n * Detects if the 2-byte packet data is a special character\n *\n * Special characters have a second byte in the range 0x30 to 0x3f,\n * with the first byte being 0x11 (for data channel 1) or 0x19 (for\n * data channel 2).\n *\n * @param  {Integer} char0 The first byte\n * @param  {Integer} char1 The second byte\n * @return {Boolean}       Whether the 2 bytes are an special character\n */\nCea608Stream.prototype.isSpecialCharacter = function(char0, char1) {\n  return (char0 === this.EXT_ && char1 >= 0x30 && char1 <= 0x3f);\n};\n\n/**\n * Detects if the 2-byte packet data is an extended character\n *\n * Extended characters have a second byte in the range 0x20 to 0x3f,\n * with the first byte being 0x12 or 0x13 (for data channel 1) or\n * 0x1a or 0x1b (for data channel 2).\n *\n * @param  {Integer} char0 The first byte\n * @param  {Integer} char1 The second byte\n * @return {Boolean}       Whether the 2 bytes are an extended character\n */\nCea608Stream.prototype.isExtCharacter = function(char0, char1) {\n  return ((char0 === (this.EXT_ + 1) || char0 === (this.EXT_ + 2)) &&\n    (char1 >= 0x20 && char1 <= 0x3f));\n};\n\n/**\n * Detects if the 2-byte packet is a mid-row code\n *\n * Mid-row codes have a second byte in the range 0x20 to 0x2f, with\n * the first byte being 0x11 (for data channel 1) or 0x19 (for data\n * channel 2).\n *\n * @param  {Integer} char0 The first byte\n * @param  {Integer} char1 The second byte\n * @return {Boolean}       Whether the 2 bytes are a mid-row code\n */\nCea608Stream.prototype.isMidRowCode = function(char0, char1) {\n  return (char0 === this.EXT_ && (char1 >= 0x20 && char1 <= 0x2f));\n};\n\n/**\n * Detects if the 2-byte packet is an offset control code\n *\n * Offset control codes have a second byte in the range 0x21 to 0x23,\n * with the first byte being 0x17 (for data channel 1) or 0x1f (for\n * data channel 2).\n *\n * @param  {Integer} char0 The first byte\n * @param  {Integer} char1 The second byte\n * @return {Boolean}       Whether the 2 bytes are an offset control code\n */\nCea608Stream.prototype.isOffsetControlCode = function(char0, char1) {\n  return (char0 === this.OFFSET_ && (char1 >= 0x21 && char1 <= 0x23));\n};\n\n/**\n * Detects if the 2-byte packet is a Preamble Address Code\n *\n * PACs have a first byte in the range 0x10 to 0x17 (for data channel 1)\n * or 0x18 to 0x1f (for data channel 2), with the second byte in the\n * range 0x40 to 0x7f.\n *\n * @param  {Integer} char0 The first byte\n * @param  {Integer} char1 The second byte\n * @return {Boolean}       Whether the 2 bytes are a PAC\n */\nCea608Stream.prototype.isPAC = function(char0, char1) {\n  return (char0 >= this.BASE_ && char0 < (this.BASE_ + 8) &&\n    (char1 >= 0x40 && char1 <= 0x7f));\n};\n\n/**\n * Detects if a packet's second byte is in the range of a PAC color code\n *\n * PAC color codes have the second byte be in the range 0x40 to 0x4f, or\n * 0x60 to 0x6f.\n *\n * @param  {Integer} char1 The second byte\n * @return {Boolean}       Whether the byte is a color PAC\n */\nCea608Stream.prototype.isColorPAC = function(char1) {\n  return ((char1 >= 0x40 && char1 <= 0x4f) || (char1 >= 0x60 && char1 <= 0x7f));\n};\n\n/**\n * Detects if a single byte is in the range of a normal character\n *\n * Normal text bytes are in the range 0x20 to 0x7f.\n *\n * @param  {Integer} char  The byte\n * @return {Boolean}       Whether the byte is a normal character\n */\nCea608Stream.prototype.isNormalChar = function(char) {\n  return (char >= 0x20 && char <= 0x7f);\n};\n\n// Adds the opening HTML tag for the passed character to the caption text,\n// and keeps track of it for later closing\nCea608Stream.prototype.addFormatting = function(pts, format) {\n  this.formatting_ = this.formatting_.concat(format);\n  var text = format.reduce(function(text, format) {\n    return text + '<' + format + '>';\n  }, '');\n  this[this.mode_](pts, text);\n};\n\n// Adds HTML closing tags for current formatting to caption text and\n// clears remembered formatting\nCea608Stream.prototype.clearFormatting = function(pts) {\n  if (!this.formatting_.length) {\n    return;\n  }\n  var text = this.formatting_.reverse().reduce(function(text, format) {\n    return text + '</' + format + '>';\n  }, '');\n  this.formatting_ = [];\n  this[this.mode_](pts, text);\n};\n\n// Mode Implementations\nCea608Stream.prototype.popOn = function(pts, text) {\n  var baseRow = this.nonDisplayed_[this.row_];\n\n  // buffer characters\n  baseRow += text;\n  this.nonDisplayed_[this.row_] = baseRow;\n};\n\nCea608Stream.prototype.rollUp = function(pts, text) {\n  var baseRow = this.displayed_[BOTTOM_ROW];\n\n  baseRow += text;\n  this.displayed_[BOTTOM_ROW] = baseRow;\n\n};\n\nCea608Stream.prototype.shiftRowsUp_ = function() {\n  var i;\n  // clear out inactive rows\n  for (i = 0; i < this.topRow_; i++) {\n    this.displayed_[i] = '';\n  }\n  // shift displayed rows up\n  for (i = this.topRow_; i < BOTTOM_ROW; i++) {\n    this.displayed_[i] = this.displayed_[i + 1];\n  }\n  // clear out the bottom row\n  this.displayed_[BOTTOM_ROW] = '';\n};\n\n// paintOn mode is not implemented\nCea608Stream.prototype.paintOn = function() {};\n\n// exports\nmodule.exports = {\n  CaptionStream: CaptionStream,\n  Cea608Stream: Cea608Stream\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/m2ts/caption-stream.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/m2ts/m2ts.js":
/*!**********************************************!*\
  !*** ./node_modules/mux.js/lib/m2ts/m2ts.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2015 Brightcove\n * All rights reserved.\n *\n * A stream-based mp2t to mp4 converter. This utility can be used to\n * deliver mp4s to a SourceBuffer on platforms that support native\n * Media Source Extensions.\n */\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\"),\n  CaptionStream = __webpack_require__(/*! ./caption-stream */ \"./node_modules/mux.js/lib/m2ts/caption-stream.js\"),\n  StreamTypes = __webpack_require__(/*! ./stream-types */ \"./node_modules/mux.js/lib/m2ts/stream-types.js\"),\n  TimestampRolloverStream = __webpack_require__(/*! ./timestamp-rollover-stream */ \"./node_modules/mux.js/lib/m2ts/timestamp-rollover-stream.js\").TimestampRolloverStream;\n\nvar m2tsStreamTypes = __webpack_require__(/*! ./stream-types.js */ \"./node_modules/mux.js/lib/m2ts/stream-types.js\");\n\n// object types\nvar TransportPacketStream, TransportParseStream, ElementaryStream;\n\n// constants\nvar\n  MP2T_PACKET_LENGTH = 188, // bytes\n  SYNC_BYTE = 0x47;\n\n/**\n * Splits an incoming stream of binary data into MPEG-2 Transport\n * Stream packets.\n */\nTransportPacketStream = function() {\n  var\n    buffer = new Uint8Array(MP2T_PACKET_LENGTH),\n    bytesInBuffer = 0;\n\n  TransportPacketStream.prototype.init.call(this);\n\n   // Deliver new bytes to the stream.\n\n  this.push = function(bytes) {\n    var\n      startIndex = 0,\n      endIndex = MP2T_PACKET_LENGTH,\n      everything;\n\n    // If there are bytes remaining from the last segment, prepend them to the\n    // bytes that were pushed in\n    if (bytesInBuffer) {\n      everything = new Uint8Array(bytes.byteLength + bytesInBuffer);\n      everything.set(buffer.subarray(0, bytesInBuffer));\n      everything.set(bytes, bytesInBuffer);\n      bytesInBuffer = 0;\n    } else {\n      everything = bytes;\n    }\n\n    // While we have enough data for a packet\n    while (endIndex < everything.byteLength) {\n      // Look for a pair of start and end sync bytes in the data..\n      if (everything[startIndex] === SYNC_BYTE && everything[endIndex] === SYNC_BYTE) {\n        // We found a packet so emit it and jump one whole packet forward in\n        // the stream\n        this.trigger('data', everything.subarray(startIndex, endIndex));\n        startIndex += MP2T_PACKET_LENGTH;\n        endIndex += MP2T_PACKET_LENGTH;\n        continue;\n      }\n      // If we get here, we have somehow become de-synchronized and we need to step\n      // forward one byte at a time until we find a pair of sync bytes that denote\n      // a packet\n      startIndex++;\n      endIndex++;\n    }\n\n    // If there was some data left over at the end of the segment that couldn't\n    // possibly be a whole packet, keep it because it might be the start of a packet\n    // that continues in the next segment\n    if (startIndex < everything.byteLength) {\n      buffer.set(everything.subarray(startIndex), 0);\n      bytesInBuffer = everything.byteLength - startIndex;\n    }\n  };\n\n  this.flush = function() {\n    // If the buffer contains a whole packet when we are being flushed, emit it\n    // and empty the buffer. Otherwise hold onto the data because it may be\n    // important for decoding the next segment\n    if (bytesInBuffer === MP2T_PACKET_LENGTH && buffer[0] === SYNC_BYTE) {\n      this.trigger('data', buffer);\n      bytesInBuffer = 0;\n    }\n    this.trigger('done');\n  };\n};\nTransportPacketStream.prototype = new Stream();\n\n/**\n * Accepts an MP2T TransportPacketStream and emits data events with parsed\n * forms of the individual transport stream packets.\n */\nTransportParseStream = function() {\n  var parsePsi, parsePat, parsePmt, self;\n  TransportParseStream.prototype.init.call(this);\n  self = this;\n\n  this.packetsWaitingForPmt = [];\n  this.programMapTable = undefined;\n\n  parsePsi = function(payload, psi) {\n    var offset = 0;\n\n    // PSI packets may be split into multiple sections and those\n    // sections may be split into multiple packets. If a PSI\n    // section starts in this packet, the payload_unit_start_indicator\n    // will be true and the first byte of the payload will indicate\n    // the offset from the current position to the start of the\n    // section.\n    if (psi.payloadUnitStartIndicator) {\n      offset += payload[offset] + 1;\n    }\n\n    if (psi.type === 'pat') {\n      parsePat(payload.subarray(offset), psi);\n    } else {\n      parsePmt(payload.subarray(offset), psi);\n    }\n  };\n\n  parsePat = function(payload, pat) {\n    pat.section_number = payload[7]; // eslint-disable-line camelcase\n    pat.last_section_number = payload[8]; // eslint-disable-line camelcase\n\n    // skip the PSI header and parse the first PMT entry\n    self.pmtPid = (payload[10] & 0x1F) << 8 | payload[11];\n    pat.pmtPid = self.pmtPid;\n  };\n\n  /**\n   * Parse out the relevant fields of a Program Map Table (PMT).\n   * @param payload {Uint8Array} the PMT-specific portion of an MP2T\n   * packet. The first byte in this array should be the table_id\n   * field.\n   * @param pmt {object} the object that should be decorated with\n   * fields parsed from the PMT.\n   */\n  parsePmt = function(payload, pmt) {\n    var sectionLength, tableEnd, programInfoLength, offset;\n\n    // PMTs can be sent ahead of the time when they should actually\n    // take effect. We don't believe this should ever be the case\n    // for HLS but we'll ignore \"forward\" PMT declarations if we see\n    // them. Future PMT declarations have the current_next_indicator\n    // set to zero.\n    if (!(payload[5] & 0x01)) {\n      return;\n    }\n\n    // overwrite any existing program map table\n    self.programMapTable = {\n      video: null,\n      audio: null,\n      'timed-metadata': {}\n    };\n\n    // the mapping table ends at the end of the current section\n    sectionLength = (payload[1] & 0x0f) << 8 | payload[2];\n    tableEnd = 3 + sectionLength - 4;\n\n    // to determine where the table is, we have to figure out how\n    // long the program info descriptors are\n    programInfoLength = (payload[10] & 0x0f) << 8 | payload[11];\n\n    // advance the offset to the first entry in the mapping table\n    offset = 12 + programInfoLength;\n    while (offset < tableEnd) {\n      var streamType = payload[offset];\n      var pid = (payload[offset + 1] & 0x1F) << 8 | payload[offset + 2];\n\n      // only map a single elementary_pid for audio and video stream types\n      // TODO: should this be done for metadata too? for now maintain behavior of\n      //       multiple metadata streams\n      if (streamType === StreamTypes.H264_STREAM_TYPE &&\n          self.programMapTable.video === null) {\n        self.programMapTable.video = pid;\n      } else if (streamType === StreamTypes.ADTS_STREAM_TYPE &&\n                 self.programMapTable.audio === null) {\n        self.programMapTable.audio = pid;\n      } else if (streamType === StreamTypes.METADATA_STREAM_TYPE) {\n        // map pid to stream type for metadata streams\n        self.programMapTable['timed-metadata'][pid] = streamType;\n      }\n\n      // move to the next table entry\n      // skip past the elementary stream descriptors, if present\n      offset += ((payload[offset + 3] & 0x0F) << 8 | payload[offset + 4]) + 5;\n    }\n\n    // record the map on the packet as well\n    pmt.programMapTable = self.programMapTable;\n  };\n\n  /**\n   * Deliver a new MP2T packet to the stream.\n   */\n  this.push = function(packet) {\n    var\n      result = {},\n      offset = 4;\n\n    result.payloadUnitStartIndicator = !!(packet[1] & 0x40);\n\n    // pid is a 13-bit field starting at the last bit of packet[1]\n    result.pid = packet[1] & 0x1f;\n    result.pid <<= 8;\n    result.pid |= packet[2];\n\n    // if an adaption field is present, its length is specified by the\n    // fifth byte of the TS packet header. The adaptation field is\n    // used to add stuffing to PES packets that don't fill a complete\n    // TS packet, and to specify some forms of timing and control data\n    // that we do not currently use.\n    if (((packet[3] & 0x30) >>> 4) > 0x01) {\n      offset += packet[offset] + 1;\n    }\n\n    // parse the rest of the packet based on the type\n    if (result.pid === 0) {\n      result.type = 'pat';\n      parsePsi(packet.subarray(offset), result);\n      this.trigger('data', result);\n    } else if (result.pid === this.pmtPid) {\n      result.type = 'pmt';\n      parsePsi(packet.subarray(offset), result);\n      this.trigger('data', result);\n\n      // if there are any packets waiting for a PMT to be found, process them now\n      while (this.packetsWaitingForPmt.length) {\n        this.processPes_.apply(this, this.packetsWaitingForPmt.shift());\n      }\n    } else if (this.programMapTable === undefined) {\n      // When we have not seen a PMT yet, defer further processing of\n      // PES packets until one has been parsed\n      this.packetsWaitingForPmt.push([packet, offset, result]);\n    } else {\n      this.processPes_(packet, offset, result);\n    }\n  };\n\n  this.processPes_ = function(packet, offset, result) {\n    // set the appropriate stream type\n    if (result.pid === this.programMapTable.video) {\n      result.streamType = StreamTypes.H264_STREAM_TYPE;\n    } else if (result.pid === this.programMapTable.audio) {\n      result.streamType = StreamTypes.ADTS_STREAM_TYPE;\n    } else {\n      // if not video or audio, it is timed-metadata or unknown\n      // if unknown, streamType will be undefined\n      result.streamType = this.programMapTable['timed-metadata'][result.pid];\n    }\n\n    result.type = 'pes';\n    result.data = packet.subarray(offset);\n\n    this.trigger('data', result);\n  };\n\n};\nTransportParseStream.prototype = new Stream();\nTransportParseStream.STREAM_TYPES  = {\n  h264: 0x1b,\n  adts: 0x0f\n};\n\n/**\n * Reconsistutes program elementary stream (PES) packets from parsed\n * transport stream packets. That is, if you pipe an\n * mp2t.TransportParseStream into a mp2t.ElementaryStream, the output\n * events will be events which capture the bytes for individual PES\n * packets plus relevant metadata that has been extracted from the\n * container.\n */\nElementaryStream = function() {\n  var\n    self = this,\n    // PES packet fragments\n    video = {\n      data: [],\n      size: 0\n    },\n    audio = {\n      data: [],\n      size: 0\n    },\n    timedMetadata = {\n      data: [],\n      size: 0\n    },\n    parsePes = function(payload, pes) {\n      var ptsDtsFlags;\n\n      // get the packet length, this will be 0 for video\n      pes.packetLength = 6 + ((payload[4] << 8) | payload[5]);\n\n      // find out if this packets starts a new keyframe\n      pes.dataAlignmentIndicator = (payload[6] & 0x04) !== 0;\n      // PES packets may be annotated with a PTS value, or a PTS value\n      // and a DTS value. Determine what combination of values is\n      // available to work with.\n      ptsDtsFlags = payload[7];\n\n      // PTS and DTS are normally stored as a 33-bit number.  Javascript\n      // performs all bitwise operations on 32-bit integers but javascript\n      // supports a much greater range (52-bits) of integer using standard\n      // mathematical operations.\n      // We construct a 31-bit value using bitwise operators over the 31\n      // most significant bits and then multiply by 4 (equal to a left-shift\n      // of 2) before we add the final 2 least significant bits of the\n      // timestamp (equal to an OR.)\n      if (ptsDtsFlags & 0xC0) {\n        // the PTS and DTS are not written out directly. For information\n        // on how they are encoded, see\n        // http://dvd.sourceforge.net/dvdinfo/pes-hdr.html\n        pes.pts = (payload[9] & 0x0E) << 27 |\n          (payload[10] & 0xFF) << 20 |\n          (payload[11] & 0xFE) << 12 |\n          (payload[12] & 0xFF) <<  5 |\n          (payload[13] & 0xFE) >>>  3;\n        pes.pts *= 4; // Left shift by 2\n        pes.pts += (payload[13] & 0x06) >>> 1; // OR by the two LSBs\n        pes.dts = pes.pts;\n        if (ptsDtsFlags & 0x40) {\n          pes.dts = (payload[14] & 0x0E) << 27 |\n            (payload[15] & 0xFF) << 20 |\n            (payload[16] & 0xFE) << 12 |\n            (payload[17] & 0xFF) << 5 |\n            (payload[18] & 0xFE) >>> 3;\n          pes.dts *= 4; // Left shift by 2\n          pes.dts += (payload[18] & 0x06) >>> 1; // OR by the two LSBs\n        }\n      }\n      // the data section starts immediately after the PES header.\n      // pes_header_data_length specifies the number of header bytes\n      // that follow the last byte of the field.\n      pes.data = payload.subarray(9 + payload[8]);\n    },\n    flushStream = function(stream, type, forceFlush) {\n      var\n        packetData = new Uint8Array(stream.size),\n        event = {\n          type: type\n        },\n        i = 0,\n        offset = 0,\n        packetFlushable = false,\n        fragment;\n\n      // do nothing if there is not enough buffered data for a complete\n      // PES header\n      if (!stream.data.length || stream.size < 9) {\n        return;\n      }\n      event.trackId = stream.data[0].pid;\n\n      // reassemble the packet\n      for (i = 0; i < stream.data.length; i++) {\n        fragment = stream.data[i];\n\n        packetData.set(fragment.data, offset);\n        offset += fragment.data.byteLength;\n      }\n\n      // parse assembled packet's PES header\n      parsePes(packetData, event);\n\n      // non-video PES packets MUST have a non-zero PES_packet_length\n      // check that there is enough stream data to fill the packet\n      packetFlushable = type === 'video' || event.packetLength <= stream.size;\n\n      // flush pending packets if the conditions are right\n      if (forceFlush || packetFlushable) {\n        stream.size = 0;\n        stream.data.length = 0;\n      }\n\n      // only emit packets that are complete. this is to avoid assembling\n      // incomplete PES packets due to poor segmentation\n      if (packetFlushable) {\n        self.trigger('data', event);\n      }\n    };\n\n  ElementaryStream.prototype.init.call(this);\n\n  this.push = function(data) {\n    ({\n      pat: function() {\n        // we have to wait for the PMT to arrive as well before we\n        // have any meaningful metadata\n      },\n      pes: function() {\n        var stream, streamType;\n\n        switch (data.streamType) {\n        case StreamTypes.H264_STREAM_TYPE:\n        case m2tsStreamTypes.H264_STREAM_TYPE:\n          stream = video;\n          streamType = 'video';\n          break;\n        case StreamTypes.ADTS_STREAM_TYPE:\n          stream = audio;\n          streamType = 'audio';\n          break;\n        case StreamTypes.METADATA_STREAM_TYPE:\n          stream = timedMetadata;\n          streamType = 'timed-metadata';\n          break;\n        default:\n          // ignore unknown stream types\n          return;\n        }\n\n        // if a new packet is starting, we can flush the completed\n        // packet\n        if (data.payloadUnitStartIndicator) {\n          flushStream(stream, streamType, true);\n        }\n\n        // buffer this fragment until we are sure we've received the\n        // complete payload\n        stream.data.push(data);\n        stream.size += data.data.byteLength;\n      },\n      pmt: function() {\n        var\n          event = {\n            type: 'metadata',\n            tracks: []\n          },\n          programMapTable = data.programMapTable;\n\n        // translate audio and video streams to tracks\n        if (programMapTable.video !== null) {\n          event.tracks.push({\n            timelineStartInfo: {\n              baseMediaDecodeTime: 0\n            },\n            id: +programMapTable.video,\n            codec: 'avc',\n            type: 'video'\n          });\n        }\n        if (programMapTable.audio !== null) {\n          event.tracks.push({\n            timelineStartInfo: {\n              baseMediaDecodeTime: 0\n            },\n            id: +programMapTable.audio,\n            codec: 'adts',\n            type: 'audio'\n          });\n        }\n\n        self.trigger('data', event);\n      }\n    })[data.type]();\n  };\n\n  /**\n   * Flush any remaining input. Video PES packets may be of variable\n   * length. Normally, the start of a new video packet can trigger the\n   * finalization of the previous packet. That is not possible if no\n   * more video is forthcoming, however. In that case, some other\n   * mechanism (like the end of the file) has to be employed. When it is\n   * clear that no additional data is forthcoming, calling this method\n   * will flush the buffered packets.\n   */\n  this.flush = function() {\n    // !!THIS ORDER IS IMPORTANT!!\n    // video first then audio\n    flushStream(video, 'video');\n    flushStream(audio, 'audio');\n    flushStream(timedMetadata, 'timed-metadata');\n    this.trigger('done');\n  };\n};\nElementaryStream.prototype = new Stream();\n\nvar m2ts = {\n  PAT_PID: 0x0000,\n  MP2T_PACKET_LENGTH: MP2T_PACKET_LENGTH,\n  TransportPacketStream: TransportPacketStream,\n  TransportParseStream: TransportParseStream,\n  ElementaryStream: ElementaryStream,\n  TimestampRolloverStream: TimestampRolloverStream,\n  CaptionStream: CaptionStream.CaptionStream,\n  Cea608Stream: CaptionStream.Cea608Stream,\n  MetadataStream: __webpack_require__(/*! ./metadata-stream */ \"./node_modules/mux.js/lib/m2ts/metadata-stream.js\")\n};\n\nfor (var type in StreamTypes) {\n  if (StreamTypes.hasOwnProperty(type)) {\n    m2ts[type] = StreamTypes[type];\n  }\n}\n\nmodule.exports = m2ts;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/m2ts/m2ts.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/m2ts/metadata-stream.js":
/*!*********************************************************!*\
  !*** ./node_modules/mux.js/lib/m2ts/metadata-stream.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * Accepts program elementary stream (PES) data events and parses out\n * ID3 metadata from them, if present.\n * @see http://id3.org/id3v2.3.0\n */\n\nvar\n  Stream = __webpack_require__(/*! ../utils/stream */ \"./node_modules/mux.js/lib/utils/stream.js\"),\n  StreamTypes = __webpack_require__(/*! ./stream-types */ \"./node_modules/mux.js/lib/m2ts/stream-types.js\"),\n  // return a percent-encoded representation of the specified byte range\n  // @see http://en.wikipedia.org/wiki/Percent-encoding\n  percentEncode = function(bytes, start, end) {\n    var i, result = '';\n    for (i = start; i < end; i++) {\n      result += '%' + ('00' + bytes[i].toString(16)).slice(-2);\n    }\n    return result;\n  },\n  // return the string representation of the specified byte range,\n  // interpreted as UTf-8.\n  parseUtf8 = function(bytes, start, end) {\n    return decodeURIComponent(percentEncode(bytes, start, end));\n  },\n  // return the string representation of the specified byte range,\n  // interpreted as ISO-8859-1.\n  parseIso88591 = function(bytes, start, end) {\n    return unescape(percentEncode(bytes, start, end)); // jshint ignore:line\n  },\n  parseSyncSafeInteger = function(data) {\n    return (data[0] << 21) |\n            (data[1] << 14) |\n            (data[2] << 7) |\n            (data[3]);\n  },\n  tagParsers = {\n    TXXX: function(tag) {\n      var i;\n      if (tag.data[0] !== 3) {\n        // ignore frames with unrecognized character encodings\n        return;\n      }\n\n      for (i = 1; i < tag.data.length; i++) {\n        if (tag.data[i] === 0) {\n          // parse the text fields\n          tag.description = parseUtf8(tag.data, 1, i);\n          // do not include the null terminator in the tag value\n          tag.value = parseUtf8(tag.data, i + 1, tag.data.length).replace(/\\0*$/, '');\n          break;\n        }\n      }\n      tag.data = tag.value;\n    },\n    WXXX: function(tag) {\n      var i;\n      if (tag.data[0] !== 3) {\n        // ignore frames with unrecognized character encodings\n        return;\n      }\n\n      for (i = 1; i < tag.data.length; i++) {\n        if (tag.data[i] === 0) {\n          // parse the description and URL fields\n          tag.description = parseUtf8(tag.data, 1, i);\n          tag.url = parseUtf8(tag.data, i + 1, tag.data.length);\n          break;\n        }\n      }\n    },\n    PRIV: function(tag) {\n      var i;\n\n      for (i = 0; i < tag.data.length; i++) {\n        if (tag.data[i] === 0) {\n          // parse the description and URL fields\n          tag.owner = parseIso88591(tag.data, 0, i);\n          break;\n        }\n      }\n      tag.privateData = tag.data.subarray(i + 1);\n      tag.data = tag.privateData;\n    }\n  },\n  MetadataStream;\n\nMetadataStream = function(options) {\n  var\n    settings = {\n      debug: !!(options && options.debug),\n\n      // the bytes of the program-level descriptor field in MP2T\n      // see ISO/IEC 13818-1:2013 (E), section 2.6 \"Program and\n      // program element descriptors\"\n      descriptor: options && options.descriptor\n    },\n    // the total size in bytes of the ID3 tag being parsed\n    tagSize = 0,\n    // tag data that is not complete enough to be parsed\n    buffer = [],\n    // the total number of bytes currently in the buffer\n    bufferSize = 0,\n    i;\n\n  MetadataStream.prototype.init.call(this);\n\n  // calculate the text track in-band metadata track dispatch type\n  // https://html.spec.whatwg.org/multipage/embedded-content.html#steps-to-expose-a-media-resource-specific-text-track\n  this.dispatchType = StreamTypes.METADATA_STREAM_TYPE.toString(16);\n  if (settings.descriptor) {\n    for (i = 0; i < settings.descriptor.length; i++) {\n      this.dispatchType += ('00' + settings.descriptor[i].toString(16)).slice(-2);\n    }\n  }\n\n  this.push = function(chunk) {\n    var tag, frameStart, frameSize, frame, i, frameHeader;\n    if (chunk.type !== 'timed-metadata') {\n      return;\n    }\n\n    // if data_alignment_indicator is set in the PES header,\n    // we must have the start of a new ID3 tag. Assume anything\n    // remaining in the buffer was malformed and throw it out\n    if (chunk.dataAlignmentIndicator) {\n      bufferSize = 0;\n      buffer.length = 0;\n    }\n\n    // ignore events that don't look like ID3 data\n    if (buffer.length === 0 &&\n        (chunk.data.length < 10 ||\n          chunk.data[0] !== 'I'.charCodeAt(0) ||\n          chunk.data[1] !== 'D'.charCodeAt(0) ||\n          chunk.data[2] !== '3'.charCodeAt(0))) {\n      if (settings.debug) {\n        // eslint-disable-next-line no-console\n        console.log('Skipping unrecognized metadata packet');\n      }\n      return;\n    }\n\n    // add this chunk to the data we've collected so far\n\n    buffer.push(chunk);\n    bufferSize += chunk.data.byteLength;\n\n    // grab the size of the entire frame from the ID3 header\n    if (buffer.length === 1) {\n      // the frame size is transmitted as a 28-bit integer in the\n      // last four bytes of the ID3 header.\n      // The most significant bit of each byte is dropped and the\n      // results concatenated to recover the actual value.\n      tagSize = parseSyncSafeInteger(chunk.data.subarray(6, 10));\n\n      // ID3 reports the tag size excluding the header but it's more\n      // convenient for our comparisons to include it\n      tagSize += 10;\n    }\n\n    // if the entire frame has not arrived, wait for more data\n    if (bufferSize < tagSize) {\n      return;\n    }\n\n    // collect the entire frame so it can be parsed\n    tag = {\n      data: new Uint8Array(tagSize),\n      frames: [],\n      pts: buffer[0].pts,\n      dts: buffer[0].dts\n    };\n    for (i = 0; i < tagSize;) {\n      tag.data.set(buffer[0].data.subarray(0, tagSize - i), i);\n      i += buffer[0].data.byteLength;\n      bufferSize -= buffer[0].data.byteLength;\n      buffer.shift();\n    }\n\n    // find the start of the first frame and the end of the tag\n    frameStart = 10;\n    if (tag.data[5] & 0x40) {\n      // advance the frame start past the extended header\n      frameStart += 4; // header size field\n      frameStart += parseSyncSafeInteger(tag.data.subarray(10, 14));\n\n      // clip any padding off the end\n      tagSize -= parseSyncSafeInteger(tag.data.subarray(16, 20));\n    }\n\n    // parse one or more ID3 frames\n    // http://id3.org/id3v2.3.0#ID3v2_frame_overview\n    do {\n      // determine the number of bytes in this frame\n      frameSize = parseSyncSafeInteger(tag.data.subarray(frameStart + 4, frameStart + 8));\n      if (frameSize < 1) {\n         // eslint-disable-next-line no-console\n        return console.log('Malformed ID3 frame encountered. Skipping metadata parsing.');\n      }\n      frameHeader = String.fromCharCode(tag.data[frameStart],\n                                        tag.data[frameStart + 1],\n                                        tag.data[frameStart + 2],\n                                        tag.data[frameStart + 3]);\n\n\n      frame = {\n        id: frameHeader,\n        data: tag.data.subarray(frameStart + 10, frameStart + frameSize + 10)\n      };\n      frame.key = frame.id;\n      if (tagParsers[frame.id]) {\n        tagParsers[frame.id](frame);\n\n        // handle the special PRIV frame used to indicate the start\n        // time for raw AAC data\n        if (frame.owner === 'com.apple.streaming.transportStreamTimestamp') {\n          var\n            d = frame.data,\n            size = ((d[3] & 0x01)  << 30) |\n                   (d[4]  << 22) |\n                   (d[5] << 14) |\n                   (d[6] << 6) |\n                   (d[7] >>> 2);\n\n          size *= 4;\n          size += d[7] & 0x03;\n          frame.timeStamp = size;\n          // in raw AAC, all subsequent data will be timestamped based\n          // on the value of this frame\n          // we couldn't have known the appropriate pts and dts before\n          // parsing this ID3 tag so set those values now\n          if (tag.pts === undefined && tag.dts === undefined) {\n            tag.pts = frame.timeStamp;\n            tag.dts = frame.timeStamp;\n          }\n          this.trigger('timestamp', frame);\n        }\n      }\n      tag.frames.push(frame);\n\n      frameStart += 10; // advance past the frame header\n      frameStart += frameSize; // advance past the frame body\n    } while (frameStart < tagSize);\n    this.trigger('data', tag);\n  };\n};\nMetadataStream.prototype = new Stream();\n\nmodule.exports = MetadataStream;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/m2ts/metadata-stream.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/m2ts/probe.js":
/*!***********************************************!*\
  !*** ./node_modules/mux.js/lib/m2ts/probe.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2016 Brightcove\n * All rights reserved.\n *\n * Utilities to detect basic properties and metadata about TS Segments.\n */\n\n\nvar StreamTypes = __webpack_require__(/*! ./stream-types.js */ \"./node_modules/mux.js/lib/m2ts/stream-types.js\");\n\nvar parsePid = function(packet) {\n  var pid = packet[1] & 0x1f;\n  pid <<= 8;\n  pid |= packet[2];\n  return pid;\n};\n\nvar parsePayloadUnitStartIndicator = function(packet) {\n  return !!(packet[1] & 0x40);\n};\n\nvar parseAdaptionField = function(packet) {\n  var offset = 0;\n  // if an adaption field is present, its length is specified by the\n  // fifth byte of the TS packet header. The adaptation field is\n  // used to add stuffing to PES packets that don't fill a complete\n  // TS packet, and to specify some forms of timing and control data\n  // that we do not currently use.\n  if (((packet[3] & 0x30) >>> 4) > 0x01) {\n    offset += packet[4] + 1;\n  }\n  return offset;\n};\n\nvar parseType = function(packet, pmtPid) {\n  var pid = parsePid(packet);\n  if (pid === 0) {\n    return 'pat';\n  } else if (pid === pmtPid) {\n    return 'pmt';\n  } else if (pmtPid) {\n    return 'pes';\n  }\n  return null;\n};\n\nvar parsePat = function(packet) {\n  var pusi = parsePayloadUnitStartIndicator(packet);\n  var offset = 4 + parseAdaptionField(packet);\n\n  if (pusi) {\n    offset += packet[offset] + 1;\n  }\n\n  return (packet[offset + 10] & 0x1f) << 8 | packet[offset + 11];\n};\n\nvar parsePmt = function(packet) {\n  var programMapTable = {};\n  var pusi = parsePayloadUnitStartIndicator(packet);\n  var payloadOffset = 4 + parseAdaptionField(packet);\n\n  if (pusi) {\n    payloadOffset += packet[payloadOffset] + 1;\n  }\n\n  // PMTs can be sent ahead of the time when they should actually\n  // take effect. We don't believe this should ever be the case\n  // for HLS but we'll ignore \"forward\" PMT declarations if we see\n  // them. Future PMT declarations have the current_next_indicator\n  // set to zero.\n  if (!(packet[payloadOffset + 5] & 0x01)) {\n    return;\n  }\n\n  var sectionLength, tableEnd, programInfoLength;\n  // the mapping table ends at the end of the current section\n  sectionLength = (packet[payloadOffset + 1] & 0x0f) << 8 | packet[payloadOffset + 2];\n  tableEnd = 3 + sectionLength - 4;\n\n  // to determine where the table is, we have to figure out how\n  // long the program info descriptors are\n  programInfoLength = (packet[payloadOffset + 10] & 0x0f) << 8 | packet[payloadOffset + 11];\n\n  // advance the offset to the first entry in the mapping table\n  var offset = 12 + programInfoLength;\n  while (offset < tableEnd) {\n    var i = payloadOffset + offset;\n    // add an entry that maps the elementary_pid to the stream_type\n    programMapTable[(packet[i + 1] & 0x1F) << 8 | packet[i + 2]] = packet[i];\n\n    // move to the next table entry\n    // skip past the elementary stream descriptors, if present\n    offset += ((packet[i + 3] & 0x0F) << 8 | packet[i + 4]) + 5;\n  }\n  return programMapTable;\n};\n\nvar parsePesType = function(packet, programMapTable) {\n  var pid = parsePid(packet);\n  var type = programMapTable[pid];\n  switch (type) {\n    case StreamTypes.H264_STREAM_TYPE:\n      return 'video';\n    case StreamTypes.ADTS_STREAM_TYPE:\n      return 'audio';\n    case StreamTypes.METADATA_STREAM_TYPE:\n      return 'timed-metadata';\n    default:\n      return null;\n  }\n};\n\nvar parsePesTime = function(packet) {\n  var pusi = parsePayloadUnitStartIndicator(packet);\n  if (!pusi) {\n    return null;\n  }\n\n  var offset = 4 + parseAdaptionField(packet);\n\n  if (offset >= packet.byteLength) {\n    // From the H 222.0 MPEG-TS spec\n    // \"For transport stream packets carrying PES packets, stuffing is needed when there\n    //  is insufficient PES packet data to completely fill the transport stream packet\n    //  payload bytes. Stuffing is accomplished by defining an adaptation field longer than\n    //  the sum of the lengths of the data elements in it, so that the payload bytes\n    //  remaining after the adaptation field exactly accommodates the available PES packet\n    //  data.\"\n    //\n    // If the offset is >= the length of the packet, then the packet contains no data\n    // and instead is just adaption field stuffing bytes\n    return null;\n  }\n\n  var pes = null;\n  var ptsDtsFlags;\n\n  // PES packets may be annotated with a PTS value, or a PTS value\n  // and a DTS value. Determine what combination of values is\n  // available to work with.\n  ptsDtsFlags = packet[offset + 7];\n\n  // PTS and DTS are normally stored as a 33-bit number.  Javascript\n  // performs all bitwise operations on 32-bit integers but javascript\n  // supports a much greater range (52-bits) of integer using standard\n  // mathematical operations.\n  // We construct a 31-bit value using bitwise operators over the 31\n  // most significant bits and then multiply by 4 (equal to a left-shift\n  // of 2) before we add the final 2 least significant bits of the\n  // timestamp (equal to an OR.)\n  if (ptsDtsFlags & 0xC0) {\n    pes = {};\n    // the PTS and DTS are not written out directly. For information\n    // on how they are encoded, see\n    // http://dvd.sourceforge.net/dvdinfo/pes-hdr.html\n    pes.pts = (packet[offset + 9] & 0x0E) << 27 |\n      (packet[offset + 10] & 0xFF) << 20 |\n      (packet[offset + 11] & 0xFE) << 12 |\n      (packet[offset + 12] & 0xFF) <<  5 |\n      (packet[offset + 13] & 0xFE) >>>  3;\n    pes.pts *= 4; // Left shift by 2\n    pes.pts += (packet[offset + 13] & 0x06) >>> 1; // OR by the two LSBs\n    pes.dts = pes.pts;\n    if (ptsDtsFlags & 0x40) {\n      pes.dts = (packet[offset + 14] & 0x0E) << 27 |\n        (packet[offset + 15] & 0xFF) << 20 |\n        (packet[offset + 16] & 0xFE) << 12 |\n        (packet[offset + 17] & 0xFF) << 5 |\n        (packet[offset + 18] & 0xFE) >>> 3;\n      pes.dts *= 4; // Left shift by 2\n      pes.dts += (packet[offset + 18] & 0x06) >>> 1; // OR by the two LSBs\n    }\n  }\n  return pes;\n};\n\nvar parseNalUnitType = function(type) {\n  switch (type) {\n    case 0x05:\n      return 'slice_layer_without_partitioning_rbsp_idr';\n    case 0x06:\n      return 'sei_rbsp';\n    case 0x07:\n      return 'seq_parameter_set_rbsp';\n    case 0x08:\n      return 'pic_parameter_set_rbsp';\n    case 0x09:\n      return 'access_unit_delimiter_rbsp';\n    default:\n      return null;\n  }\n};\n\nvar videoPacketContainsKeyFrame = function(packet) {\n  var offset = 4 + parseAdaptionField(packet);\n  var frameBuffer = packet.subarray(offset);\n  var frameI = 0;\n  var frameSyncPoint = 0;\n  var foundKeyFrame = false;\n  var nalType;\n\n  // advance the sync point to a NAL start, if necessary\n  for (; frameSyncPoint < frameBuffer.byteLength - 3; frameSyncPoint++) {\n    if (frameBuffer[frameSyncPoint + 2] === 1) {\n      // the sync point is properly aligned\n      frameI = frameSyncPoint + 5;\n      break;\n    }\n  }\n\n  while (frameI < frameBuffer.byteLength) {\n    // look at the current byte to determine if we've hit the end of\n    // a NAL unit boundary\n    switch (frameBuffer[frameI]) {\n    case 0:\n      // skip past non-sync sequences\n      if (frameBuffer[frameI - 1] !== 0) {\n        frameI += 2;\n        break;\n      } else if (frameBuffer[frameI - 2] !== 0) {\n        frameI++;\n        break;\n      }\n\n      if (frameSyncPoint + 3 !== frameI - 2) {\n        nalType = parseNalUnitType(frameBuffer[frameSyncPoint + 3] & 0x1f);\n        if (nalType === 'slice_layer_without_partitioning_rbsp_idr') {\n          foundKeyFrame = true;\n        }\n      }\n\n      // drop trailing zeroes\n      do {\n        frameI++;\n      } while (frameBuffer[frameI] !== 1 && frameI < frameBuffer.length);\n      frameSyncPoint = frameI - 2;\n      frameI += 3;\n      break;\n    case 1:\n      // skip past non-sync sequences\n      if (frameBuffer[frameI - 1] !== 0 ||\n          frameBuffer[frameI - 2] !== 0) {\n        frameI += 3;\n        break;\n      }\n\n      nalType = parseNalUnitType(frameBuffer[frameSyncPoint + 3] & 0x1f);\n      if (nalType === 'slice_layer_without_partitioning_rbsp_idr') {\n        foundKeyFrame = true;\n      }\n      frameSyncPoint = frameI - 2;\n      frameI += 3;\n      break;\n    default:\n      // the current byte isn't a one or zero, so it cannot be part\n      // of a sync sequence\n      frameI += 3;\n      break;\n    }\n  }\n  frameBuffer = frameBuffer.subarray(frameSyncPoint);\n  frameI -= frameSyncPoint;\n  frameSyncPoint = 0;\n  // parse the final nal\n  if (frameBuffer && frameBuffer.byteLength > 3) {\n    nalType = parseNalUnitType(frameBuffer[frameSyncPoint + 3] & 0x1f);\n    if (nalType === 'slice_layer_without_partitioning_rbsp_idr') {\n      foundKeyFrame = true;\n    }\n  }\n\n  return foundKeyFrame;\n};\n\n\nmodule.exports = {\n  parseType: parseType,\n  parsePat: parsePat,\n  parsePmt: parsePmt,\n  parsePayloadUnitStartIndicator: parsePayloadUnitStartIndicator,\n  parsePesType: parsePesType,\n  parsePesTime: parsePesTime,\n  videoPacketContainsKeyFrame: videoPacketContainsKeyFrame\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/m2ts/probe.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/m2ts/stream-types.js":
/*!******************************************************!*\
  !*** ./node_modules/mux.js/lib/m2ts/stream-types.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = {\n  H264_STREAM_TYPE: 0x1B,\n  ADTS_STREAM_TYPE: 0x0F,\n  METADATA_STREAM_TYPE: 0x15\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/m2ts/stream-types.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/m2ts/timestamp-rollover-stream.js":
/*!*******************************************************************!*\
  !*** ./node_modules/mux.js/lib/m2ts/timestamp-rollover-stream.js ***!
  \*******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2016 Brightcove\n * All rights reserved.\n *\n * Accepts program elementary stream (PES) data events and corrects\n * decode and presentation time stamps to account for a rollover\n * of the 33 bit value.\n */\n\n\n\nvar Stream = __webpack_require__(/*! ../utils/stream */ \"./node_modules/mux.js/lib/utils/stream.js\");\n\nvar MAX_TS = 8589934592;\n\nvar RO_THRESH = 4294967296;\n\nvar handleRollover = function(value, reference) {\n  var direction = 1;\n\n  if (value > reference) {\n    // If the current timestamp value is greater than our reference timestamp and we detect a\n    // timestamp rollover, this means the roll over is happening in the opposite direction.\n    // Example scenario: Enter a long stream/video just after a rollover occurred. The reference\n    // point will be set to a small number, e.g. 1. The user then seeks backwards over the\n    // rollover point. In loading this segment, the timestamp values will be very large,\n    // e.g. 2^33 - 1. Since this comes before the data we loaded previously, we want to adjust\n    // the time stamp to be `value - 2^33`.\n    direction = -1;\n  }\n\n  // Note: A seek forwards or back that is greater than the RO_THRESH (2^32, ~13 hours) will\n  // cause an incorrect adjustment.\n  while (Math.abs(reference - value) > RO_THRESH) {\n    value += (direction * MAX_TS);\n  }\n\n  return value;\n};\n\nvar TimestampRolloverStream = function(type) {\n  var lastDTS, referenceDTS;\n\n  TimestampRolloverStream.prototype.init.call(this);\n\n  this.type_ = type;\n\n  this.push = function(data) {\n    if (data.type !== this.type_) {\n      return;\n    }\n\n    if (referenceDTS === undefined) {\n      referenceDTS = data.dts;\n    }\n\n    data.dts = handleRollover(data.dts, referenceDTS);\n    data.pts = handleRollover(data.pts, referenceDTS);\n\n    lastDTS = data.dts;\n\n    this.trigger('data', data);\n  };\n\n  this.flush = function() {\n    referenceDTS = lastDTS;\n    this.trigger('done');\n  };\n\n  this.discontinuity = function() {\n    referenceDTS = void 0;\n    lastDTS = void 0;\n  };\n\n};\n\nTimestampRolloverStream.prototype = new Stream();\n\nmodule.exports = {\n  TimestampRolloverStream: TimestampRolloverStream,\n  handleRollover: handleRollover\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/m2ts/timestamp-rollover-stream.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/mp4/index.js":
/*!**********************************************!*\
  !*** ./node_modules/mux.js/lib/mp4/index.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("module.exports = {\n  generator: __webpack_require__(/*! ./mp4-generator */ \"./node_modules/mux.js/lib/mp4/mp4-generator.js\"),\n  Transmuxer: __webpack_require__(/*! ./transmuxer */ \"./node_modules/mux.js/lib/mp4/transmuxer.js\").Transmuxer,\n  AudioSegmentStream: __webpack_require__(/*! ./transmuxer */ \"./node_modules/mux.js/lib/mp4/transmuxer.js\").AudioSegmentStream,\n  VideoSegmentStream: __webpack_require__(/*! ./transmuxer */ \"./node_modules/mux.js/lib/mp4/transmuxer.js\").VideoSegmentStream\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/mp4/index.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/mp4/mp4-generator.js":
/*!******************************************************!*\
  !*** ./node_modules/mux.js/lib/mp4/mp4-generator.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2015 Brightcove\n * All rights reserved.\n *\n * Functions that generate fragmented MP4s suitable for use with Media\n * Source Extensions.\n */\n\n\nvar UINT32_MAX = Math.pow(2, 32) - 1;\n\nvar box, dinf, esds, ftyp, mdat, mfhd, minf, moof, moov, mvex, mvhd,\n    trak, tkhd, mdia, mdhd, hdlr, sdtp, stbl, stsd, traf, trex,\n    trun, types, MAJOR_BRAND, MINOR_VERSION, AVC1_BRAND, VIDEO_HDLR,\n    AUDIO_HDLR, HDLR_TYPES, VMHD, SMHD, DREF, STCO, STSC, STSZ, STTS;\n\n// pre-calculate constants\n(function() {\n  var i;\n  types = {\n    avc1: [], // codingname\n    avcC: [],\n    btrt: [],\n    dinf: [],\n    dref: [],\n    esds: [],\n    ftyp: [],\n    hdlr: [],\n    mdat: [],\n    mdhd: [],\n    mdia: [],\n    mfhd: [],\n    minf: [],\n    moof: [],\n    moov: [],\n    mp4a: [], // codingname\n    mvex: [],\n    mvhd: [],\n    sdtp: [],\n    smhd: [],\n    stbl: [],\n    stco: [],\n    stsc: [],\n    stsd: [],\n    stsz: [],\n    stts: [],\n    styp: [],\n    tfdt: [],\n    tfhd: [],\n    traf: [],\n    trak: [],\n    trun: [],\n    trex: [],\n    tkhd: [],\n    vmhd: []\n  };\n\n  // In environments where Uint8Array is undefined (e.g., IE8), skip set up so that we\n  // don't throw an error\n  if (typeof Uint8Array === 'undefined') {\n    return;\n  }\n\n  for (i in types) {\n    if (types.hasOwnProperty(i)) {\n      types[i] = [\n        i.charCodeAt(0),\n        i.charCodeAt(1),\n        i.charCodeAt(2),\n        i.charCodeAt(3)\n      ];\n    }\n  }\n\n  MAJOR_BRAND = new Uint8Array([\n    'i'.charCodeAt(0),\n    's'.charCodeAt(0),\n    'o'.charCodeAt(0),\n    'm'.charCodeAt(0)\n  ]);\n  AVC1_BRAND = new Uint8Array([\n    'a'.charCodeAt(0),\n    'v'.charCodeAt(0),\n    'c'.charCodeAt(0),\n    '1'.charCodeAt(0)\n  ]);\n  MINOR_VERSION = new Uint8Array([0, 0, 0, 1]);\n  VIDEO_HDLR = new Uint8Array([\n    0x00, // version 0\n    0x00, 0x00, 0x00, // flags\n    0x00, 0x00, 0x00, 0x00, // pre_defined\n    0x76, 0x69, 0x64, 0x65, // handler_type: 'vide'\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x56, 0x69, 0x64, 0x65,\n    0x6f, 0x48, 0x61, 0x6e,\n    0x64, 0x6c, 0x65, 0x72, 0x00 // name: 'VideoHandler'\n  ]);\n  AUDIO_HDLR = new Uint8Array([\n    0x00, // version 0\n    0x00, 0x00, 0x00, // flags\n    0x00, 0x00, 0x00, 0x00, // pre_defined\n    0x73, 0x6f, 0x75, 0x6e, // handler_type: 'soun'\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x53, 0x6f, 0x75, 0x6e,\n    0x64, 0x48, 0x61, 0x6e,\n    0x64, 0x6c, 0x65, 0x72, 0x00 // name: 'SoundHandler'\n  ]);\n  HDLR_TYPES = {\n    video: VIDEO_HDLR,\n    audio: AUDIO_HDLR\n  };\n  DREF = new Uint8Array([\n    0x00, // version 0\n    0x00, 0x00, 0x00, // flags\n    0x00, 0x00, 0x00, 0x01, // entry_count\n    0x00, 0x00, 0x00, 0x0c, // entry_size\n    0x75, 0x72, 0x6c, 0x20, // 'url' type\n    0x00, // version 0\n    0x00, 0x00, 0x01 // entry_flags\n  ]);\n  SMHD = new Uint8Array([\n    0x00,             // version\n    0x00, 0x00, 0x00, // flags\n    0x00, 0x00,       // balance, 0 means centered\n    0x00, 0x00        // reserved\n  ]);\n  STCO = new Uint8Array([\n    0x00, // version\n    0x00, 0x00, 0x00, // flags\n    0x00, 0x00, 0x00, 0x00 // entry_count\n  ]);\n  STSC = STCO;\n  STSZ = new Uint8Array([\n    0x00, // version\n    0x00, 0x00, 0x00, // flags\n    0x00, 0x00, 0x00, 0x00, // sample_size\n    0x00, 0x00, 0x00, 0x00 // sample_count\n  ]);\n  STTS = STCO;\n  VMHD = new Uint8Array([\n    0x00, // version\n    0x00, 0x00, 0x01, // flags\n    0x00, 0x00, // graphicsmode\n    0x00, 0x00,\n    0x00, 0x00,\n    0x00, 0x00 // opcolor\n  ]);\n}());\n\nbox = function(type) {\n  var\n    payload = [],\n    size = 0,\n    i,\n    result,\n    view;\n\n  for (i = 1; i < arguments.length; i++) {\n    payload.push(arguments[i]);\n  }\n\n  i = payload.length;\n\n  // calculate the total size we need to allocate\n  while (i--) {\n    size += payload[i].byteLength;\n  }\n  result = new Uint8Array(size + 8);\n  view = new DataView(result.buffer, result.byteOffset, result.byteLength);\n  view.setUint32(0, result.byteLength);\n  result.set(type, 4);\n\n  // copy the payload into the result\n  for (i = 0, size = 8; i < payload.length; i++) {\n    result.set(payload[i], size);\n    size += payload[i].byteLength;\n  }\n  return result;\n};\n\ndinf = function() {\n  return box(types.dinf, box(types.dref, DREF));\n};\n\nesds = function(track) {\n  return box(types.esds, new Uint8Array([\n    0x00, // version\n    0x00, 0x00, 0x00, // flags\n\n    // ES_Descriptor\n    0x03, // tag, ES_DescrTag\n    0x19, // length\n    0x00, 0x00, // ES_ID\n    0x00, // streamDependenceFlag, URL_flag, reserved, streamPriority\n\n    // DecoderConfigDescriptor\n    0x04, // tag, DecoderConfigDescrTag\n    0x11, // length\n    0x40, // object type\n    0x15,  // streamType\n    0x00, 0x06, 0x00, // bufferSizeDB\n    0x00, 0x00, 0xda, 0xc0, // maxBitrate\n    0x00, 0x00, 0xda, 0xc0, // avgBitrate\n\n    // DecoderSpecificInfo\n    0x05, // tag, DecoderSpecificInfoTag\n    0x02, // length\n    // ISO/IEC 14496-3, AudioSpecificConfig\n    // for samplingFrequencyIndex see ISO/IEC 13818-7:2006, 8.1.3.2.2, Table 35\n    (track.audioobjecttype << 3) | (track.samplingfrequencyindex >>> 1),\n    (track.samplingfrequencyindex << 7) | (track.channelcount << 3),\n    0x06, 0x01, 0x02 // GASpecificConfig\n  ]));\n};\n\nftyp = function() {\n  return box(types.ftyp, MAJOR_BRAND, MINOR_VERSION, MAJOR_BRAND, AVC1_BRAND);\n};\n\nhdlr = function(type) {\n  return box(types.hdlr, HDLR_TYPES[type]);\n};\nmdat = function(data) {\n  return box(types.mdat, data);\n};\nmdhd = function(track) {\n  var result = new Uint8Array([\n    0x00,                   // version 0\n    0x00, 0x00, 0x00,       // flags\n    0x00, 0x00, 0x00, 0x02, // creation_time\n    0x00, 0x00, 0x00, 0x03, // modification_time\n    0x00, 0x01, 0x5f, 0x90, // timescale, 90,000 \"ticks\" per second\n\n    (track.duration >>> 24) & 0xFF,\n    (track.duration >>> 16) & 0xFF,\n    (track.duration >>>  8) & 0xFF,\n    track.duration & 0xFF,  // duration\n    0x55, 0xc4,             // 'und' language (undetermined)\n    0x00, 0x00\n  ]);\n\n  // Use the sample rate from the track metadata, when it is\n  // defined. The sample rate can be parsed out of an ADTS header, for\n  // instance.\n  if (track.samplerate) {\n    result[12] = (track.samplerate >>> 24) & 0xFF;\n    result[13] = (track.samplerate >>> 16) & 0xFF;\n    result[14] = (track.samplerate >>>  8) & 0xFF;\n    result[15] = (track.samplerate)        & 0xFF;\n  }\n\n  return box(types.mdhd, result);\n};\nmdia = function(track) {\n  return box(types.mdia, mdhd(track), hdlr(track.type), minf(track));\n};\nmfhd = function(sequenceNumber) {\n  return box(types.mfhd, new Uint8Array([\n    0x00,\n    0x00, 0x00, 0x00, // flags\n    (sequenceNumber & 0xFF000000) >> 24,\n    (sequenceNumber & 0xFF0000) >> 16,\n    (sequenceNumber & 0xFF00) >> 8,\n    sequenceNumber & 0xFF // sequence_number\n  ]));\n};\nminf = function(track) {\n  return box(types.minf,\n             track.type === 'video' ? box(types.vmhd, VMHD) : box(types.smhd, SMHD),\n             dinf(),\n             stbl(track));\n};\nmoof = function(sequenceNumber, tracks) {\n  var\n    trackFragments = [],\n    i = tracks.length;\n  // build traf boxes for each track fragment\n  while (i--) {\n    trackFragments[i] = traf(tracks[i]);\n  }\n  return box.apply(null, [\n    types.moof,\n    mfhd(sequenceNumber)\n  ].concat(trackFragments));\n};\n/**\n * Returns a movie box.\n * @param tracks {array} the tracks associated with this movie\n * @see ISO/IEC 14496-12:2012(E), section 8.2.1\n */\nmoov = function(tracks) {\n  var\n    i = tracks.length,\n    boxes = [];\n\n  while (i--) {\n    boxes[i] = trak(tracks[i]);\n  }\n\n  return box.apply(null, [types.moov, mvhd(0xffffffff)].concat(boxes).concat(mvex(tracks)));\n};\nmvex = function(tracks) {\n  var\n    i = tracks.length,\n    boxes = [];\n\n  while (i--) {\n    boxes[i] = trex(tracks[i]);\n  }\n  return box.apply(null, [types.mvex].concat(boxes));\n};\nmvhd = function(duration) {\n  var\n    bytes = new Uint8Array([\n      0x00, // version 0\n      0x00, 0x00, 0x00, // flags\n      0x00, 0x00, 0x00, 0x01, // creation_time\n      0x00, 0x00, 0x00, 0x02, // modification_time\n      0x00, 0x01, 0x5f, 0x90, // timescale, 90,000 \"ticks\" per second\n      (duration & 0xFF000000) >> 24,\n      (duration & 0xFF0000) >> 16,\n      (duration & 0xFF00) >> 8,\n      duration & 0xFF, // duration\n      0x00, 0x01, 0x00, 0x00, // 1.0 rate\n      0x01, 0x00, // 1.0 volume\n      0x00, 0x00, // reserved\n      0x00, 0x00, 0x00, 0x00, // reserved\n      0x00, 0x00, 0x00, 0x00, // reserved\n      0x00, 0x01, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x01, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x40, 0x00, 0x00, 0x00, // transformation: unity matrix\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00, // pre_defined\n      0xff, 0xff, 0xff, 0xff // next_track_ID\n    ]);\n  return box(types.mvhd, bytes);\n};\n\nsdtp = function(track) {\n  var\n    samples = track.samples || [],\n    bytes = new Uint8Array(4 + samples.length),\n    flags,\n    i;\n\n  // leave the full box header (4 bytes) all zero\n\n  // write the sample table\n  for (i = 0; i < samples.length; i++) {\n    flags = samples[i].flags;\n\n    bytes[i + 4] = (flags.dependsOn << 4) |\n      (flags.isDependedOn << 2) |\n      (flags.hasRedundancy);\n  }\n\n  return box(types.sdtp,\n             bytes);\n};\n\nstbl = function(track) {\n  return box(types.stbl,\n             stsd(track),\n             box(types.stts, STTS),\n             box(types.stsc, STSC),\n             box(types.stsz, STSZ),\n             box(types.stco, STCO));\n};\n\n(function() {\n  var videoSample, audioSample;\n\n  stsd = function(track) {\n\n    return box(types.stsd, new Uint8Array([\n      0x00, // version 0\n      0x00, 0x00, 0x00, // flags\n      0x00, 0x00, 0x00, 0x01\n    ]), track.type === 'video' ? videoSample(track) : audioSample(track));\n  };\n\n  videoSample = function(track) {\n    var\n      sps = track.sps || [],\n      pps = track.pps || [],\n      sequenceParameterSets = [],\n      pictureParameterSets = [],\n      i;\n\n    // assemble the SPSs\n    for (i = 0; i < sps.length; i++) {\n      sequenceParameterSets.push((sps[i].byteLength & 0xFF00) >>> 8);\n      sequenceParameterSets.push((sps[i].byteLength & 0xFF)); // sequenceParameterSetLength\n      sequenceParameterSets = sequenceParameterSets.concat(Array.prototype.slice.call(sps[i])); // SPS\n    }\n\n    // assemble the PPSs\n    for (i = 0; i < pps.length; i++) {\n      pictureParameterSets.push((pps[i].byteLength & 0xFF00) >>> 8);\n      pictureParameterSets.push((pps[i].byteLength & 0xFF));\n      pictureParameterSets = pictureParameterSets.concat(Array.prototype.slice.call(pps[i]));\n    }\n\n    return box(types.avc1, new Uint8Array([\n      0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, // reserved\n      0x00, 0x01, // data_reference_index\n      0x00, 0x00, // pre_defined\n      0x00, 0x00, // reserved\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00, // pre_defined\n      (track.width & 0xff00) >> 8,\n      track.width & 0xff, // width\n      (track.height & 0xff00) >> 8,\n      track.height & 0xff, // height\n      0x00, 0x48, 0x00, 0x00, // horizresolution\n      0x00, 0x48, 0x00, 0x00, // vertresolution\n      0x00, 0x00, 0x00, 0x00, // reserved\n      0x00, 0x01, // frame_count\n      0x13,\n      0x76, 0x69, 0x64, 0x65,\n      0x6f, 0x6a, 0x73, 0x2d,\n      0x63, 0x6f, 0x6e, 0x74,\n      0x72, 0x69, 0x62, 0x2d,\n      0x68, 0x6c, 0x73, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, // compressorname\n      0x00, 0x18, // depth = 24\n      0x11, 0x11 // pre_defined = -1\n    ]), box(types.avcC, new Uint8Array([\n      0x01, // configurationVersion\n      track.profileIdc, // AVCProfileIndication\n      track.profileCompatibility, // profile_compatibility\n      track.levelIdc, // AVCLevelIndication\n      0xff // lengthSizeMinusOne, hard-coded to 4 bytes\n    ].concat([\n      sps.length // numOfSequenceParameterSets\n    ]).concat(sequenceParameterSets).concat([\n      pps.length // numOfPictureParameterSets\n    ]).concat(pictureParameterSets))), // \"PPS\"\n            box(types.btrt, new Uint8Array([\n              0x00, 0x1c, 0x9c, 0x80, // bufferSizeDB\n              0x00, 0x2d, 0xc6, 0xc0, // maxBitrate\n              0x00, 0x2d, 0xc6, 0xc0\n            ])) // avgBitrate\n              );\n  };\n\n  audioSample = function(track) {\n    return box(types.mp4a, new Uint8Array([\n\n      // SampleEntry, ISO/IEC 14496-12\n      0x00, 0x00, 0x00,\n      0x00, 0x00, 0x00, // reserved\n      0x00, 0x01, // data_reference_index\n\n      // AudioSampleEntry, ISO/IEC 14496-12\n      0x00, 0x00, 0x00, 0x00, // reserved\n      0x00, 0x00, 0x00, 0x00, // reserved\n      (track.channelcount & 0xff00) >> 8,\n      (track.channelcount & 0xff), // channelcount\n\n      (track.samplesize & 0xff00) >> 8,\n      (track.samplesize & 0xff), // samplesize\n      0x00, 0x00, // pre_defined\n      0x00, 0x00, // reserved\n\n      (track.samplerate & 0xff00) >> 8,\n      (track.samplerate & 0xff),\n      0x00, 0x00 // samplerate, 16.16\n\n      // MP4AudioSampleEntry, ISO/IEC 14496-14\n    ]), esds(track));\n  };\n}());\n\ntkhd = function(track) {\n  var result = new Uint8Array([\n    0x00, // version 0\n    0x00, 0x00, 0x07, // flags\n    0x00, 0x00, 0x00, 0x00, // creation_time\n    0x00, 0x00, 0x00, 0x00, // modification_time\n    (track.id & 0xFF000000) >> 24,\n    (track.id & 0xFF0000) >> 16,\n    (track.id & 0xFF00) >> 8,\n    track.id & 0xFF, // track_ID\n    0x00, 0x00, 0x00, 0x00, // reserved\n    (track.duration & 0xFF000000) >> 24,\n    (track.duration & 0xFF0000) >> 16,\n    (track.duration & 0xFF00) >> 8,\n    track.duration & 0xFF, // duration\n    0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00, // reserved\n    0x00, 0x00, // layer\n    0x00, 0x00, // alternate_group\n    0x01, 0x00, // non-audio track volume\n    0x00, 0x00, // reserved\n    0x00, 0x01, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00,\n    0x00, 0x01, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00,\n    0x00, 0x00, 0x00, 0x00,\n    0x40, 0x00, 0x00, 0x00, // transformation: unity matrix\n    (track.width & 0xFF00) >> 8,\n    track.width & 0xFF,\n    0x00, 0x00, // width\n    (track.height & 0xFF00) >> 8,\n    track.height & 0xFF,\n    0x00, 0x00 // height\n  ]);\n\n  return box(types.tkhd, result);\n};\n\n/**\n * Generate a track fragment (traf) box. A traf box collects metadata\n * about tracks in a movie fragment (moof) box.\n */\ntraf = function(track) {\n  var trackFragmentHeader, trackFragmentDecodeTime, trackFragmentRun,\n      sampleDependencyTable, dataOffset,\n      upperWordBaseMediaDecodeTime, lowerWordBaseMediaDecodeTime;\n\n  trackFragmentHeader = box(types.tfhd, new Uint8Array([\n    0x00, // version 0\n    0x00, 0x00, 0x3a, // flags\n    (track.id & 0xFF000000) >> 24,\n    (track.id & 0xFF0000) >> 16,\n    (track.id & 0xFF00) >> 8,\n    (track.id & 0xFF), // track_ID\n    0x00, 0x00, 0x00, 0x01, // sample_description_index\n    0x00, 0x00, 0x00, 0x00, // default_sample_duration\n    0x00, 0x00, 0x00, 0x00, // default_sample_size\n    0x00, 0x00, 0x00, 0x00  // default_sample_flags\n  ]));\n\n  upperWordBaseMediaDecodeTime = Math.floor(track.baseMediaDecodeTime / (UINT32_MAX + 1));\n  lowerWordBaseMediaDecodeTime = Math.floor(track.baseMediaDecodeTime % (UINT32_MAX + 1));\n\n  trackFragmentDecodeTime = box(types.tfdt, new Uint8Array([\n    0x01, // version 1\n    0x00, 0x00, 0x00, // flags\n    // baseMediaDecodeTime\n    (upperWordBaseMediaDecodeTime >>> 24) & 0xFF,\n    (upperWordBaseMediaDecodeTime >>> 16) & 0xFF,\n    (upperWordBaseMediaDecodeTime >>>  8) & 0xFF,\n    upperWordBaseMediaDecodeTime & 0xFF,\n    (lowerWordBaseMediaDecodeTime >>> 24) & 0xFF,\n    (lowerWordBaseMediaDecodeTime >>> 16) & 0xFF,\n    (lowerWordBaseMediaDecodeTime >>>  8) & 0xFF,\n    lowerWordBaseMediaDecodeTime & 0xFF\n  ]));\n\n  // the data offset specifies the number of bytes from the start of\n  // the containing moof to the first payload byte of the associated\n  // mdat\n  dataOffset = (32 + // tfhd\n                20 + // tfdt\n                8 +  // traf header\n                16 + // mfhd\n                8 +  // moof header\n                8);  // mdat header\n\n  // audio tracks require less metadata\n  if (track.type === 'audio') {\n    trackFragmentRun = trun(track, dataOffset);\n    return box(types.traf,\n               trackFragmentHeader,\n               trackFragmentDecodeTime,\n               trackFragmentRun);\n  }\n\n  // video tracks should contain an independent and disposable samples\n  // box (sdtp)\n  // generate one and adjust offsets to match\n  sampleDependencyTable = sdtp(track);\n  trackFragmentRun = trun(track,\n                          sampleDependencyTable.length + dataOffset);\n  return box(types.traf,\n             trackFragmentHeader,\n             trackFragmentDecodeTime,\n             trackFragmentRun,\n             sampleDependencyTable);\n};\n\n/**\n * Generate a track box.\n * @param track {object} a track definition\n * @return {Uint8Array} the track box\n */\ntrak = function(track) {\n  track.duration = track.duration || 0xffffffff;\n  return box(types.trak,\n             tkhd(track),\n             mdia(track));\n};\n\ntrex = function(track) {\n  var result = new Uint8Array([\n    0x00, // version 0\n    0x00, 0x00, 0x00, // flags\n    (track.id & 0xFF000000) >> 24,\n    (track.id & 0xFF0000) >> 16,\n    (track.id & 0xFF00) >> 8,\n    (track.id & 0xFF), // track_ID\n    0x00, 0x00, 0x00, 0x01, // default_sample_description_index\n    0x00, 0x00, 0x00, 0x00, // default_sample_duration\n    0x00, 0x00, 0x00, 0x00, // default_sample_size\n    0x00, 0x01, 0x00, 0x01 // default_sample_flags\n  ]);\n  // the last two bytes of default_sample_flags is the sample\n  // degradation priority, a hint about the importance of this sample\n  // relative to others. Lower the degradation priority for all sample\n  // types other than video.\n  if (track.type !== 'video') {\n    result[result.length - 1] = 0x00;\n  }\n\n  return box(types.trex, result);\n};\n\n(function() {\n  var audioTrun, videoTrun, trunHeader;\n\n  // This method assumes all samples are uniform. That is, if a\n  // duration is present for the first sample, it will be present for\n  // all subsequent samples.\n  // see ISO/IEC 14496-12:2012, Section 8.8.8.1\n  trunHeader = function(samples, offset) {\n    var durationPresent = 0, sizePresent = 0,\n        flagsPresent = 0, compositionTimeOffset = 0;\n\n    // trun flag constants\n    if (samples.length) {\n      if (samples[0].duration !== undefined) {\n        durationPresent = 0x1;\n      }\n      if (samples[0].size !== undefined) {\n        sizePresent = 0x2;\n      }\n      if (samples[0].flags !== undefined) {\n        flagsPresent = 0x4;\n      }\n      if (samples[0].compositionTimeOffset !== undefined) {\n        compositionTimeOffset = 0x8;\n      }\n    }\n\n    return [\n      0x00, // version 0\n      0x00,\n      durationPresent | sizePresent | flagsPresent | compositionTimeOffset,\n      0x01, // flags\n      (samples.length & 0xFF000000) >>> 24,\n      (samples.length & 0xFF0000) >>> 16,\n      (samples.length & 0xFF00) >>> 8,\n      samples.length & 0xFF, // sample_count\n      (offset & 0xFF000000) >>> 24,\n      (offset & 0xFF0000) >>> 16,\n      (offset & 0xFF00) >>> 8,\n      offset & 0xFF // data_offset\n    ];\n  };\n\n  videoTrun = function(track, offset) {\n    var bytes, samples, sample, i;\n\n    samples = track.samples || [];\n    offset += 8 + 12 + (16 * samples.length);\n\n    bytes = trunHeader(samples, offset);\n\n    for (i = 0; i < samples.length; i++) {\n      sample = samples[i];\n      bytes = bytes.concat([\n        (sample.duration & 0xFF000000) >>> 24,\n        (sample.duration & 0xFF0000) >>> 16,\n        (sample.duration & 0xFF00) >>> 8,\n        sample.duration & 0xFF, // sample_duration\n        (sample.size & 0xFF000000) >>> 24,\n        (sample.size & 0xFF0000) >>> 16,\n        (sample.size & 0xFF00) >>> 8,\n        sample.size & 0xFF, // sample_size\n        (sample.flags.isLeading << 2) | sample.flags.dependsOn,\n        (sample.flags.isDependedOn << 6) |\n          (sample.flags.hasRedundancy << 4) |\n          (sample.flags.paddingValue << 1) |\n          sample.flags.isNonSyncSample,\n        sample.flags.degradationPriority & 0xF0 << 8,\n        sample.flags.degradationPriority & 0x0F, // sample_flags\n        (sample.compositionTimeOffset & 0xFF000000) >>> 24,\n        (sample.compositionTimeOffset & 0xFF0000) >>> 16,\n        (sample.compositionTimeOffset & 0xFF00) >>> 8,\n        sample.compositionTimeOffset & 0xFF // sample_composition_time_offset\n      ]);\n    }\n    return box(types.trun, new Uint8Array(bytes));\n  };\n\n  audioTrun = function(track, offset) {\n    var bytes, samples, sample, i;\n\n    samples = track.samples || [];\n    offset += 8 + 12 + (8 * samples.length);\n\n    bytes = trunHeader(samples, offset);\n\n    for (i = 0; i < samples.length; i++) {\n      sample = samples[i];\n      bytes = bytes.concat([\n        (sample.duration & 0xFF000000) >>> 24,\n        (sample.duration & 0xFF0000) >>> 16,\n        (sample.duration & 0xFF00) >>> 8,\n        sample.duration & 0xFF, // sample_duration\n        (sample.size & 0xFF000000) >>> 24,\n        (sample.size & 0xFF0000) >>> 16,\n        (sample.size & 0xFF00) >>> 8,\n        sample.size & 0xFF]); // sample_size\n    }\n\n    return box(types.trun, new Uint8Array(bytes));\n  };\n\n  trun = function(track, offset) {\n    if (track.type === 'audio') {\n      return audioTrun(track, offset);\n    }\n\n    return videoTrun(track, offset);\n  };\n}());\n\nmodule.exports = {\n  ftyp: ftyp,\n  mdat: mdat,\n  moof: moof,\n  moov: moov,\n  initSegment: function(tracks) {\n    var\n      fileType = ftyp(),\n      movie = moov(tracks),\n      result;\n\n    result = new Uint8Array(fileType.byteLength + movie.byteLength);\n    result.set(fileType);\n    result.set(movie, fileType.byteLength);\n    return result;\n  }\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/mp4/mp4-generator.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/mp4/probe.js":
/*!**********************************************!*\
  !*** ./node_modules/mux.js/lib/mp4/probe.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2015 Brightcove\n * All rights reserved.\n *\n * Utilities to detect basic properties and metadata about MP4s.\n */\n\n\nvar findBox, parseType, timescale, startTime;\n\n// Find the data for a box specified by its path\nfindBox = function(data, path) {\n  var results = [],\n      i, size, type, end, subresults;\n\n  if (!path.length) {\n    // short-circuit the search for empty paths\n    return null;\n  }\n\n  for (i = 0; i < data.byteLength;) {\n    size  = data[i]     << 24;\n    size |= data[i + 1] << 16;\n    size |= data[i + 2] << 8;\n    size |= data[i + 3];\n\n    type = parseType(data.subarray(i + 4, i + 8));\n\n    end = size > 1 ? i + size : data.byteLength;\n\n    if (type === path[0]) {\n      if (path.length === 1) {\n        // this is the end of the path and we've found the box we were\n        // looking for\n        results.push(data.subarray(i + 8, end));\n      } else {\n        // recursively search for the next box along the path\n        subresults = findBox(data.subarray(i + 8, end), path.slice(1));\n        if (subresults.length) {\n          results = results.concat(subresults);\n        }\n      }\n    }\n    i = end;\n  }\n\n  // we've finished searching all of data\n  return results;\n};\n\n/**\n * Returns the string representation of an ASCII encoded four byte buffer.\n * @param buffer {Uint8Array} a four-byte buffer to translate\n * @return {string} the corresponding string\n */\nparseType = function(buffer) {\n  var result = '';\n  result += String.fromCharCode(buffer[0]);\n  result += String.fromCharCode(buffer[1]);\n  result += String.fromCharCode(buffer[2]);\n  result += String.fromCharCode(buffer[3]);\n  return result;\n};\n\n/**\n * Parses an MP4 initialization segment and extracts the timescale\n * values for any declared tracks. Timescale values indicate the\n * number of clock ticks per second to assume for time-based values\n * elsewhere in the MP4.\n *\n * To determine the start time of an MP4, you need two pieces of\n * information: the timescale unit and the earliest base media decode\n * time. Multiple timescales can be specified within an MP4 but the\n * base media decode time is always expressed in the timescale from\n * the media header box for the track:\n * ```\n * moov > trak > mdia > mdhd.timescale\n * ```\n * @param init {Uint8Array} the bytes of the init segment\n * @return {object} a hash of track ids to timescale values or null if\n * the init segment is malformed.\n */\ntimescale = function(init) {\n  var\n    result = {},\n    traks = findBox(init, ['moov', 'trak']);\n\n  // mdhd timescale\n  return traks.reduce(function(result, trak) {\n    var tkhd, version, index, id, mdhd;\n\n    tkhd = findBox(trak, ['tkhd'])[0];\n    if (!tkhd) {\n      return null;\n    }\n    version = tkhd[0];\n    index = version === 0 ? 12 : 20;\n    id = tkhd[index]     << 24 |\n         tkhd[index + 1] << 16 |\n         tkhd[index + 2] <<  8 |\n         tkhd[index + 3];\n\n    mdhd = findBox(trak, ['mdia', 'mdhd'])[0];\n    if (!mdhd) {\n      return null;\n    }\n    version = mdhd[0];\n    index = version === 0 ? 12 : 20;\n    result[id] = mdhd[index]     << 24 |\n                 mdhd[index + 1] << 16 |\n                 mdhd[index + 2] <<  8 |\n                 mdhd[index + 3];\n    return result;\n  }, result);\n};\n\n/**\n * Determine the base media decode start time, in seconds, for an MP4\n * fragment. If multiple fragments are specified, the earliest time is\n * returned.\n *\n * The base media decode time can be parsed from track fragment\n * metadata:\n * ```\n * moof > traf > tfdt.baseMediaDecodeTime\n * ```\n * It requires the timescale value from the mdhd to interpret.\n *\n * @param timescale {object} a hash of track ids to timescale values.\n * @return {number} the earliest base media decode start time for the\n * fragment, in seconds\n */\nstartTime = function(timescale, fragment) {\n  var trafs, baseTimes, result;\n\n  // we need info from two childrend of each track fragment box\n  trafs = findBox(fragment, ['moof', 'traf']);\n\n  // determine the start times for each track\n  baseTimes = [].concat.apply([], trafs.map(function(traf) {\n    return findBox(traf, ['tfhd']).map(function(tfhd) {\n      var id, scale, baseTime;\n\n      // get the track id from the tfhd\n      id = tfhd[4] << 24 |\n           tfhd[5] << 16 |\n           tfhd[6] << 8 |\n           tfhd[7];\n      // assume a 90kHz clock if no timescale was specified\n      scale = timescale[id] || 90e3;\n\n      // get the base media decode time from the tfdt\n      baseTime = findBox(traf, ['tfdt']).map(function(tfdt) {\n        var version, result;\n\n        version = tfdt[0];\n        result = tfdt[4] << 24 |\n                 tfdt[5] << 16 |\n                 tfdt[6] <<  8 |\n                 tfdt[7];\n        if (version ===  1) {\n          result *= Math.pow(2, 32);\n          result += tfdt[8]  << 24 |\n                    tfdt[9]  << 16 |\n                    tfdt[10] <<  8 |\n                    tfdt[11];\n        }\n        return result;\n      })[0];\n      baseTime = baseTime || Infinity;\n\n      // convert base time to seconds\n      return baseTime / scale;\n    });\n  }));\n\n  // return the minimum\n  result = Math.min.apply(null, baseTimes);\n  return isFinite(result) ? result : 0;\n};\n\nmodule.exports = {\n  parseType: parseType,\n  timescale: timescale,\n  startTime: startTime\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/mp4/probe.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/mp4/transmuxer.js":
/*!***************************************************!*\
  !*** ./node_modules/mux.js/lib/mp4/transmuxer.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2015 Brightcove\n * All rights reserved.\n *\n * A stream-based mp2t to mp4 converter. This utility can be used to\n * deliver mp4s to a SourceBuffer on platforms that support native\n * Media Source Extensions.\n */\n\n\nvar Stream = __webpack_require__(/*! ../utils/stream.js */ \"./node_modules/mux.js/lib/utils/stream.js\");\nvar mp4 = __webpack_require__(/*! ./mp4-generator.js */ \"./node_modules/mux.js/lib/mp4/mp4-generator.js\");\nvar m2ts = __webpack_require__(/*! ../m2ts/m2ts.js */ \"./node_modules/mux.js/lib/m2ts/m2ts.js\");\nvar AdtsStream = __webpack_require__(/*! ../codecs/adts.js */ \"./node_modules/mux.js/lib/codecs/adts.js\");\nvar H264Stream = __webpack_require__(/*! ../codecs/h264 */ \"./node_modules/mux.js/lib/codecs/h264.js\").H264Stream;\nvar AacStream = __webpack_require__(/*! ../aac */ \"./node_modules/mux.js/lib/aac/index.js\");\nvar coneOfSilence = __webpack_require__(/*! ../data/silence */ \"./node_modules/mux.js/lib/data/silence.js\");\nvar clock = __webpack_require__(/*! ../utils/clock */ \"./node_modules/mux.js/lib/utils/clock.js\");\n\n// constants\nvar AUDIO_PROPERTIES = [\n  'audioobjecttype',\n  'channelcount',\n  'samplerate',\n  'samplingfrequencyindex',\n  'samplesize'\n];\n\nvar VIDEO_PROPERTIES = [\n  'width',\n  'height',\n  'profileIdc',\n  'levelIdc',\n  'profileCompatibility'\n];\n\nvar ONE_SECOND_IN_TS = 90000; // 90kHz clock\n\n// object types\nvar VideoSegmentStream, AudioSegmentStream, Transmuxer, CoalesceStream;\n\n// Helper functions\nvar\n  createDefaultSample,\n  isLikelyAacData,\n  collectDtsInfo,\n  clearDtsInfo,\n  calculateTrackBaseMediaDecodeTime,\n  arrayEquals,\n  sumFrameByteLengths;\n\n/**\n * Default sample object\n * see ISO/IEC 14496-12:2012, section 8.6.4.3\n */\ncreateDefaultSample = function() {\n  return {\n    size: 0,\n    flags: {\n      isLeading: 0,\n      dependsOn: 1,\n      isDependedOn: 0,\n      hasRedundancy: 0,\n      degradationPriority: 0\n    }\n  };\n};\n\nisLikelyAacData = function(data) {\n  if ((data[0] === 'I'.charCodeAt(0)) &&\n      (data[1] === 'D'.charCodeAt(0)) &&\n      (data[2] === '3'.charCodeAt(0))) {\n    return true;\n  }\n  return false;\n};\n\n/**\n * Compare two arrays (even typed) for same-ness\n */\narrayEquals = function(a, b) {\n  var\n    i;\n\n  if (a.length !== b.length) {\n    return false;\n  }\n\n  // compare the value of each element in the array\n  for (i = 0; i < a.length; i++) {\n    if (a[i] !== b[i]) {\n      return false;\n    }\n  }\n\n  return true;\n};\n\n/**\n * Sum the `byteLength` properties of the data in each AAC frame\n */\nsumFrameByteLengths = function(array) {\n  var\n    i,\n    currentObj,\n    sum = 0;\n\n  // sum the byteLength's all each nal unit in the frame\n  for (i = 0; i < array.length; i++) {\n    currentObj = array[i];\n    sum += currentObj.data.byteLength;\n  }\n\n  return sum;\n};\n\n/**\n * Constructs a single-track, ISO BMFF media segment from AAC data\n * events. The output of this stream can be fed to a SourceBuffer\n * configured with a suitable initialization segment.\n */\nAudioSegmentStream = function(track) {\n  var\n    adtsFrames = [],\n    sequenceNumber = 0,\n    earliestAllowedDts = 0,\n    audioAppendStartTs = 0,\n    videoBaseMediaDecodeTime = Infinity;\n\n  AudioSegmentStream.prototype.init.call(this);\n\n  this.push = function(data) {\n    collectDtsInfo(track, data);\n\n    if (track) {\n      AUDIO_PROPERTIES.forEach(function(prop) {\n        track[prop] = data[prop];\n      });\n    }\n\n    // buffer audio data until end() is called\n    adtsFrames.push(data);\n  };\n\n  this.setEarliestDts = function(earliestDts) {\n    earliestAllowedDts = earliestDts - track.timelineStartInfo.baseMediaDecodeTime;\n  };\n\n  this.setVideoBaseMediaDecodeTime = function(baseMediaDecodeTime) {\n    videoBaseMediaDecodeTime = baseMediaDecodeTime;\n  };\n\n  this.setAudioAppendStart = function(timestamp) {\n    audioAppendStartTs = timestamp;\n  };\n\n  this.flush = function() {\n    var\n      frames,\n      moof,\n      mdat,\n      boxes;\n\n    // return early if no audio data has been observed\n    if (adtsFrames.length === 0) {\n      this.trigger('done', 'AudioSegmentStream');\n      return;\n    }\n\n    frames = this.trimAdtsFramesByEarliestDts_(adtsFrames);\n    track.baseMediaDecodeTime = calculateTrackBaseMediaDecodeTime(track);\n\n    this.prefixWithSilence_(track, frames);\n\n    // we have to build the index from byte locations to\n    // samples (that is, adts frames) in the audio data\n    track.samples = this.generateSampleTable_(frames);\n\n    // concatenate the audio data to constuct the mdat\n    mdat = mp4.mdat(this.concatenateFrameData_(frames));\n\n    adtsFrames = [];\n\n    moof = mp4.moof(sequenceNumber, [track]);\n    boxes = new Uint8Array(moof.byteLength + mdat.byteLength);\n\n    // bump the sequence number for next time\n    sequenceNumber++;\n\n    boxes.set(moof);\n    boxes.set(mdat, moof.byteLength);\n\n    clearDtsInfo(track);\n\n    this.trigger('data', {track: track, boxes: boxes});\n    this.trigger('done', 'AudioSegmentStream');\n  };\n\n  // Possibly pad (prefix) the audio track with silence if appending this track\n  // would lead to the introduction of a gap in the audio buffer\n  this.prefixWithSilence_ = function(track, frames) {\n    var\n      baseMediaDecodeTimeTs,\n      frameDuration = 0,\n      audioGapDuration = 0,\n      audioFillFrameCount = 0,\n      audioFillDuration = 0,\n      silentFrame,\n      i;\n\n    if (!frames.length) {\n      return;\n    }\n\n    baseMediaDecodeTimeTs = clock.audioTsToVideoTs(track.baseMediaDecodeTime, track.samplerate);\n    // determine frame clock duration based on sample rate, round up to avoid overfills\n    frameDuration = Math.ceil(ONE_SECOND_IN_TS / (track.samplerate / 1024));\n\n    if (audioAppendStartTs && videoBaseMediaDecodeTime) {\n      // insert the shortest possible amount (audio gap or audio to video gap)\n      audioGapDuration =\n        baseMediaDecodeTimeTs - Math.max(audioAppendStartTs, videoBaseMediaDecodeTime);\n      // number of full frames in the audio gap\n      audioFillFrameCount = Math.floor(audioGapDuration / frameDuration);\n      audioFillDuration = audioFillFrameCount * frameDuration;\n    }\n\n    // don't attempt to fill gaps smaller than a single frame or larger\n    // than a half second\n    if (audioFillFrameCount < 1 || audioFillDuration > ONE_SECOND_IN_TS / 2) {\n      return;\n    }\n\n    silentFrame = coneOfSilence[track.samplerate];\n\n    if (!silentFrame) {\n      // we don't have a silent frame pregenerated for the sample rate, so use a frame\n      // from the content instead\n      silentFrame = frames[0].data;\n    }\n\n    for (i = 0; i < audioFillFrameCount; i++) {\n      frames.splice(i, 0, {\n        data: silentFrame\n      });\n    }\n\n    track.baseMediaDecodeTime -=\n      Math.floor(clock.videoTsToAudioTs(audioFillDuration, track.samplerate));\n  };\n\n  // If the audio segment extends before the earliest allowed dts\n  // value, remove AAC frames until starts at or after the earliest\n  // allowed DTS so that we don't end up with a negative baseMedia-\n  // DecodeTime for the audio track\n  this.trimAdtsFramesByEarliestDts_ = function(adtsFrames) {\n    if (track.minSegmentDts >= earliestAllowedDts) {\n      return adtsFrames;\n    }\n\n    // We will need to recalculate the earliest segment Dts\n    track.minSegmentDts = Infinity;\n\n    return adtsFrames.filter(function(currentFrame) {\n      // If this is an allowed frame, keep it and record it's Dts\n      if (currentFrame.dts >= earliestAllowedDts) {\n        track.minSegmentDts = Math.min(track.minSegmentDts, currentFrame.dts);\n        track.minSegmentPts = track.minSegmentDts;\n        return true;\n      }\n      // Otherwise, discard it\n      return false;\n    });\n  };\n\n  // generate the track's raw mdat data from an array of frames\n  this.generateSampleTable_ = function(frames) {\n    var\n      i,\n      currentFrame,\n      samples = [];\n\n    for (i = 0; i < frames.length; i++) {\n      currentFrame = frames[i];\n      samples.push({\n        size: currentFrame.data.byteLength,\n        duration: 1024 // For AAC audio, all samples contain 1024 samples\n      });\n    }\n    return samples;\n  };\n\n  // generate the track's sample table from an array of frames\n  this.concatenateFrameData_ = function(frames) {\n    var\n      i,\n      currentFrame,\n      dataOffset = 0,\n      data = new Uint8Array(sumFrameByteLengths(frames));\n\n    for (i = 0; i < frames.length; i++) {\n      currentFrame = frames[i];\n\n      data.set(currentFrame.data, dataOffset);\n      dataOffset += currentFrame.data.byteLength;\n    }\n    return data;\n  };\n};\n\nAudioSegmentStream.prototype = new Stream();\n\n/**\n * Constructs a single-track, ISO BMFF media segment from H264 data\n * events. The output of this stream can be fed to a SourceBuffer\n * configured with a suitable initialization segment.\n * @param track {object} track metadata configuration\n * @param options {object} transmuxer options object\n * @param options.alignGopsAtEnd {boolean} If true, start from the end of the\n *        gopsToAlignWith list when attempting to align gop pts\n */\nVideoSegmentStream = function(track, options) {\n  var\n    sequenceNumber = 0,\n    nalUnits = [],\n    gopsToAlignWith = [],\n    config,\n    pps;\n\n  options = options || {};\n\n  VideoSegmentStream.prototype.init.call(this);\n\n  delete track.minPTS;\n\n  this.gopCache_ = [];\n\n  this.push = function(nalUnit) {\n    collectDtsInfo(track, nalUnit);\n\n    // record the track config\n    if (nalUnit.nalUnitType === 'seq_parameter_set_rbsp' && !config) {\n      config = nalUnit.config;\n      track.sps = [nalUnit.data];\n\n      VIDEO_PROPERTIES.forEach(function(prop) {\n        track[prop] = config[prop];\n      }, this);\n    }\n\n    if (nalUnit.nalUnitType === 'pic_parameter_set_rbsp' &&\n        !pps) {\n      pps = nalUnit.data;\n      track.pps = [nalUnit.data];\n    }\n\n    // buffer video until flush() is called\n    nalUnits.push(nalUnit);\n  };\n\n  this.flush = function() {\n    var\n      frames,\n      gopForFusion,\n      gops,\n      moof,\n      mdat,\n      boxes;\n\n    // Throw away nalUnits at the start of the byte stream until\n    // we find the first AUD\n    while (nalUnits.length) {\n      if (nalUnits[0].nalUnitType === 'access_unit_delimiter_rbsp') {\n        break;\n      }\n      nalUnits.shift();\n    }\n\n    // Return early if no video data has been observed\n    if (nalUnits.length === 0) {\n      this.resetStream_();\n      this.trigger('done', 'VideoSegmentStream');\n      return;\n    }\n\n    // Organize the raw nal-units into arrays that represent\n    // higher-level constructs such as frames and gops\n    // (group-of-pictures)\n    frames = this.groupNalsIntoFrames_(nalUnits);\n    gops = this.groupFramesIntoGops_(frames);\n\n    // If the first frame of this fragment is not a keyframe we have\n    // a problem since MSE (on Chrome) requires a leading keyframe.\n    //\n    // We have two approaches to repairing this situation:\n    // 1) GOP-FUSION:\n    //    This is where we keep track of the GOPS (group-of-pictures)\n    //    from previous fragments and attempt to find one that we can\n    //    prepend to the current fragment in order to create a valid\n    //    fragment.\n    // 2) KEYFRAME-PULLING:\n    //    Here we search for the first keyframe in the fragment and\n    //    throw away all the frames between the start of the fragment\n    //    and that keyframe. We then extend the duration and pull the\n    //    PTS of the keyframe forward so that it covers the time range\n    //    of the frames that were disposed of.\n    //\n    // #1 is far prefereable over #2 which can cause \"stuttering\" but\n    // requires more things to be just right.\n    if (!gops[0][0].keyFrame) {\n      // Search for a gop for fusion from our gopCache\n      gopForFusion = this.getGopForFusion_(nalUnits[0], track);\n\n      if (gopForFusion) {\n        gops.unshift(gopForFusion);\n        // Adjust Gops' metadata to account for the inclusion of the\n        // new gop at the beginning\n        gops.byteLength += gopForFusion.byteLength;\n        gops.nalCount += gopForFusion.nalCount;\n        gops.pts = gopForFusion.pts;\n        gops.dts = gopForFusion.dts;\n        gops.duration += gopForFusion.duration;\n      } else {\n        // If we didn't find a candidate gop fall back to keyrame-pulling\n        gops = this.extendFirstKeyFrame_(gops);\n      }\n    }\n\n    // Trim gops to align with gopsToAlignWith\n    if (gopsToAlignWith.length) {\n      var alignedGops;\n\n      if (options.alignGopsAtEnd) {\n        alignedGops = this.alignGopsAtEnd_(gops);\n      } else {\n        alignedGops = this.alignGopsAtStart_(gops);\n      }\n\n      if (!alignedGops) {\n        // save all the nals in the last GOP into the gop cache\n        this.gopCache_.unshift({\n          gop: gops.pop(),\n          pps: track.pps,\n          sps: track.sps\n        });\n\n        // Keep a maximum of 6 GOPs in the cache\n        this.gopCache_.length = Math.min(6, this.gopCache_.length);\n\n        // Clear nalUnits\n        nalUnits = [];\n\n        // return early no gops can be aligned with desired gopsToAlignWith\n        this.resetStream_();\n        this.trigger('done', 'VideoSegmentStream');\n        return;\n      }\n\n      // Some gops were trimmed. clear dts info so minSegmentDts and pts are correct\n      // when recalculated before sending off to CoalesceStream\n      clearDtsInfo(track);\n\n      gops = alignedGops;\n    }\n\n    collectDtsInfo(track, gops);\n\n    // First, we have to build the index from byte locations to\n    // samples (that is, frames) in the video data\n    track.samples = this.generateSampleTable_(gops);\n\n    // Concatenate the video data and construct the mdat\n    mdat = mp4.mdat(this.concatenateNalData_(gops));\n\n    track.baseMediaDecodeTime = calculateTrackBaseMediaDecodeTime(track);\n\n    this.trigger('processedGopsInfo', gops.map(function(gop) {\n      return {\n        pts: gop.pts,\n        dts: gop.dts,\n        byteLength: gop.byteLength\n      };\n    }));\n\n    // save all the nals in the last GOP into the gop cache\n    this.gopCache_.unshift({\n      gop: gops.pop(),\n      pps: track.pps,\n      sps: track.sps\n    });\n\n    // Keep a maximum of 6 GOPs in the cache\n    this.gopCache_.length = Math.min(6, this.gopCache_.length);\n\n    // Clear nalUnits\n    nalUnits = [];\n\n    this.trigger('baseMediaDecodeTime', track.baseMediaDecodeTime);\n    this.trigger('timelineStartInfo', track.timelineStartInfo);\n\n    moof = mp4.moof(sequenceNumber, [track]);\n\n    // it would be great to allocate this array up front instead of\n    // throwing away hundreds of media segment fragments\n    boxes = new Uint8Array(moof.byteLength + mdat.byteLength);\n\n    // Bump the sequence number for next time\n    sequenceNumber++;\n\n    boxes.set(moof);\n    boxes.set(mdat, moof.byteLength);\n\n    this.trigger('data', {track: track, boxes: boxes});\n\n    this.resetStream_();\n\n    // Continue with the flush process now\n    this.trigger('done', 'VideoSegmentStream');\n  };\n\n  this.resetStream_ = function() {\n    clearDtsInfo(track);\n\n    // reset config and pps because they may differ across segments\n    // for instance, when we are rendition switching\n    config = undefined;\n    pps = undefined;\n  };\n\n  // Search for a candidate Gop for gop-fusion from the gop cache and\n  // return it or return null if no good candidate was found\n  this.getGopForFusion_ = function(nalUnit) {\n    var\n      halfSecond = 45000, // Half-a-second in a 90khz clock\n      allowableOverlap = 10000, // About 3 frames @ 30fps\n      nearestDistance = Infinity,\n      dtsDistance,\n      nearestGopObj,\n      currentGop,\n      currentGopObj,\n      i;\n\n    // Search for the GOP nearest to the beginning of this nal unit\n    for (i = 0; i < this.gopCache_.length; i++) {\n      currentGopObj = this.gopCache_[i];\n      currentGop = currentGopObj.gop;\n\n      // Reject Gops with different SPS or PPS\n      if (!(track.pps && arrayEquals(track.pps[0], currentGopObj.pps[0])) ||\n          !(track.sps && arrayEquals(track.sps[0], currentGopObj.sps[0]))) {\n        continue;\n      }\n\n      // Reject Gops that would require a negative baseMediaDecodeTime\n      if (currentGop.dts < track.timelineStartInfo.dts) {\n        continue;\n      }\n\n      // The distance between the end of the gop and the start of the nalUnit\n      dtsDistance = (nalUnit.dts - currentGop.dts) - currentGop.duration;\n\n      // Only consider GOPS that start before the nal unit and end within\n      // a half-second of the nal unit\n      if (dtsDistance >= -allowableOverlap &&\n          dtsDistance <= halfSecond) {\n\n        // Always use the closest GOP we found if there is more than\n        // one candidate\n        if (!nearestGopObj ||\n            nearestDistance > dtsDistance) {\n          nearestGopObj = currentGopObj;\n          nearestDistance = dtsDistance;\n        }\n      }\n    }\n\n    if (nearestGopObj) {\n      return nearestGopObj.gop;\n    }\n    return null;\n  };\n\n  this.extendFirstKeyFrame_ = function(gops) {\n    var currentGop;\n\n    if (!gops[0][0].keyFrame && gops.length > 1) {\n      // Remove the first GOP\n      currentGop = gops.shift();\n\n      gops.byteLength -=  currentGop.byteLength;\n      gops.nalCount -= currentGop.nalCount;\n\n      // Extend the first frame of what is now the\n      // first gop to cover the time period of the\n      // frames we just removed\n      gops[0][0].dts = currentGop.dts;\n      gops[0][0].pts = currentGop.pts;\n      gops[0][0].duration += currentGop.duration;\n    }\n\n    return gops;\n  };\n\n  // Convert an array of nal units into an array of frames with each frame being\n  // composed of the nal units that make up that frame\n  // Also keep track of cummulative data about the frame from the nal units such\n  // as the frame duration, starting pts, etc.\n  this.groupNalsIntoFrames_ = function(nalUnits) {\n    var\n      i,\n      currentNal,\n      currentFrame = [],\n      frames = [];\n\n    currentFrame.byteLength = 0;\n\n    for (i = 0; i < nalUnits.length; i++) {\n      currentNal = nalUnits[i];\n\n      // Split on 'aud'-type nal units\n      if (currentNal.nalUnitType === 'access_unit_delimiter_rbsp') {\n        // Since the very first nal unit is expected to be an AUD\n        // only push to the frames array when currentFrame is not empty\n        if (currentFrame.length) {\n          currentFrame.duration = currentNal.dts - currentFrame.dts;\n          frames.push(currentFrame);\n        }\n        currentFrame = [currentNal];\n        currentFrame.byteLength = currentNal.data.byteLength;\n        currentFrame.pts = currentNal.pts;\n        currentFrame.dts = currentNal.dts;\n      } else {\n        // Specifically flag key frames for ease of use later\n        if (currentNal.nalUnitType === 'slice_layer_without_partitioning_rbsp_idr') {\n          currentFrame.keyFrame = true;\n        }\n        currentFrame.duration = currentNal.dts - currentFrame.dts;\n        currentFrame.byteLength += currentNal.data.byteLength;\n        currentFrame.push(currentNal);\n      }\n    }\n\n    // For the last frame, use the duration of the previous frame if we\n    // have nothing better to go on\n    if (frames.length &&\n        (!currentFrame.duration ||\n         currentFrame.duration <= 0)) {\n      currentFrame.duration = frames[frames.length - 1].duration;\n    }\n\n    // Push the final frame\n    frames.push(currentFrame);\n    return frames;\n  };\n\n  // Convert an array of frames into an array of Gop with each Gop being composed\n  // of the frames that make up that Gop\n  // Also keep track of cummulative data about the Gop from the frames such as the\n  // Gop duration, starting pts, etc.\n  this.groupFramesIntoGops_ = function(frames) {\n    var\n      i,\n      currentFrame,\n      currentGop = [],\n      gops = [];\n\n    // We must pre-set some of the values on the Gop since we\n    // keep running totals of these values\n    currentGop.byteLength = 0;\n    currentGop.nalCount = 0;\n    currentGop.duration = 0;\n    currentGop.pts = frames[0].pts;\n    currentGop.dts = frames[0].dts;\n\n    // store some metadata about all the Gops\n    gops.byteLength = 0;\n    gops.nalCount = 0;\n    gops.duration = 0;\n    gops.pts = frames[0].pts;\n    gops.dts = frames[0].dts;\n\n    for (i = 0; i < frames.length; i++) {\n      currentFrame = frames[i];\n\n      if (currentFrame.keyFrame) {\n        // Since the very first frame is expected to be an keyframe\n        // only push to the gops array when currentGop is not empty\n        if (currentGop.length) {\n          gops.push(currentGop);\n          gops.byteLength += currentGop.byteLength;\n          gops.nalCount += currentGop.nalCount;\n          gops.duration += currentGop.duration;\n        }\n\n        currentGop = [currentFrame];\n        currentGop.nalCount = currentFrame.length;\n        currentGop.byteLength = currentFrame.byteLength;\n        currentGop.pts = currentFrame.pts;\n        currentGop.dts = currentFrame.dts;\n        currentGop.duration = currentFrame.duration;\n      } else {\n        currentGop.duration += currentFrame.duration;\n        currentGop.nalCount += currentFrame.length;\n        currentGop.byteLength += currentFrame.byteLength;\n        currentGop.push(currentFrame);\n      }\n    }\n\n    if (gops.length && currentGop.duration <= 0) {\n      currentGop.duration = gops[gops.length - 1].duration;\n    }\n    gops.byteLength += currentGop.byteLength;\n    gops.nalCount += currentGop.nalCount;\n    gops.duration += currentGop.duration;\n\n    // push the final Gop\n    gops.push(currentGop);\n    return gops;\n  };\n\n  // generate the track's sample table from an array of gops\n  this.generateSampleTable_ = function(gops, baseDataOffset) {\n    var\n      h, i,\n      sample,\n      currentGop,\n      currentFrame,\n      dataOffset = baseDataOffset || 0,\n      samples = [];\n\n    for (h = 0; h < gops.length; h++) {\n      currentGop = gops[h];\n\n      for (i = 0; i < currentGop.length; i++) {\n        currentFrame = currentGop[i];\n\n        sample = createDefaultSample();\n\n        sample.dataOffset = dataOffset;\n        sample.compositionTimeOffset = currentFrame.pts - currentFrame.dts;\n        sample.duration = currentFrame.duration;\n        sample.size = 4 * currentFrame.length; // Space for nal unit size\n        sample.size += currentFrame.byteLength;\n\n        if (currentFrame.keyFrame) {\n          sample.flags.dependsOn = 2;\n        }\n\n        dataOffset += sample.size;\n\n        samples.push(sample);\n      }\n    }\n    return samples;\n  };\n\n  // generate the track's raw mdat data from an array of gops\n  this.concatenateNalData_ = function(gops) {\n    var\n      h, i, j,\n      currentGop,\n      currentFrame,\n      currentNal,\n      dataOffset = 0,\n      nalsByteLength = gops.byteLength,\n      numberOfNals = gops.nalCount,\n      totalByteLength = nalsByteLength + 4 * numberOfNals,\n      data = new Uint8Array(totalByteLength),\n      view = new DataView(data.buffer);\n\n    // For each Gop..\n    for (h = 0; h < gops.length; h++) {\n      currentGop = gops[h];\n\n      // For each Frame..\n      for (i = 0; i < currentGop.length; i++) {\n        currentFrame = currentGop[i];\n\n        // For each NAL..\n        for (j = 0; j < currentFrame.length; j++) {\n          currentNal = currentFrame[j];\n\n          view.setUint32(dataOffset, currentNal.data.byteLength);\n          dataOffset += 4;\n          data.set(currentNal.data, dataOffset);\n          dataOffset += currentNal.data.byteLength;\n        }\n      }\n    }\n    return data;\n  };\n\n  // trim gop list to the first gop found that has a matching pts with a gop in the list\n  // of gopsToAlignWith starting from the START of the list\n  this.alignGopsAtStart_ = function(gops) {\n    var alignIndex, gopIndex, align, gop, byteLength, nalCount, duration, alignedGops;\n\n    byteLength = gops.byteLength;\n    nalCount = gops.nalCount;\n    duration = gops.duration;\n    alignIndex = gopIndex = 0;\n\n    while (alignIndex < gopsToAlignWith.length && gopIndex < gops.length) {\n      align = gopsToAlignWith[alignIndex];\n      gop = gops[gopIndex];\n\n      if (align.pts === gop.pts) {\n        break;\n      }\n\n      if (gop.pts > align.pts) {\n        // this current gop starts after the current gop we want to align on, so increment\n        // align index\n        alignIndex++;\n        continue;\n      }\n\n      // current gop starts before the current gop we want to align on. so increment gop\n      // index\n      gopIndex++;\n      byteLength -= gop.byteLength;\n      nalCount -= gop.nalCount;\n      duration -= gop.duration;\n    }\n\n    if (gopIndex === 0) {\n      // no gops to trim\n      return gops;\n    }\n\n    if (gopIndex === gops.length) {\n      // all gops trimmed, skip appending all gops\n      return null;\n    }\n\n    alignedGops = gops.slice(gopIndex);\n    alignedGops.byteLength = byteLength;\n    alignedGops.duration = duration;\n    alignedGops.nalCount = nalCount;\n    alignedGops.pts = alignedGops[0].pts;\n    alignedGops.dts = alignedGops[0].dts;\n\n    return alignedGops;\n  };\n\n  // trim gop list to the first gop found that has a matching pts with a gop in the list\n  // of gopsToAlignWith starting from the END of the list\n  this.alignGopsAtEnd_ = function(gops) {\n    var alignIndex, gopIndex, align, gop, alignEndIndex, matchFound;\n\n    alignIndex = gopsToAlignWith.length - 1;\n    gopIndex = gops.length - 1;\n    alignEndIndex = null;\n    matchFound = false;\n\n    while (alignIndex >= 0 && gopIndex >= 0) {\n      align = gopsToAlignWith[alignIndex];\n      gop = gops[gopIndex];\n\n      if (align.pts === gop.pts) {\n        matchFound = true;\n        break;\n      }\n\n      if (align.pts > gop.pts) {\n        alignIndex--;\n        continue;\n      }\n\n      if (alignIndex === gopsToAlignWith.length - 1) {\n        // gop.pts is greater than the last alignment candidate. If no match is found\n        // by the end of this loop, we still want to append gops that come after this\n        // point\n        alignEndIndex = gopIndex;\n      }\n\n      gopIndex--;\n    }\n\n    if (!matchFound && alignEndIndex === null) {\n      return null;\n    }\n\n    var trimIndex;\n\n    if (matchFound) {\n      trimIndex = gopIndex;\n    } else {\n      trimIndex = alignEndIndex;\n    }\n\n    if (trimIndex === 0) {\n      return gops;\n    }\n\n    var alignedGops = gops.slice(trimIndex);\n    var metadata = alignedGops.reduce(function(total, gop) {\n      total.byteLength += gop.byteLength;\n      total.duration += gop.duration;\n      total.nalCount += gop.nalCount;\n      return total;\n    }, { byteLength: 0, duration: 0, nalCount: 0 });\n\n    alignedGops.byteLength = metadata.byteLength;\n    alignedGops.duration = metadata.duration;\n    alignedGops.nalCount = metadata.nalCount;\n    alignedGops.pts = alignedGops[0].pts;\n    alignedGops.dts = alignedGops[0].dts;\n\n    return alignedGops;\n  };\n\n  this.alignGopsWith = function(newGopsToAlignWith) {\n    gopsToAlignWith = newGopsToAlignWith;\n  };\n};\n\nVideoSegmentStream.prototype = new Stream();\n\n/**\n * Store information about the start and end of the track and the\n * duration for each frame/sample we process in order to calculate\n * the baseMediaDecodeTime\n */\ncollectDtsInfo = function(track, data) {\n  if (typeof data.pts === 'number') {\n    if (track.timelineStartInfo.pts === undefined) {\n      track.timelineStartInfo.pts = data.pts;\n    }\n\n    if (track.minSegmentPts === undefined) {\n      track.minSegmentPts = data.pts;\n    } else {\n      track.minSegmentPts = Math.min(track.minSegmentPts, data.pts);\n    }\n\n    if (track.maxSegmentPts === undefined) {\n      track.maxSegmentPts = data.pts;\n    } else {\n      track.maxSegmentPts = Math.max(track.maxSegmentPts, data.pts);\n    }\n  }\n\n  if (typeof data.dts === 'number') {\n    if (track.timelineStartInfo.dts === undefined) {\n      track.timelineStartInfo.dts = data.dts;\n    }\n\n    if (track.minSegmentDts === undefined) {\n      track.minSegmentDts = data.dts;\n    } else {\n      track.minSegmentDts = Math.min(track.minSegmentDts, data.dts);\n    }\n\n    if (track.maxSegmentDts === undefined) {\n      track.maxSegmentDts = data.dts;\n    } else {\n      track.maxSegmentDts = Math.max(track.maxSegmentDts, data.dts);\n    }\n  }\n};\n\n/**\n * Clear values used to calculate the baseMediaDecodeTime between\n * tracks\n */\nclearDtsInfo = function(track) {\n  delete track.minSegmentDts;\n  delete track.maxSegmentDts;\n  delete track.minSegmentPts;\n  delete track.maxSegmentPts;\n};\n\n/**\n * Calculate the track's baseMediaDecodeTime based on the earliest\n * DTS the transmuxer has ever seen and the minimum DTS for the\n * current track\n */\ncalculateTrackBaseMediaDecodeTime = function(track) {\n  var\n    baseMediaDecodeTime,\n    scale,\n    // Calculate the distance, in time, that this segment starts from the start\n    // of the timeline (earliest time seen since the transmuxer initialized)\n    timeSinceStartOfTimeline = track.minSegmentDts - track.timelineStartInfo.dts;\n\n  // track.timelineStartInfo.baseMediaDecodeTime is the location, in time, where\n  // we want the start of the first segment to be placed\n  baseMediaDecodeTime = track.timelineStartInfo.baseMediaDecodeTime;\n\n  // Add to that the distance this segment is from the very first\n  baseMediaDecodeTime += timeSinceStartOfTimeline;\n\n  // baseMediaDecodeTime must not become negative\n  baseMediaDecodeTime = Math.max(0, baseMediaDecodeTime);\n\n  if (track.type === 'audio') {\n    // Audio has a different clock equal to the sampling_rate so we need to\n    // scale the PTS values into the clock rate of the track\n    scale = track.samplerate / ONE_SECOND_IN_TS;\n    baseMediaDecodeTime *= scale;\n    baseMediaDecodeTime = Math.floor(baseMediaDecodeTime);\n  }\n\n  return baseMediaDecodeTime;\n};\n\n/**\n * A Stream that can combine multiple streams (ie. audio & video)\n * into a single output segment for MSE. Also supports audio-only\n * and video-only streams.\n */\nCoalesceStream = function(options, metadataStream) {\n  // Number of Tracks per output segment\n  // If greater than 1, we combine multiple\n  // tracks into a single segment\n  this.numberOfTracks = 0;\n  this.metadataStream = metadataStream;\n\n  if (typeof options.remux !== 'undefined') {\n    this.remuxTracks = !!options.remux;\n  } else {\n    this.remuxTracks = true;\n  }\n\n  this.pendingTracks = [];\n  this.videoTrack = null;\n  this.pendingBoxes = [];\n  this.pendingCaptions = [];\n  this.pendingMetadata = [];\n  this.pendingBytes = 0;\n  this.emittedTracks = 0;\n\n  CoalesceStream.prototype.init.call(this);\n\n  // Take output from multiple\n  this.push = function(output) {\n    // buffer incoming captions until the associated video segment\n    // finishes\n    if (output.text) {\n      return this.pendingCaptions.push(output);\n    }\n    // buffer incoming id3 tags until the final flush\n    if (output.frames) {\n      return this.pendingMetadata.push(output);\n    }\n\n    // Add this track to the list of pending tracks and store\n    // important information required for the construction of\n    // the final segment\n    this.pendingTracks.push(output.track);\n    this.pendingBoxes.push(output.boxes);\n    this.pendingBytes += output.boxes.byteLength;\n\n    if (output.track.type === 'video') {\n      this.videoTrack = output.track;\n    }\n    if (output.track.type === 'audio') {\n      this.audioTrack = output.track;\n    }\n  };\n};\n\nCoalesceStream.prototype = new Stream();\nCoalesceStream.prototype.flush = function(flushSource) {\n  var\n    offset = 0,\n    event = {\n      captions: [],\n      captionStreams: {},\n      metadata: [],\n      info: {}\n    },\n    caption,\n    id3,\n    initSegment,\n    timelineStartPts = 0,\n    i;\n\n  if (this.pendingTracks.length < this.numberOfTracks) {\n    if (flushSource !== 'VideoSegmentStream' &&\n        flushSource !== 'AudioSegmentStream') {\n      // Return because we haven't received a flush from a data-generating\n      // portion of the segment (meaning that we have only recieved meta-data\n      // or captions.)\n      return;\n    } else if (this.remuxTracks) {\n      // Return until we have enough tracks from the pipeline to remux (if we\n      // are remuxing audio and video into a single MP4)\n      return;\n    } else if (this.pendingTracks.length === 0) {\n      // In the case where we receive a flush without any data having been\n      // received we consider it an emitted track for the purposes of coalescing\n      // `done` events.\n      // We do this for the case where there is an audio and video track in the\n      // segment but no audio data. (seen in several playlists with alternate\n      // audio tracks and no audio present in the main TS segments.)\n      this.emittedTracks++;\n\n      if (this.emittedTracks >= this.numberOfTracks) {\n        this.trigger('done');\n        this.emittedTracks = 0;\n      }\n      return;\n    }\n  }\n\n  if (this.videoTrack) {\n    timelineStartPts = this.videoTrack.timelineStartInfo.pts;\n    VIDEO_PROPERTIES.forEach(function(prop) {\n      event.info[prop] = this.videoTrack[prop];\n    }, this);\n  } else if (this.audioTrack) {\n    timelineStartPts = this.audioTrack.timelineStartInfo.pts;\n    AUDIO_PROPERTIES.forEach(function(prop) {\n      event.info[prop] = this.audioTrack[prop];\n    }, this);\n  }\n\n  if (this.pendingTracks.length === 1) {\n    event.type = this.pendingTracks[0].type;\n  } else {\n    event.type = 'combined';\n  }\n\n  this.emittedTracks += this.pendingTracks.length;\n\n  initSegment = mp4.initSegment(this.pendingTracks);\n\n  // Create a new typed array to hold the init segment\n  event.initSegment = new Uint8Array(initSegment.byteLength);\n\n  // Create an init segment containing a moov\n  // and track definitions\n  event.initSegment.set(initSegment);\n\n  // Create a new typed array to hold the moof+mdats\n  event.data = new Uint8Array(this.pendingBytes);\n\n  // Append each moof+mdat (one per track) together\n  for (i = 0; i < this.pendingBoxes.length; i++) {\n    event.data.set(this.pendingBoxes[i], offset);\n    offset += this.pendingBoxes[i].byteLength;\n  }\n\n  // Translate caption PTS times into second offsets into the\n  // video timeline for the segment, and add track info\n  for (i = 0; i < this.pendingCaptions.length; i++) {\n    caption = this.pendingCaptions[i];\n    caption.startTime = (caption.startPts - timelineStartPts);\n    caption.startTime /= 90e3;\n    caption.endTime = (caption.endPts - timelineStartPts);\n    caption.endTime /= 90e3;\n    event.captionStreams[caption.stream] = true;\n    event.captions.push(caption);\n  }\n\n  // Translate ID3 frame PTS times into second offsets into the\n  // video timeline for the segment\n  for (i = 0; i < this.pendingMetadata.length; i++) {\n    id3 = this.pendingMetadata[i];\n    id3.cueTime = (id3.pts - timelineStartPts);\n    id3.cueTime /= 90e3;\n    event.metadata.push(id3);\n  }\n  // We add this to every single emitted segment even though we only need\n  // it for the first\n  event.metadata.dispatchType = this.metadataStream.dispatchType;\n\n  // Reset stream state\n  this.pendingTracks.length = 0;\n  this.videoTrack = null;\n  this.pendingBoxes.length = 0;\n  this.pendingCaptions.length = 0;\n  this.pendingBytes = 0;\n  this.pendingMetadata.length = 0;\n\n  // Emit the built segment\n  this.trigger('data', event);\n\n  // Only emit `done` if all tracks have been flushed and emitted\n  if (this.emittedTracks >= this.numberOfTracks) {\n    this.trigger('done');\n    this.emittedTracks = 0;\n  }\n};\n/**\n * A Stream that expects MP2T binary data as input and produces\n * corresponding media segments, suitable for use with Media Source\n * Extension (MSE) implementations that support the ISO BMFF byte\n * stream format, like Chrome.\n */\nTransmuxer = function(options) {\n  var\n    self = this,\n    hasFlushed = true,\n    videoTrack,\n    audioTrack;\n\n  Transmuxer.prototype.init.call(this);\n\n  options = options || {};\n  this.baseMediaDecodeTime = options.baseMediaDecodeTime || 0;\n  this.transmuxPipeline_ = {};\n\n  this.setupAacPipeline = function() {\n    var pipeline = {};\n    this.transmuxPipeline_ = pipeline;\n\n    pipeline.type = 'aac';\n    pipeline.metadataStream = new m2ts.MetadataStream();\n\n    // set up the parsing pipeline\n    pipeline.aacStream = new AacStream();\n    pipeline.audioTimestampRolloverStream = new m2ts.TimestampRolloverStream('audio');\n    pipeline.timedMetadataTimestampRolloverStream = new m2ts.TimestampRolloverStream('timed-metadata');\n    pipeline.adtsStream = new AdtsStream();\n    pipeline.coalesceStream = new CoalesceStream(options, pipeline.metadataStream);\n    pipeline.headOfPipeline = pipeline.aacStream;\n\n    pipeline.aacStream\n      .pipe(pipeline.audioTimestampRolloverStream)\n      .pipe(pipeline.adtsStream);\n    pipeline.aacStream\n      .pipe(pipeline.timedMetadataTimestampRolloverStream)\n      .pipe(pipeline.metadataStream)\n      .pipe(pipeline.coalesceStream);\n\n    pipeline.metadataStream.on('timestamp', function(frame) {\n      pipeline.aacStream.setTimestamp(frame.timeStamp);\n    });\n\n    pipeline.aacStream.on('data', function(data) {\n      if (data.type === 'timed-metadata' && !pipeline.audioSegmentStream) {\n        audioTrack = audioTrack || {\n          timelineStartInfo: {\n            baseMediaDecodeTime: self.baseMediaDecodeTime\n          },\n          codec: 'adts',\n          type: 'audio'\n        };\n        // hook up the audio segment stream to the first track with aac data\n        pipeline.coalesceStream.numberOfTracks++;\n        pipeline.audioSegmentStream = new AudioSegmentStream(audioTrack);\n        // Set up the final part of the audio pipeline\n        pipeline.adtsStream\n          .pipe(pipeline.audioSegmentStream)\n          .pipe(pipeline.coalesceStream);\n      }\n    });\n\n    // Re-emit any data coming from the coalesce stream to the outside world\n    pipeline.coalesceStream.on('data', this.trigger.bind(this, 'data'));\n    // Let the consumer know we have finished flushing the entire pipeline\n    pipeline.coalesceStream.on('done', this.trigger.bind(this, 'done'));\n  };\n\n  this.setupTsPipeline = function() {\n    var pipeline = {};\n    this.transmuxPipeline_ = pipeline;\n\n    pipeline.type = 'ts';\n    pipeline.metadataStream = new m2ts.MetadataStream();\n\n    // set up the parsing pipeline\n    pipeline.packetStream = new m2ts.TransportPacketStream();\n    pipeline.parseStream = new m2ts.TransportParseStream();\n    pipeline.elementaryStream = new m2ts.ElementaryStream();\n    pipeline.videoTimestampRolloverStream = new m2ts.TimestampRolloverStream('video');\n    pipeline.audioTimestampRolloverStream = new m2ts.TimestampRolloverStream('audio');\n    pipeline.timedMetadataTimestampRolloverStream = new m2ts.TimestampRolloverStream('timed-metadata');\n    pipeline.adtsStream = new AdtsStream();\n    pipeline.h264Stream = new H264Stream();\n    pipeline.captionStream = new m2ts.CaptionStream();\n    pipeline.coalesceStream = new CoalesceStream(options, pipeline.metadataStream);\n    pipeline.headOfPipeline = pipeline.packetStream;\n\n    // disassemble MPEG2-TS packets into elementary streams\n    pipeline.packetStream\n      .pipe(pipeline.parseStream)\n      .pipe(pipeline.elementaryStream);\n\n    // !!THIS ORDER IS IMPORTANT!!\n    // demux the streams\n    pipeline.elementaryStream\n      .pipe(pipeline.videoTimestampRolloverStream)\n      .pipe(pipeline.h264Stream);\n    pipeline.elementaryStream\n      .pipe(pipeline.audioTimestampRolloverStream)\n      .pipe(pipeline.adtsStream);\n\n    pipeline.elementaryStream\n      .pipe(pipeline.timedMetadataTimestampRolloverStream)\n      .pipe(pipeline.metadataStream)\n      .pipe(pipeline.coalesceStream);\n\n    // Hook up CEA-608/708 caption stream\n    pipeline.h264Stream.pipe(pipeline.captionStream)\n      .pipe(pipeline.coalesceStream);\n\n    pipeline.elementaryStream.on('data', function(data) {\n      var i;\n\n      if (data.type === 'metadata') {\n        i = data.tracks.length;\n\n        // scan the tracks listed in the metadata\n        while (i--) {\n          if (!videoTrack && data.tracks[i].type === 'video') {\n            videoTrack = data.tracks[i];\n            videoTrack.timelineStartInfo.baseMediaDecodeTime = self.baseMediaDecodeTime;\n          } else if (!audioTrack && data.tracks[i].type === 'audio') {\n            audioTrack = data.tracks[i];\n            audioTrack.timelineStartInfo.baseMediaDecodeTime = self.baseMediaDecodeTime;\n          }\n        }\n\n        // hook up the video segment stream to the first track with h264 data\n        if (videoTrack && !pipeline.videoSegmentStream) {\n          pipeline.coalesceStream.numberOfTracks++;\n          pipeline.videoSegmentStream = new VideoSegmentStream(videoTrack, options);\n\n          pipeline.videoSegmentStream.on('timelineStartInfo', function(timelineStartInfo) {\n          // When video emits timelineStartInfo data after a flush, we forward that\n          // info to the AudioSegmentStream, if it exists, because video timeline\n          // data takes precedence.\n            if (audioTrack) {\n              audioTrack.timelineStartInfo = timelineStartInfo;\n              // On the first segment we trim AAC frames that exist before the\n              // very earliest DTS we have seen in video because Chrome will\n              // interpret any video track with a baseMediaDecodeTime that is\n              // non-zero as a gap.\n              pipeline.audioSegmentStream.setEarliestDts(timelineStartInfo.dts);\n            }\n          });\n\n          pipeline.videoSegmentStream.on('processedGopsInfo',\n            self.trigger.bind(self, 'gopInfo'));\n\n          pipeline.videoSegmentStream.on('baseMediaDecodeTime', function(baseMediaDecodeTime) {\n            if (audioTrack) {\n              pipeline.audioSegmentStream.setVideoBaseMediaDecodeTime(baseMediaDecodeTime);\n            }\n          });\n\n          // Set up the final part of the video pipeline\n          pipeline.h264Stream\n            .pipe(pipeline.videoSegmentStream)\n            .pipe(pipeline.coalesceStream);\n        }\n\n        if (audioTrack && !pipeline.audioSegmentStream) {\n          // hook up the audio segment stream to the first track with aac data\n          pipeline.coalesceStream.numberOfTracks++;\n          pipeline.audioSegmentStream = new AudioSegmentStream(audioTrack);\n\n          // Set up the final part of the audio pipeline\n          pipeline.adtsStream\n            .pipe(pipeline.audioSegmentStream)\n            .pipe(pipeline.coalesceStream);\n        }\n      }\n    });\n\n    // Re-emit any data coming from the coalesce stream to the outside world\n    pipeline.coalesceStream.on('data', this.trigger.bind(this, 'data'));\n    // Let the consumer know we have finished flushing the entire pipeline\n    pipeline.coalesceStream.on('done', this.trigger.bind(this, 'done'));\n  };\n\n  // hook up the segment streams once track metadata is delivered\n  this.setBaseMediaDecodeTime = function(baseMediaDecodeTime) {\n    var pipeline = this.transmuxPipeline_;\n\n    this.baseMediaDecodeTime = baseMediaDecodeTime;\n    if (audioTrack) {\n      audioTrack.timelineStartInfo.dts = undefined;\n      audioTrack.timelineStartInfo.pts = undefined;\n      clearDtsInfo(audioTrack);\n      audioTrack.timelineStartInfo.baseMediaDecodeTime = baseMediaDecodeTime;\n      if (pipeline.audioTimestampRolloverStream) {\n        pipeline.audioTimestampRolloverStream.discontinuity();\n      }\n    }\n    if (videoTrack) {\n      if (pipeline.videoSegmentStream) {\n        pipeline.videoSegmentStream.gopCache_ = [];\n        pipeline.videoTimestampRolloverStream.discontinuity();\n      }\n      videoTrack.timelineStartInfo.dts = undefined;\n      videoTrack.timelineStartInfo.pts = undefined;\n      clearDtsInfo(videoTrack);\n      pipeline.captionStream.reset();\n      videoTrack.timelineStartInfo.baseMediaDecodeTime = baseMediaDecodeTime;\n    }\n\n    if (pipeline.timedMetadataTimestampRolloverStream) {\n      pipeline.timedMetadataTimestampRolloverStream.discontinuity();\n    }\n  };\n\n  this.setAudioAppendStart = function(timestamp) {\n    if (audioTrack) {\n      this.transmuxPipeline_.audioSegmentStream.setAudioAppendStart(timestamp);\n    }\n  };\n\n  this.alignGopsWith = function(gopsToAlignWith) {\n    if (videoTrack && this.transmuxPipeline_.videoSegmentStream) {\n      this.transmuxPipeline_.videoSegmentStream.alignGopsWith(gopsToAlignWith);\n    }\n  };\n\n  // feed incoming data to the front of the parsing pipeline\n  this.push = function(data) {\n    if (hasFlushed) {\n      var isAac = isLikelyAacData(data);\n\n      if (isAac && this.transmuxPipeline_.type !== 'aac') {\n        this.setupAacPipeline();\n      } else if (!isAac && this.transmuxPipeline_.type !== 'ts') {\n        this.setupTsPipeline();\n      }\n      hasFlushed = false;\n    }\n    this.transmuxPipeline_.headOfPipeline.push(data);\n  };\n\n  // flush any buffered data\n  this.flush = function() {\n      hasFlushed = true;\n    // Start at the top of the pipeline and flush all pending work\n    this.transmuxPipeline_.headOfPipeline.flush();\n  };\n\n  // Caption data has to be reset when seeking outside buffered range\n  this.resetCaptions = function() {\n    if (this.transmuxPipeline_.captionStream) {\n      this.transmuxPipeline_.captionStream.reset();\n    }\n  };\n\n};\nTransmuxer.prototype = new Stream();\n\nmodule.exports = {\n  Transmuxer: Transmuxer,\n  VideoSegmentStream: VideoSegmentStream,\n  AudioSegmentStream: AudioSegmentStream,\n  AUDIO_PROPERTIES: AUDIO_PROPERTIES,\n  VIDEO_PROPERTIES: VIDEO_PROPERTIES\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/mp4/transmuxer.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/tools/ts-inspector.js":
/*!*******************************************************!*\
  !*** ./node_modules/mux.js/lib/tools/ts-inspector.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2016 Brightcove\n * All rights reserved.\n *\n * Parse mpeg2 transport stream packets to extract basic timing information\n */\n\n\nvar StreamTypes = __webpack_require__(/*! ../m2ts/stream-types.js */ \"./node_modules/mux.js/lib/m2ts/stream-types.js\");\nvar handleRollover = __webpack_require__(/*! ../m2ts/timestamp-rollover-stream.js */ \"./node_modules/mux.js/lib/m2ts/timestamp-rollover-stream.js\").handleRollover;\nvar probe = {};\nprobe.ts = __webpack_require__(/*! ../m2ts/probe.js */ \"./node_modules/mux.js/lib/m2ts/probe.js\");\nprobe.aac = __webpack_require__(/*! ../aac/probe.js */ \"./node_modules/mux.js/lib/aac/probe.js\");\n\n\nvar\n  PES_TIMESCALE = 90000,\n  MP2T_PACKET_LENGTH = 188, // bytes\n  SYNC_BYTE = 0x47;\n\nvar isLikelyAacData = function(data) {\n  if ((data[0] === 'I'.charCodeAt(0)) &&\n      (data[1] === 'D'.charCodeAt(0)) &&\n      (data[2] === '3'.charCodeAt(0))) {\n    return true;\n  }\n  return false;\n};\n\n/**\n * walks through segment data looking for pat and pmt packets to parse out\n * program map table information\n */\nvar parsePsi_ = function(bytes, pmt) {\n  var\n    startIndex = 0,\n    endIndex = MP2T_PACKET_LENGTH,\n    packet, type;\n\n  while (endIndex < bytes.byteLength) {\n    // Look for a pair of start and end sync bytes in the data..\n    if (bytes[startIndex] === SYNC_BYTE && bytes[endIndex] === SYNC_BYTE) {\n      // We found a packet\n      packet = bytes.subarray(startIndex, endIndex);\n      type = probe.ts.parseType(packet, pmt.pid);\n\n      switch (type) {\n        case 'pat':\n          if (!pmt.pid) {\n            pmt.pid = probe.ts.parsePat(packet);\n          }\n          break;\n        case 'pmt':\n          if (!pmt.table) {\n            pmt.table = probe.ts.parsePmt(packet);\n          }\n          break;\n        default:\n          break;\n      }\n\n      // Found the pat and pmt, we can stop walking the segment\n      if (pmt.pid && pmt.table) {\n        return;\n      }\n\n      startIndex += MP2T_PACKET_LENGTH;\n      endIndex += MP2T_PACKET_LENGTH;\n      continue;\n    }\n\n    // If we get here, we have somehow become de-synchronized and we need to step\n    // forward one byte at a time until we find a pair of sync bytes that denote\n    // a packet\n    startIndex++;\n    endIndex++;\n  }\n};\n\n/**\n * walks through the segment data from the start and end to get timing information\n * for the first and last audio pes packets\n */\nvar parseAudioPes_ = function(bytes, pmt, result) {\n  var\n    startIndex = 0,\n    endIndex = MP2T_PACKET_LENGTH,\n    packet, type, pesType, pusi, parsed;\n\n  var endLoop = false;\n\n  // Start walking from start of segment to get first audio packet\n  while (endIndex < bytes.byteLength) {\n    // Look for a pair of start and end sync bytes in the data..\n    if (bytes[startIndex] === SYNC_BYTE && bytes[endIndex] === SYNC_BYTE) {\n      // We found a packet\n      packet = bytes.subarray(startIndex, endIndex);\n      type = probe.ts.parseType(packet, pmt.pid);\n\n      switch (type) {\n        case 'pes':\n          pesType = probe.ts.parsePesType(packet, pmt.table);\n          pusi = probe.ts.parsePayloadUnitStartIndicator(packet);\n          if (pesType === 'audio' && pusi) {\n            parsed = probe.ts.parsePesTime(packet);\n            if (parsed) {\n              parsed.type = 'audio';\n              result.audio.push(parsed);\n              endLoop = true;\n            }\n          }\n          break;\n        default:\n          break;\n      }\n\n      if (endLoop) {\n        break;\n      }\n\n      startIndex += MP2T_PACKET_LENGTH;\n      endIndex += MP2T_PACKET_LENGTH;\n      continue;\n    }\n\n    // If we get here, we have somehow become de-synchronized and we need to step\n    // forward one byte at a time until we find a pair of sync bytes that denote\n    // a packet\n    startIndex++;\n    endIndex++;\n  }\n\n  // Start walking from end of segment to get last audio packet\n  endIndex = bytes.byteLength;\n  startIndex = endIndex - MP2T_PACKET_LENGTH;\n  endLoop = false;\n  while (startIndex >= 0) {\n    // Look for a pair of start and end sync bytes in the data..\n    if (bytes[startIndex] === SYNC_BYTE && bytes[endIndex] === SYNC_BYTE) {\n      // We found a packet\n      packet = bytes.subarray(startIndex, endIndex);\n      type = probe.ts.parseType(packet, pmt.pid);\n\n      switch (type) {\n        case 'pes':\n          pesType = probe.ts.parsePesType(packet, pmt.table);\n          pusi = probe.ts.parsePayloadUnitStartIndicator(packet);\n          if (pesType === 'audio' && pusi) {\n            parsed = probe.ts.parsePesTime(packet);\n            if (parsed) {\n              parsed.type = 'audio';\n              result.audio.push(parsed);\n              endLoop = true;\n            }\n          }\n          break;\n        default:\n          break;\n      }\n\n      if (endLoop) {\n        break;\n      }\n\n      startIndex -= MP2T_PACKET_LENGTH;\n      endIndex -= MP2T_PACKET_LENGTH;\n      continue;\n    }\n\n    // If we get here, we have somehow become de-synchronized and we need to step\n    // forward one byte at a time until we find a pair of sync bytes that denote\n    // a packet\n    startIndex--;\n    endIndex--;\n  }\n};\n\n/**\n * walks through the segment data from the start and end to get timing information\n * for the first and last video pes packets as well as timing information for the first\n * key frame.\n */\nvar parseVideoPes_ = function(bytes, pmt, result) {\n  var\n    startIndex = 0,\n    endIndex = MP2T_PACKET_LENGTH,\n    packet, type, pesType, pusi, parsed, frame, i, pes;\n\n  var endLoop = false;\n\n  var currentFrame = {\n    data: [],\n    size: 0\n  };\n\n  // Start walking from start of segment to get first video packet\n  while (endIndex < bytes.byteLength) {\n    // Look for a pair of start and end sync bytes in the data..\n    if (bytes[startIndex] === SYNC_BYTE && bytes[endIndex] === SYNC_BYTE) {\n      // We found a packet\n      packet = bytes.subarray(startIndex, endIndex);\n      type = probe.ts.parseType(packet, pmt.pid);\n\n      switch (type) {\n        case 'pes':\n          pesType = probe.ts.parsePesType(packet, pmt.table);\n          pusi = probe.ts.parsePayloadUnitStartIndicator(packet);\n          if (pesType === 'video') {\n            if (pusi && !endLoop) {\n              parsed = probe.ts.parsePesTime(packet);\n              if (parsed) {\n                parsed.type = 'video';\n                result.video.push(parsed);\n                endLoop = true;\n              }\n            }\n            if (!result.firstKeyFrame) {\n              if (pusi) {\n                if (currentFrame.size !== 0) {\n                  frame = new Uint8Array(currentFrame.size);\n                  i = 0;\n                  while (currentFrame.data.length) {\n                    pes = currentFrame.data.shift();\n                    frame.set(pes, i);\n                    i += pes.byteLength;\n                  }\n                  if (probe.ts.videoPacketContainsKeyFrame(frame)) {\n                    result.firstKeyFrame = probe.ts.parsePesTime(frame);\n                    result.firstKeyFrame.type = 'video';\n                  }\n                  currentFrame.size = 0;\n                }\n              }\n              currentFrame.data.push(packet);\n              currentFrame.size += packet.byteLength;\n            }\n          }\n          break;\n        default:\n          break;\n      }\n\n      if (endLoop && result.firstKeyFrame) {\n        break;\n      }\n\n      startIndex += MP2T_PACKET_LENGTH;\n      endIndex += MP2T_PACKET_LENGTH;\n      continue;\n    }\n\n    // If we get here, we have somehow become de-synchronized and we need to step\n    // forward one byte at a time until we find a pair of sync bytes that denote\n    // a packet\n    startIndex++;\n    endIndex++;\n  }\n\n  // Start walking from end of segment to get last video packet\n  endIndex = bytes.byteLength;\n  startIndex = endIndex - MP2T_PACKET_LENGTH;\n  endLoop = false;\n  while (startIndex >= 0) {\n    // Look for a pair of start and end sync bytes in the data..\n    if (bytes[startIndex] === SYNC_BYTE && bytes[endIndex] === SYNC_BYTE) {\n      // We found a packet\n      packet = bytes.subarray(startIndex, endIndex);\n      type = probe.ts.parseType(packet, pmt.pid);\n\n      switch (type) {\n        case 'pes':\n          pesType = probe.ts.parsePesType(packet, pmt.table);\n          pusi = probe.ts.parsePayloadUnitStartIndicator(packet);\n          if (pesType === 'video' && pusi) {\n              parsed = probe.ts.parsePesTime(packet);\n              if (parsed) {\n                parsed.type = 'video';\n                result.video.push(parsed);\n                endLoop = true;\n              }\n          }\n          break;\n        default:\n          break;\n      }\n\n      if (endLoop) {\n        break;\n      }\n\n      startIndex -= MP2T_PACKET_LENGTH;\n      endIndex -= MP2T_PACKET_LENGTH;\n      continue;\n    }\n\n    // If we get here, we have somehow become de-synchronized and we need to step\n    // forward one byte at a time until we find a pair of sync bytes that denote\n    // a packet\n    startIndex--;\n    endIndex--;\n  }\n};\n\n/**\n * Adjusts the timestamp information for the segment to account for\n * rollover and convert to seconds based on pes packet timescale (90khz clock)\n */\nvar adjustTimestamp_ = function(segmentInfo, baseTimestamp) {\n  if (segmentInfo.audio && segmentInfo.audio.length) {\n    var audioBaseTimestamp = baseTimestamp;\n    if (typeof audioBaseTimestamp === 'undefined') {\n      audioBaseTimestamp = segmentInfo.audio[0].dts;\n    }\n    segmentInfo.audio.forEach(function(info) {\n      info.dts = handleRollover(info.dts, audioBaseTimestamp);\n      info.pts = handleRollover(info.pts, audioBaseTimestamp);\n      // time in seconds\n      info.dtsTime = info.dts / PES_TIMESCALE;\n      info.ptsTime = info.pts / PES_TIMESCALE;\n    });\n  }\n\n  if (segmentInfo.video && segmentInfo.video.length) {\n    var videoBaseTimestamp = baseTimestamp;\n    if (typeof videoBaseTimestamp === 'undefined') {\n      videoBaseTimestamp = segmentInfo.video[0].dts;\n    }\n    segmentInfo.video.forEach(function(info) {\n      info.dts = handleRollover(info.dts, videoBaseTimestamp);\n      info.pts = handleRollover(info.pts, videoBaseTimestamp);\n      // time in seconds\n      info.dtsTime = info.dts / PES_TIMESCALE;\n      info.ptsTime = info.pts / PES_TIMESCALE;\n    });\n    if (segmentInfo.firstKeyFrame) {\n      var frame = segmentInfo.firstKeyFrame;\n      frame.dts = handleRollover(frame.dts, videoBaseTimestamp);\n      frame.pts = handleRollover(frame.pts, videoBaseTimestamp);\n      // time in seconds\n      frame.dtsTime = frame.dts / PES_TIMESCALE;\n      frame.ptsTime = frame.dts / PES_TIMESCALE;\n    }\n  }\n};\n\n/**\n * inspects the aac data stream for start and end time information\n */\nvar inspectAac_ = function(bytes) {\n  var\n    endLoop = false,\n    audioCount = 0,\n    sampleRate = null,\n    timestamp = null,\n    frameSize = 0,\n    byteIndex = 0,\n    packet;\n\n  while (bytes.length - byteIndex >= 3) {\n    var type = probe.aac.parseType(bytes, byteIndex);\n    switch (type) {\n      case 'timed-metadata':\n        // Exit early because we don't have enough to parse\n        // the ID3 tag header\n        if (bytes.length - byteIndex < 10) {\n          endLoop = true;\n          break;\n        }\n\n        frameSize = probe.aac.parseId3TagSize(bytes, byteIndex);\n\n        // Exit early if we don't have enough in the buffer\n        // to emit a full packet\n        if (frameSize > bytes.length) {\n          endLoop = true;\n          break;\n        }\n        if (timestamp === null) {\n          packet = bytes.subarray(byteIndex, byteIndex + frameSize);\n          timestamp = probe.aac.parseAacTimestamp(packet);\n        }\n        byteIndex += frameSize;\n        break;\n      case 'audio':\n        // Exit early because we don't have enough to parse\n        // the ADTS frame header\n        if (bytes.length - byteIndex < 7) {\n          endLoop = true;\n          break;\n        }\n\n        frameSize = probe.aac.parseAdtsSize(bytes, byteIndex);\n\n        // Exit early if we don't have enough in the buffer\n        // to emit a full packet\n        if (frameSize > bytes.length) {\n          endLoop = true;\n          break;\n        }\n        if (sampleRate === null) {\n          packet = bytes.subarray(byteIndex, byteIndex + frameSize);\n          sampleRate = probe.aac.parseSampleRate(packet);\n        }\n        audioCount++;\n        byteIndex += frameSize;\n        break;\n      default:\n        byteIndex++;\n        break;\n    }\n    if (endLoop) {\n      return null;\n    }\n  }\n  if (sampleRate === null || timestamp === null) {\n    return null;\n  }\n\n  var audioTimescale = PES_TIMESCALE / sampleRate;\n\n  var result = {\n    audio: [\n      {\n        type: 'audio',\n        dts: timestamp,\n        pts: timestamp\n      },\n      {\n        type: 'audio',\n        dts: timestamp + (audioCount * 1024 * audioTimescale),\n        pts: timestamp + (audioCount * 1024 * audioTimescale)\n      }\n    ]\n  };\n\n  return result;\n};\n\n/**\n * inspects the transport stream segment data for start and end time information\n * of the audio and video tracks (when present) as well as the first key frame's\n * start time.\n */\nvar inspectTs_ = function(bytes) {\n  var pmt = {\n    pid: null,\n    table: null\n  };\n\n  var result = {};\n\n  parsePsi_(bytes, pmt);\n\n  for (var pid in pmt.table) {\n    if (pmt.table.hasOwnProperty(pid)) {\n      var type = pmt.table[pid];\n      switch (type) {\n        case StreamTypes.H264_STREAM_TYPE:\n          result.video = [];\n          parseVideoPes_(bytes, pmt, result);\n          if (result.video.length === 0) {\n            delete result.video;\n          }\n          break;\n        case StreamTypes.ADTS_STREAM_TYPE:\n          result.audio = [];\n          parseAudioPes_(bytes, pmt, result);\n          if (result.audio.length === 0) {\n            delete result.audio;\n          }\n          break;\n        default:\n          break;\n      }\n    }\n  }\n  return result;\n};\n\n/**\n * Inspects segment byte data and returns an object with start and end timing information\n *\n * @param {Uint8Array} bytes The segment byte data\n * @param {Number} baseTimestamp Relative reference timestamp used when adjusting frame\n *  timestamps for rollover. This value must be in 90khz clock.\n * @return {Object} Object containing start and end frame timing info of segment.\n */\nvar inspect = function(bytes, baseTimestamp) {\n  var isAacData = isLikelyAacData(bytes);\n\n  var result;\n\n  if (isAacData) {\n    result = inspectAac_(bytes);\n  } else {\n    result = inspectTs_(bytes);\n  }\n\n  if (!result || (!result.audio && !result.video)) {\n    return null;\n  }\n\n  adjustTimestamp_(result, baseTimestamp);\n\n  return result;\n};\n\nmodule.exports = {\n  inspect: inspect\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/tools/ts-inspector.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/utils/clock.js":
/*!************************************************!*\
  !*** ./node_modules/mux.js/lib/utils/clock.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("var\n  ONE_SECOND_IN_TS = 90000, // 90kHz clock\n  secondsToVideoTs,\n  secondsToAudioTs,\n  videoTsToSeconds,\n  audioTsToSeconds,\n  audioTsToVideoTs,\n  videoTsToAudioTs;\n\nsecondsToVideoTs = function(seconds) {\n  return seconds * ONE_SECOND_IN_TS;\n};\n\nsecondsToAudioTs = function(seconds, sampleRate) {\n  return seconds * sampleRate;\n};\n\nvideoTsToSeconds = function(timestamp) {\n  return timestamp / ONE_SECOND_IN_TS;\n};\n\naudioTsToSeconds = function(timestamp, sampleRate) {\n  return timestamp / sampleRate;\n};\n\naudioTsToVideoTs = function(timestamp, sampleRate) {\n  return secondsToVideoTs(audioTsToSeconds(timestamp, sampleRate));\n};\n\nvideoTsToAudioTs = function(timestamp, sampleRate) {\n  return secondsToAudioTs(videoTsToSeconds(timestamp), sampleRate);\n};\n\nmodule.exports = {\n  secondsToVideoTs: secondsToVideoTs,\n  secondsToAudioTs: secondsToAudioTs,\n  videoTsToSeconds: videoTsToSeconds,\n  audioTsToSeconds: audioTsToSeconds,\n  audioTsToVideoTs: audioTsToVideoTs,\n  videoTsToAudioTs: videoTsToAudioTs\n};\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/utils/clock.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/utils/exp-golomb.js":
/*!*****************************************************!*\
  !*** ./node_modules/mux.js/lib/utils/exp-golomb.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar ExpGolomb;\n\n/**\n * Parser for exponential Golomb codes, a variable-bitwidth number encoding\n * scheme used by h264.\n */\nExpGolomb = function(workingData) {\n  var\n    // the number of bytes left to examine in workingData\n    workingBytesAvailable = workingData.byteLength,\n\n    // the current word being examined\n    workingWord = 0, // :uint\n\n    // the number of bits left to examine in the current word\n    workingBitsAvailable = 0; // :uint;\n\n  // ():uint\n  this.length = function() {\n    return (8 * workingBytesAvailable);\n  };\n\n  // ():uint\n  this.bitsAvailable = function() {\n    return (8 * workingBytesAvailable) + workingBitsAvailable;\n  };\n\n  // ():void\n  this.loadWord = function() {\n    var\n      position = workingData.byteLength - workingBytesAvailable,\n      workingBytes = new Uint8Array(4),\n      availableBytes = Math.min(4, workingBytesAvailable);\n\n    if (availableBytes === 0) {\n      throw new Error('no bytes available');\n    }\n\n    workingBytes.set(workingData.subarray(position,\n                                          position + availableBytes));\n    workingWord = new DataView(workingBytes.buffer).getUint32(0);\n\n    // track the amount of workingData that has been processed\n    workingBitsAvailable = availableBytes * 8;\n    workingBytesAvailable -= availableBytes;\n  };\n\n  // (count:int):void\n  this.skipBits = function(count) {\n    var skipBytes; // :int\n    if (workingBitsAvailable > count) {\n      workingWord          <<= count;\n      workingBitsAvailable -= count;\n    } else {\n      count -= workingBitsAvailable;\n      skipBytes = Math.floor(count / 8);\n\n      count -= (skipBytes * 8);\n      workingBytesAvailable -= skipBytes;\n\n      this.loadWord();\n\n      workingWord <<= count;\n      workingBitsAvailable -= count;\n    }\n  };\n\n  // (size:int):uint\n  this.readBits = function(size) {\n    var\n      bits = Math.min(workingBitsAvailable, size), // :uint\n      valu = workingWord >>> (32 - bits); // :uint\n    // if size > 31, handle error\n    workingBitsAvailable -= bits;\n    if (workingBitsAvailable > 0) {\n      workingWord <<= bits;\n    } else if (workingBytesAvailable > 0) {\n      this.loadWord();\n    }\n\n    bits = size - bits;\n    if (bits > 0) {\n      return valu << bits | this.readBits(bits);\n    }\n    return valu;\n  };\n\n  // ():uint\n  this.skipLeadingZeros = function() {\n    var leadingZeroCount; // :uint\n    for (leadingZeroCount = 0; leadingZeroCount < workingBitsAvailable; ++leadingZeroCount) {\n      if ((workingWord & (0x80000000 >>> leadingZeroCount)) !== 0) {\n        // the first bit of working word is 1\n        workingWord <<= leadingZeroCount;\n        workingBitsAvailable -= leadingZeroCount;\n        return leadingZeroCount;\n      }\n    }\n\n    // we exhausted workingWord and still have not found a 1\n    this.loadWord();\n    return leadingZeroCount + this.skipLeadingZeros();\n  };\n\n  // ():void\n  this.skipUnsignedExpGolomb = function() {\n    this.skipBits(1 + this.skipLeadingZeros());\n  };\n\n  // ():void\n  this.skipExpGolomb = function() {\n    this.skipBits(1 + this.skipLeadingZeros());\n  };\n\n  // ():uint\n  this.readUnsignedExpGolomb = function() {\n    var clz = this.skipLeadingZeros(); // :uint\n    return this.readBits(clz + 1) - 1;\n  };\n\n  // ():int\n  this.readExpGolomb = function() {\n    var valu = this.readUnsignedExpGolomb(); // :int\n    if (0x01 & valu) {\n      // the number is odd if the low order bit is set\n      return (1 + valu) >>> 1; // add 1 to make it even, and divide by 2\n    }\n    return -1 * (valu >>> 1); // divide by two then make it negative\n  };\n\n  // Some convenience functions\n  // :Boolean\n  this.readBoolean = function() {\n    return this.readBits(1) === 1;\n  };\n\n  // ():int\n  this.readUnsignedByte = function() {\n    return this.readBits(8);\n  };\n\n  this.loadWord();\n};\n\nmodule.exports = ExpGolomb;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/utils/exp-golomb.js?");

/***/ }),

/***/ "./node_modules/mux.js/lib/utils/stream.js":
/*!*************************************************!*\
  !*** ./node_modules/mux.js/lib/utils/stream.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * mux.js\n *\n * Copyright (c) 2014 Brightcove\n * All rights reserved.\n *\n * A lightweight readable stream implemention that handles event dispatching.\n * Objects that inherit from streams should call init in their constructors.\n */\n\n\nvar Stream = function() {\n  this.init = function() {\n    var listeners = {};\n    /**\n     * Add a listener for a specified event type.\n     * @param type {string} the event name\n     * @param listener {function} the callback to be invoked when an event of\n     * the specified type occurs\n     */\n    this.on = function(type, listener) {\n      if (!listeners[type]) {\n        listeners[type] = [];\n      }\n      listeners[type] = listeners[type].concat(listener);\n    };\n    /**\n     * Remove a listener for a specified event type.\n     * @param type {string} the event name\n     * @param listener {function} a function previously registered for this\n     * type of event through `on`\n     */\n    this.off = function(type, listener) {\n      var index;\n      if (!listeners[type]) {\n        return false;\n      }\n      index = listeners[type].indexOf(listener);\n      listeners[type] = listeners[type].slice();\n      listeners[type].splice(index, 1);\n      return index > -1;\n    };\n    /**\n     * Trigger an event of the specified type on this stream. Any additional\n     * arguments to this function are passed as parameters to event listeners.\n     * @param type {string} the event name\n     */\n    this.trigger = function(type) {\n      var callbacks, i, length, args;\n      callbacks = listeners[type];\n      if (!callbacks) {\n        return;\n      }\n      // Slicing the arguments on every invocation of this method\n      // can add a significant amount of overhead. Avoid the\n      // intermediate object creation for the common case of a\n      // single callback argument\n      if (arguments.length === 2) {\n        length = callbacks.length;\n        for (i = 0; i < length; ++i) {\n          callbacks[i].call(this, arguments[1]);\n        }\n      } else {\n        args = [];\n        i = arguments.length;\n        for (i = 1; i < arguments.length; ++i) {\n          args.push(arguments[i]);\n        }\n        length = callbacks.length;\n        for (i = 0; i < length; ++i) {\n          callbacks[i].apply(this, args);\n        }\n      }\n    };\n    /**\n     * Destroys the stream and cleans up.\n     */\n    this.dispose = function() {\n      listeners = {};\n    };\n  };\n};\n\n/**\n * Forwards all `data` events on this stream to the destination stream. The\n * destination stream should provide a method `push` to receive the data\n * events as they arrive.\n * @param destination {stream} the stream that will receive all `data` events\n * @param autoFlush {boolean} if false, we will not call `flush` on the destination\n *                            when the current stream emits a 'done' event\n * @see http://nodejs.org/api/stream.html#stream_readable_pipe_destination_options\n */\nStream.prototype.pipe = function(destination) {\n  this.on('data', function(data) {\n    destination.push(data);\n  });\n\n  this.on('done', function(flushSource) {\n    destination.flush(flushSource);\n  });\n\n  return destination;\n};\n\n// Default stream functions that are expected to be overridden to perform\n// actual work. These are provided by the prototype as a sort of no-op\n// implementation so that we don't have to check for their existence in the\n// `pipe` function above.\nStream.prototype.push = function(data) {\n  this.trigger('data', data);\n};\n\nStream.prototype.flush = function(flushSource) {\n  this.trigger('done', flushSource);\n};\n\nmodule.exports = Stream;\n\n\n//# sourceURL=webpack:///./node_modules/mux.js/lib/utils/stream.js?");

/***/ }),

/***/ "./node_modules/pkcs7/lib/pad.js":
/*!***************************************!*\
  !*** ./node_modules/pkcs7/lib/pad.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/*\n * pkcs7.pad\n * https://github.com/brightcove/pkcs7\n *\n * Copyright (c) 2014 Brightcove\n * Licensed under the apache2 license.\n */\n\n\n\nvar PADDING;\n\n/**\n * Returns a new Uint8Array that is padded with PKCS#7 padding.\n * @param plaintext {Uint8Array} the input bytes before encryption\n * @return {Uint8Array} the padded bytes\n * @see http://tools.ietf.org/html/rfc5652\n */\nmodule.exports = function pad(plaintext) {\n  var padding = PADDING[(plaintext.byteLength % 16) || 0],\n      result = new Uint8Array(plaintext.byteLength + padding.length);\n  result.set(plaintext);\n  result.set(padding, plaintext.byteLength);\n  return result;\n};\n\n// pre-define the padding values\nPADDING = [\n  [16, 16, 16, 16,\n   16, 16, 16, 16,\n   16, 16, 16, 16,\n   16, 16, 16, 16],\n\n  [15, 15, 15, 15,\n   15, 15, 15, 15,\n   15, 15, 15, 15,\n   15, 15, 15],\n\n  [14, 14, 14, 14,\n   14, 14, 14, 14,\n   14, 14, 14, 14,\n   14, 14],\n\n  [13, 13, 13, 13,\n   13, 13, 13, 13,\n   13, 13, 13, 13,\n   13],\n\n  [12, 12, 12, 12,\n   12, 12, 12, 12,\n   12, 12, 12, 12],\n\n  [11, 11, 11, 11,\n   11, 11, 11, 11,\n   11, 11, 11],\n\n  [10, 10, 10, 10,\n   10, 10, 10, 10,\n   10, 10],\n\n  [9, 9, 9, 9,\n   9, 9, 9, 9,\n   9],\n\n  [8, 8, 8, 8,\n   8, 8, 8, 8],\n\n  [7, 7, 7, 7,\n   7, 7, 7],\n\n  [6, 6, 6, 6,\n   6, 6],\n\n  [5, 5, 5, 5,\n   5],\n\n  [4, 4, 4, 4],\n\n  [3, 3, 3],\n\n  [2, 2],\n\n  [1]\n];\n\n\n//# sourceURL=webpack:///./node_modules/pkcs7/lib/pad.js?");

/***/ }),

/***/ "./node_modules/pkcs7/lib/pkcs7.js":
/*!*****************************************!*\
  !*** ./node_modules/pkcs7/lib/pkcs7.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/*\n * pkcs7\n * https://github.com/brightcove/pkcs7\n *\n * Copyright (c) 2014 Brightcove\n * Licensed under the apache2 license.\n */\n\n\n\nexports.pad = __webpack_require__(/*! ./pad.js */ \"./node_modules/pkcs7/lib/pad.js\");\nexports.unpad = __webpack_require__(/*! ./unpad.js */ \"./node_modules/pkcs7/lib/unpad.js\");\n\n\n//# sourceURL=webpack:///./node_modules/pkcs7/lib/pkcs7.js?");

/***/ }),

/***/ "./node_modules/pkcs7/lib/unpad.js":
/*!*****************************************!*\
  !*** ./node_modules/pkcs7/lib/unpad.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/*\n * pkcs7.unpad\n * https://github.com/brightcove/pkcs7\n *\n * Copyright (c) 2014 Brightcove\n * Licensed under the apache2 license.\n */\n\n\n\n/**\n * Returns the subarray of a Uint8Array without PKCS#7 padding.\n * @param padded {Uint8Array} unencrypted bytes that have been padded\n * @return {Uint8Array} the unpadded bytes\n * @see http://tools.ietf.org/html/rfc5652\n */\nmodule.exports = function unpad(padded) {\n  return padded.subarray(0, padded.byteLength - padded[padded.byteLength - 1]);\n};\n\n\n//# sourceURL=webpack:///./node_modules/pkcs7/lib/unpad.js?");

/***/ }),

/***/ "./node_modules/url-toolkit/src/url-toolkit.js":
/*!*****************************************************!*\
  !*** ./node_modules/url-toolkit/src/url-toolkit.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("// see https://tools.ietf.org/html/rfc1808\n\n/* jshint ignore:start */\n(function(root) { \n/* jshint ignore:end */\n\n  var URL_REGEX = /^((?:[a-zA-Z0-9+\\-.]+:)?)(\\/\\/[^\\/?#]*)?((?:[^\\/\\?#]*\\/)*.*?)??(;.*?)?(\\?.*?)?(#.*?)?$/;\n  var FIRST_SEGMENT_REGEX = /^([^\\/?#]*)(.*)$/;\n  var SLASH_DOT_REGEX = /(?:\\/|^)\\.(?=\\/)/g;\n  var SLASH_DOT_DOT_REGEX = /(?:\\/|^)\\.\\.\\/(?!\\.\\.\\/).*?(?=\\/)/g;\n\n  var URLToolkit = { // jshint ignore:line\n    // If opts.alwaysNormalize is true then the path will always be normalized even when it starts with / or //\n    // E.g\n    // With opts.alwaysNormalize = false (default, spec compliant)\n    // http://a.com/b/cd + /e/f/../g => http://a.com/e/f/../g\n    // With opts.alwaysNormalize = true (not spec compliant)\n    // http://a.com/b/cd + /e/f/../g => http://a.com/e/g\n    buildAbsoluteURL: function(baseURL, relativeURL, opts) {\n      opts = opts || {};\n      // remove any remaining space and CRLF\n      baseURL = baseURL.trim();\n      relativeURL = relativeURL.trim();\n      if (!relativeURL) {\n        // 2a) If the embedded URL is entirely empty, it inherits the\n        // entire base URL (i.e., is set equal to the base URL)\n        // and we are done.\n        if (!opts.alwaysNormalize) {\n          return baseURL;\n        }\n        var basePartsForNormalise = URLToolkit.parseURL(baseURL);\n        if (!basePartsForNormalise) {\n          throw new Error('Error trying to parse base URL.');\n        }\n        basePartsForNormalise.path = URLToolkit.normalizePath(basePartsForNormalise.path);\n        return URLToolkit.buildURLFromParts(basePartsForNormalise);\n      }\n      var relativeParts = URLToolkit.parseURL(relativeURL);\n      if (!relativeParts) {\n        throw new Error('Error trying to parse relative URL.');\n      }\n      if (relativeParts.scheme) {\n        // 2b) If the embedded URL starts with a scheme name, it is\n        // interpreted as an absolute URL and we are done.\n        if (!opts.alwaysNormalize) {\n          return relativeURL;\n        }\n        relativeParts.path = URLToolkit.normalizePath(relativeParts.path);\n        return URLToolkit.buildURLFromParts(relativeParts);\n      }\n      var baseParts = URLToolkit.parseURL(baseURL);\n      if (!baseParts) {\n        throw new Error('Error trying to parse base URL.');\n      }\n      if (!baseParts.netLoc && baseParts.path && baseParts.path[0] !== '/') {\n        // If netLoc missing and path doesn't start with '/', assume everthing before the first '/' is the netLoc\n        // This causes 'example.com/a' to be handled as '//example.com/a' instead of '/example.com/a'\n        var pathParts = FIRST_SEGMENT_REGEX.exec(baseParts.path);\n        baseParts.netLoc = pathParts[1];\n        baseParts.path = pathParts[2];\n      }\n      if (baseParts.netLoc && !baseParts.path) {\n        baseParts.path = '/';\n      }\n      var builtParts = {\n        // 2c) Otherwise, the embedded URL inherits the scheme of\n        // the base URL.\n        scheme: baseParts.scheme,\n        netLoc: relativeParts.netLoc,\n        path: null,\n        params: relativeParts.params,\n        query: relativeParts.query,\n        fragment: relativeParts.fragment\n      };\n      if (!relativeParts.netLoc) {\n        // 3) If the embedded URL's <net_loc> is non-empty, we skip to\n        // Step 7.  Otherwise, the embedded URL inherits the <net_loc>\n        // (if any) of the base URL.\n        builtParts.netLoc = baseParts.netLoc;\n        // 4) If the embedded URL path is preceded by a slash \"/\", the\n        // path is not relative and we skip to Step 7.\n        if (relativeParts.path[0] !== '/') {\n          if (!relativeParts.path) {\n            // 5) If the embedded URL path is empty (and not preceded by a\n            // slash), then the embedded URL inherits the base URL path\n            builtParts.path = baseParts.path;\n            // 5a) if the embedded URL's <params> is non-empty, we skip to\n            // step 7; otherwise, it inherits the <params> of the base\n            // URL (if any) and\n            if (!relativeParts.params) {\n              builtParts.params = baseParts.params;\n              // 5b) if the embedded URL's <query> is non-empty, we skip to\n              // step 7; otherwise, it inherits the <query> of the base\n              // URL (if any) and we skip to step 7.\n              if (!relativeParts.query) {\n                builtParts.query = baseParts.query;\n              }\n            }\n          } else {\n            // 6) The last segment of the base URL's path (anything\n            // following the rightmost slash \"/\", or the entire path if no\n            // slash is present) is removed and the embedded URL's path is\n            // appended in its place.\n            var baseURLPath = baseParts.path;\n            var newPath = baseURLPath.substring(0, baseURLPath.lastIndexOf('/') + 1) + relativeParts.path;\n            builtParts.path = URLToolkit.normalizePath(newPath);\n          }\n        }\n      }\n      if (builtParts.path === null) {\n        builtParts.path = opts.alwaysNormalize ? URLToolkit.normalizePath(relativeParts.path) : relativeParts.path;\n      }\n      return URLToolkit.buildURLFromParts(builtParts);\n    },\n    parseURL: function(url) {\n      var parts = URL_REGEX.exec(url);\n      if (!parts) {\n        return null;\n      }\n      return {\n        scheme: parts[1] || '',\n        netLoc: parts[2] || '',\n        path: parts[3] || '',\n        params: parts[4] || '',\n        query: parts[5] || '',\n        fragment: parts[6] || ''\n      };\n    },\n    normalizePath: function(path) {\n      // The following operations are\n      // then applied, in order, to the new path:\n      // 6a) All occurrences of \"./\", where \".\" is a complete path\n      // segment, are removed.\n      // 6b) If the path ends with \".\" as a complete path segment,\n      // that \".\" is removed.\n      path = path.split('').reverse().join('').replace(SLASH_DOT_REGEX, '');\n      // 6c) All occurrences of \"<segment>/../\", where <segment> is a\n      // complete path segment not equal to \"..\", are removed.\n      // Removal of these path segments is performed iteratively,\n      // removing the leftmost matching pattern on each iteration,\n      // until no matching pattern remains.\n      // 6d) If the path ends with \"<segment>/..\", where <segment> is a\n      // complete path segment not equal to \"..\", that\n      // \"<segment>/..\" is removed.\n      while (path.length !== (path = path.replace(SLASH_DOT_DOT_REGEX, '')).length) {} // jshint ignore:line\n      return path.split('').reverse().join('');\n    },\n    buildURLFromParts: function(parts) {\n      return parts.scheme + parts.netLoc + parts.path + parts.params + parts.query + parts.fragment;\n    }\n  };\n\n/* jshint ignore:start */\n  if(true)\n    module.exports = URLToolkit;\n  else {}\n})(this);\n/* jshint ignore:end */\n\n\n//# sourceURL=webpack:///./node_modules/url-toolkit/src/url-toolkit.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/ad-cue-tags.js":
/*!*************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/ad-cue-tags.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file ad-cue-tags.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _slicedToArray = (function () { function sliceIterator(arr, i) { var _arr = []; var _n = true; var _d = false; var _e = undefined; try { for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i['return']) _i['return'](); } finally { if (_d) throw _e; } } return _arr; } return function (arr, i) { if (Array.isArray(arr)) { return arr; } else if (Symbol.iterator in Object(arr)) { return sliceIterator(arr, i); } else { throw new TypeError('Invalid attempt to destructure non-iterable instance'); } }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\n/**\n * Searches for an ad cue that overlaps with the given mediaTime\n */\nvar findAdCue = function findAdCue(track, mediaTime) {\n  var cues = track.cues;\n\n  for (var i = 0; i < cues.length; i++) {\n    var cue = cues[i];\n\n    if (mediaTime >= cue.adStartTime && mediaTime <= cue.adEndTime) {\n      return cue;\n    }\n  }\n  return null;\n};\n\nvar updateAdCues = function updateAdCues(media, track) {\n  var offset = arguments.length <= 2 || arguments[2] === undefined ? 0 : arguments[2];\n\n  if (!media.segments) {\n    return;\n  }\n\n  var mediaTime = offset;\n  var cue = undefined;\n\n  for (var i = 0; i < media.segments.length; i++) {\n    var segment = media.segments[i];\n\n    if (!cue) {\n      // Since the cues will span for at least the segment duration, adding a fudge\n      // factor of half segment duration will prevent duplicate cues from being\n      // created when timing info is not exact (e.g. cue start time initialized\n      // at 10.006677, but next call mediaTime is 10.003332 )\n      cue = findAdCue(track, mediaTime + segment.duration / 2);\n    }\n\n    if (cue) {\n      if ('cueIn' in segment) {\n        // Found a CUE-IN so end the cue\n        cue.endTime = mediaTime;\n        cue.adEndTime = mediaTime;\n        mediaTime += segment.duration;\n        cue = null;\n        continue;\n      }\n\n      if (mediaTime < cue.endTime) {\n        // Already processed this mediaTime for this cue\n        mediaTime += segment.duration;\n        continue;\n      }\n\n      // otherwise extend cue until a CUE-IN is found\n      cue.endTime += segment.duration;\n    } else {\n      if ('cueOut' in segment) {\n        cue = new _globalWindow2['default'].VTTCue(mediaTime, mediaTime + segment.duration, segment.cueOut);\n        cue.adStartTime = mediaTime;\n        // Assumes tag format to be\n        // #EXT-X-CUE-OUT:30\n        cue.adEndTime = mediaTime + parseFloat(segment.cueOut);\n        track.addCue(cue);\n      }\n\n      if ('cueOutCont' in segment) {\n        // Entered into the middle of an ad cue\n        var adOffset = undefined;\n        var adTotal = undefined;\n\n        // Assumes tag formate to be\n        // #EXT-X-CUE-OUT-CONT:10/30\n\n        var _segment$cueOutCont$split$map = segment.cueOutCont.split('/').map(parseFloat);\n\n        var _segment$cueOutCont$split$map2 = _slicedToArray(_segment$cueOutCont$split$map, 2);\n\n        adOffset = _segment$cueOutCont$split$map2[0];\n        adTotal = _segment$cueOutCont$split$map2[1];\n\n        cue = new _globalWindow2['default'].VTTCue(mediaTime, mediaTime + segment.duration, '');\n        cue.adStartTime = mediaTime - adOffset;\n        cue.adEndTime = cue.adStartTime + adTotal;\n        track.addCue(cue);\n      }\n    }\n    mediaTime += segment.duration;\n  }\n};\n\nexports['default'] = {\n  updateAdCues: updateAdCues,\n  findAdCue: findAdCue\n};\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/ad-cue-tags.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/bin-utils.js":
/*!***********************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/bin-utils.js ***!
  \***********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file bin-utils.js\n */\n\n/**\n * convert a TimeRange to text\n *\n * @param {TimeRange} range the timerange to use for conversion\n * @param {Number} i the iterator on the range to convert\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\nvar textRange = function textRange(range, i) {\n  return range.start(i) + '-' + range.end(i);\n};\n\n/**\n * format a number as hex string\n *\n * @param {Number} e The number\n * @param {Number} i the iterator\n */\nvar formatHexString = function formatHexString(e, i) {\n  var value = e.toString(16);\n\n  return '00'.substring(0, 2 - value.length) + value + (i % 2 ? ' ' : '');\n};\nvar formatAsciiString = function formatAsciiString(e) {\n  if (e >= 0x20 && e < 0x7e) {\n    return String.fromCharCode(e);\n  }\n  return '.';\n};\n\n/**\n * Creates an object for sending to a web worker modifying properties that are TypedArrays\n * into a new object with seperated properties for the buffer, byteOffset, and byteLength.\n *\n * @param {Object} message\n *        Object of properties and values to send to the web worker\n * @return {Object}\n *         Modified message with TypedArray values expanded\n * @function createTransferableMessage\n */\nvar createTransferableMessage = function createTransferableMessage(message) {\n  var transferable = {};\n\n  Object.keys(message).forEach(function (key) {\n    var value = message[key];\n\n    if (ArrayBuffer.isView(value)) {\n      transferable[key] = {\n        bytes: value.buffer,\n        byteOffset: value.byteOffset,\n        byteLength: value.byteLength\n      };\n    } else {\n      transferable[key] = value;\n    }\n  });\n\n  return transferable;\n};\n\n/**\n * Returns a unique string identifier for a media initialization\n * segment.\n */\nvar initSegmentId = function initSegmentId(initSegment) {\n  var byterange = initSegment.byterange || {\n    length: Infinity,\n    offset: 0\n  };\n\n  return [byterange.length, byterange.offset, initSegment.resolvedUri].join(',');\n};\n\n/**\n * utils to help dump binary data to the console\n */\nvar utils = {\n  hexDump: function hexDump(data) {\n    var bytes = Array.prototype.slice.call(data);\n    var step = 16;\n    var result = '';\n    var hex = undefined;\n    var ascii = undefined;\n\n    for (var j = 0; j < bytes.length / step; j++) {\n      hex = bytes.slice(j * step, j * step + step).map(formatHexString).join('');\n      ascii = bytes.slice(j * step, j * step + step).map(formatAsciiString).join('');\n      result += hex + ' ' + ascii + '\\n';\n    }\n    return result;\n  },\n  tagDump: function tagDump(tag) {\n    return utils.hexDump(tag.bytes);\n  },\n  textRanges: function textRanges(ranges) {\n    var result = '';\n    var i = undefined;\n\n    for (i = 0; i < ranges.length; i++) {\n      result += textRange(ranges, i) + ' ';\n    }\n    return result;\n  },\n  createTransferableMessage: createTransferableMessage,\n  initSegmentId: initSegmentId\n};\n\nexports['default'] = utils;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/bin-utils.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/config.js":
/*!********************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/config.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports[\"default\"] = {\n  GOAL_BUFFER_LENGTH: 30,\n  MAX_GOAL_BUFFER_LENGTH: 60,\n  GOAL_BUFFER_LENGTH_RATE: 1,\n  // A fudge factor to apply to advertised playlist bitrates to account for\n  // temporary flucations in client bandwidth\n  BANDWIDTH_VARIANCE: 1.2,\n  // How much of the buffer must be filled before we consider upswitching\n  BUFFER_LOW_WATER_LINE: 0,\n  MAX_BUFFER_LOW_WATER_LINE: 30,\n  BUFFER_LOW_WATER_LINE_RATE: 1\n};\nmodule.exports = exports[\"default\"];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/config.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/decrypter-worker.js":
/*!******************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/decrypter-worker.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _aesDecrypter = __webpack_require__(/*! aes-decrypter */ \"./node_modules/aes-decrypter/es5/index.js\");\n\nvar _binUtils = __webpack_require__(/*! ./bin-utils */ \"./node_modules/videojs-contrib-hls/es5/bin-utils.js\");\n\n/**\n * Our web worker interface so that things can talk to aes-decrypter\n * that will be running in a web worker. the scope is passed to this by\n * webworkify.\n *\n * @param {Object} self\n *        the scope for the web worker\n */\nvar DecrypterWorker = function DecrypterWorker(self) {\n  self.onmessage = function (event) {\n    var data = event.data;\n    var encrypted = new Uint8Array(data.encrypted.bytes, data.encrypted.byteOffset, data.encrypted.byteLength);\n    var key = new Uint32Array(data.key.bytes, data.key.byteOffset, data.key.byteLength / 4);\n    var iv = new Uint32Array(data.iv.bytes, data.iv.byteOffset, data.iv.byteLength / 4);\n\n    /* eslint-disable no-new, handle-callback-err */\n    new _aesDecrypter.Decrypter(encrypted, key, iv, function (err, bytes) {\n      _globalWindow2['default'].postMessage((0, _binUtils.createTransferableMessage)({\n        source: data.source,\n        decrypted: bytes\n      }), [bytes.buffer]);\n    });\n    /* eslint-enable */\n  };\n};\n\nexports['default'] = function (self) {\n  return new DecrypterWorker(self);\n};\n\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/decrypter-worker.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/master-playlist-controller.js":
/*!****************************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/master-playlist-controller.js ***!
  \****************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file master-playlist-controller.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _playlistLoader = __webpack_require__(/*! ./playlist-loader */ \"./node_modules/videojs-contrib-hls/es5/playlist-loader.js\");\n\nvar _playlistLoader2 = _interopRequireDefault(_playlistLoader);\n\nvar _playlistJs = __webpack_require__(/*! ./playlist.js */ \"./node_modules/videojs-contrib-hls/es5/playlist.js\");\n\nvar _segmentLoader = __webpack_require__(/*! ./segment-loader */ \"./node_modules/videojs-contrib-hls/es5/segment-loader.js\");\n\nvar _segmentLoader2 = _interopRequireDefault(_segmentLoader);\n\nvar _vttSegmentLoader = __webpack_require__(/*! ./vtt-segment-loader */ \"./node_modules/videojs-contrib-hls/es5/vtt-segment-loader.js\");\n\nvar _vttSegmentLoader2 = _interopRequireDefault(_vttSegmentLoader);\n\nvar _ranges = __webpack_require__(/*! ./ranges */ \"./node_modules/videojs-contrib-hls/es5/ranges.js\");\n\nvar _ranges2 = _interopRequireDefault(_ranges);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _adCueTags = __webpack_require__(/*! ./ad-cue-tags */ \"./node_modules/videojs-contrib-hls/es5/ad-cue-tags.js\");\n\nvar _adCueTags2 = _interopRequireDefault(_adCueTags);\n\nvar _syncController = __webpack_require__(/*! ./sync-controller */ \"./node_modules/videojs-contrib-hls/es5/sync-controller.js\");\n\nvar _syncController2 = _interopRequireDefault(_syncController);\n\nvar _videojsContribMediaSourcesEs5CodecUtils = __webpack_require__(/*! videojs-contrib-media-sources/es5/codec-utils */ \"./node_modules/videojs-contrib-media-sources/es5/codec-utils.js\");\n\nvar _webwackify = __webpack_require__(/*! webwackify */ \"./node_modules/webwackify/index.js\");\n\nvar _webwackify2 = _interopRequireDefault(_webwackify);\n\nvar _decrypterWorker = __webpack_require__(/*! ./decrypter-worker */ \"./node_modules/videojs-contrib-hls/es5/decrypter-worker.js\");\n\nvar _decrypterWorker2 = _interopRequireDefault(_decrypterWorker);\n\nvar _config = __webpack_require__(/*! ./config */ \"./node_modules/videojs-contrib-hls/es5/config.js\");\n\nvar _config2 = _interopRequireDefault(_config);\n\nvar _utilCodecsJs = __webpack_require__(/*! ./util/codecs.js */ \"./node_modules/videojs-contrib-hls/es5/util/codecs.js\");\n\nvar _mediaGroups = __webpack_require__(/*! ./media-groups */ \"./node_modules/videojs-contrib-hls/es5/media-groups.js\");\n\nvar ABORT_EARLY_BLACKLIST_SECONDS = 60 * 2;\n\nvar Hls = undefined;\n\n// Default codec parameters if none were provided for video and/or audio\nvar defaultCodecs = {\n  videoCodec: 'avc1',\n  videoObjectTypeIndicator: '.4d400d',\n  // AAC-LC\n  audioProfile: '2'\n};\n\n// SegmentLoader stats that need to have each loader's\n// values summed to calculate the final value\nvar loaderStats = ['mediaRequests', 'mediaRequestsAborted', 'mediaRequestsTimedout', 'mediaRequestsErrored', 'mediaTransferDuration', 'mediaBytesTransferred'];\nvar sumLoaderStat = function sumLoaderStat(stat) {\n  return this.audioSegmentLoader_[stat] + this.mainSegmentLoader_[stat];\n};\n\nvar resolveDecrypterWorker = function resolveDecrypterWorker() {\n  var result = undefined;\n\n  try {\n    result = /*require.resolve*/(/*! ./decrypter-worker */ \"./node_modules/videojs-contrib-hls/es5/decrypter-worker.js\");\n  } catch (e) {\n    // no result\n  }\n\n  return result;\n};\n\n/**\n * Replace codecs in the codec string with the old apple-style `avc1.<dd>.<dd>` to the\n * standard `avc1.<hhhhhh>`.\n *\n * @param codecString {String} the codec string\n * @return {String} the codec string with old apple-style codecs replaced\n *\n * @private\n */\nvar mapLegacyAvcCodecs_ = function mapLegacyAvcCodecs_(codecString) {\n  return codecString.replace(/avc1\\.(\\d+)\\.(\\d+)/i, function (match) {\n    return (0, _videojsContribMediaSourcesEs5CodecUtils.translateLegacyCodecs)([match])[0];\n  });\n};\n\nexports.mapLegacyAvcCodecs_ = mapLegacyAvcCodecs_;\n/**\n * Build a media mime-type string from a set of parameters\n * @param {String} type either 'audio' or 'video'\n * @param {String} container either 'mp2t' or 'mp4'\n * @param {Array} codecs an array of codec strings to add\n * @return {String} a valid media mime-type\n */\nvar makeMimeTypeString = function makeMimeTypeString(type, container, codecs) {\n  // The codecs array is filtered so that falsey values are\n  // dropped and don't cause Array#join to create spurious\n  // commas\n  return type + '/' + container + '; codecs=\"' + codecs.filter(function (c) {\n    return !!c;\n  }).join(', ') + '\"';\n};\n\n/**\n * Returns the type container based on information in the playlist\n * @param {Playlist} media the current media playlist\n * @return {String} a valid media container type\n */\nvar getContainerType = function getContainerType(media) {\n  // An initialization segment means the media playlist is an iframe\n  // playlist or is using the mp4 container. We don't currently\n  // support iframe playlists, so assume this is signalling mp4\n  // fragments.\n  if (media.segments && media.segments.length && media.segments[0].map) {\n    return 'mp4';\n  }\n  return 'mp2t';\n};\n\n/**\n * Returns a set of codec strings parsed from the playlist or the default\n * codec strings if no codecs were specified in the playlist\n * @param {Playlist} media the current media playlist\n * @return {Object} an object with the video and audio codecs\n */\nvar getCodecs = function getCodecs(media) {\n  // if the codecs were explicitly specified, use them instead of the\n  // defaults\n  var mediaAttributes = media.attributes || {};\n\n  if (mediaAttributes.CODECS) {\n    return (0, _utilCodecsJs.parseCodecs)(mediaAttributes.CODECS);\n  }\n  return defaultCodecs;\n};\n\n/**\n * Calculates the MIME type strings for a working configuration of\n * SourceBuffers to play variant streams in a master playlist. If\n * there is no possible working configuration, an empty array will be\n * returned.\n *\n * @param master {Object} the m3u8 object for the master playlist\n * @param media {Object} the m3u8 object for the variant playlist\n * @return {Array} the MIME type strings. If the array has more than\n * one entry, the first element should be applied to the video\n * SourceBuffer and the second to the audio SourceBuffer.\n *\n * @private\n */\nvar mimeTypesForPlaylist_ = function mimeTypesForPlaylist_(master, media) {\n  var containerType = getContainerType(media);\n  var codecInfo = getCodecs(media);\n  var mediaAttributes = media.attributes || {};\n  // Default condition for a traditional HLS (no demuxed audio/video)\n  var isMuxed = true;\n  var isMaat = false;\n\n  if (!media) {\n    // Not enough information\n    return [];\n  }\n\n  if (master.mediaGroups.AUDIO && mediaAttributes.AUDIO) {\n    var audioGroup = master.mediaGroups.AUDIO[mediaAttributes.AUDIO];\n\n    // Handle the case where we are in a multiple-audio track scenario\n    if (audioGroup) {\n      isMaat = true;\n      // Start with the everything demuxed then...\n      isMuxed = false;\n      // ...check to see if any audio group tracks are muxed (ie. lacking a uri)\n      for (var groupId in audioGroup) {\n        if (!audioGroup[groupId].uri) {\n          isMuxed = true;\n          break;\n        }\n      }\n    }\n  }\n\n  // HLS with multiple-audio tracks must always get an audio codec.\n  // Put another way, there is no way to have a video-only multiple-audio HLS!\n  if (isMaat && !codecInfo.audioProfile) {\n    _videoJs2['default'].log.warn('Multiple audio tracks present but no audio codec string is specified. ' + 'Attempting to use the default audio codec (mp4a.40.2)');\n    codecInfo.audioProfile = defaultCodecs.audioProfile;\n  }\n\n  // Generate the final codec strings from the codec object generated above\n  var codecStrings = {};\n\n  if (codecInfo.videoCodec) {\n    codecStrings.video = '' + codecInfo.videoCodec + codecInfo.videoObjectTypeIndicator;\n  }\n\n  if (codecInfo.audioProfile) {\n    codecStrings.audio = 'mp4a.40.' + codecInfo.audioProfile;\n  }\n\n  // Finally, make and return an array with proper mime-types depending on\n  // the configuration\n  var justAudio = makeMimeTypeString('audio', containerType, [codecStrings.audio]);\n  var justVideo = makeMimeTypeString('video', containerType, [codecStrings.video]);\n  var bothVideoAudio = makeMimeTypeString('video', containerType, [codecStrings.video, codecStrings.audio]);\n\n  if (isMaat) {\n    if (!isMuxed && codecStrings.video) {\n      return [justVideo, justAudio];\n    }\n    // There exists the possiblity that this will return a `video/container`\n    // mime-type for the first entry in the array even when there is only audio.\n    // This doesn't appear to be a problem and simplifies the code.\n    return [bothVideoAudio, justAudio];\n  }\n\n  // If there is ano video codec at all, always just return a single\n  // audio/<container> mime-type\n  if (!codecStrings.video) {\n    return [justAudio];\n  }\n\n  // When not using separate audio media groups, audio and video is\n  // *always* muxed\n  return [bothVideoAudio];\n};\n\nexports.mimeTypesForPlaylist_ = mimeTypesForPlaylist_;\n/**\n * the master playlist controller controls all interactons\n * between playlists and segmentloaders. At this time this mainly\n * involves a master playlist and a series of audio playlists\n * if they are available\n *\n * @class MasterPlaylistController\n * @extends videojs.EventTarget\n */\n\nvar MasterPlaylistController = (function (_videojs$EventTarget) {\n  _inherits(MasterPlaylistController, _videojs$EventTarget);\n\n  function MasterPlaylistController(options) {\n    var _this = this;\n\n    _classCallCheck(this, MasterPlaylistController);\n\n    _get(Object.getPrototypeOf(MasterPlaylistController.prototype), 'constructor', this).call(this);\n\n    var url = options.url;\n    var handleManifestRedirects = options.handleManifestRedirects;\n    var withCredentials = options.withCredentials;\n    var mode = options.mode;\n    var tech = options.tech;\n    var bandwidth = options.bandwidth;\n    var externHls = options.externHls;\n    var useCueTags = options.useCueTags;\n    var blacklistDuration = options.blacklistDuration;\n    var enableLowInitialPlaylist = options.enableLowInitialPlaylist;\n\n    if (!url) {\n      throw new Error('A non-empty playlist URL is required');\n    }\n\n    Hls = externHls;\n\n    this.tech_ = tech;\n    this.hls_ = tech.hls;\n    this.mode_ = mode;\n    this.useCueTags_ = useCueTags;\n    this.blacklistDuration = blacklistDuration;\n    this.enableLowInitialPlaylist = enableLowInitialPlaylist;\n\n    if (this.useCueTags_) {\n      this.cueTagsTrack_ = this.tech_.addTextTrack('metadata', 'ad-cues');\n      this.cueTagsTrack_.inBandMetadataTrackDispatchType = '';\n    }\n\n    this.requestOptions_ = {\n      withCredentials: withCredentials,\n      handleManifestRedirects: handleManifestRedirects,\n      timeout: null\n    };\n\n    this.mediaTypes_ = (0, _mediaGroups.createMediaTypes)();\n\n    this.mediaSource = new _videoJs2['default'].MediaSource({ mode: mode });\n\n    // load the media source into the player\n    this.mediaSource.addEventListener('sourceopen', this.handleSourceOpen_.bind(this));\n\n    this.seekable_ = _videoJs2['default'].createTimeRanges();\n    this.hasPlayed_ = function () {\n      return false;\n    };\n\n    this.syncController_ = new _syncController2['default'](options);\n    this.segmentMetadataTrack_ = tech.addRemoteTextTrack({\n      kind: 'metadata',\n      label: 'segment-metadata'\n    }, false).track;\n\n    this.decrypter_ = (0, _webwackify2['default'])(_decrypterWorker2['default'], resolveDecrypterWorker());\n\n    var segmentLoaderSettings = {\n      hls: this.hls_,\n      mediaSource: this.mediaSource,\n      currentTime: this.tech_.currentTime.bind(this.tech_),\n      seekable: function seekable() {\n        return _this.seekable();\n      },\n      seeking: function seeking() {\n        return _this.tech_.seeking();\n      },\n      duration: function duration() {\n        return _this.mediaSource.duration;\n      },\n      hasPlayed: function hasPlayed() {\n        return _this.hasPlayed_();\n      },\n      goalBufferLength: function goalBufferLength() {\n        return _this.goalBufferLength();\n      },\n      bandwidth: bandwidth,\n      syncController: this.syncController_,\n      decrypter: this.decrypter_\n    };\n\n    // setup playlist loaders\n    this.masterPlaylistLoader_ = new _playlistLoader2['default'](url, this.hls_, this.requestOptions_);\n    this.setupMasterPlaylistLoaderListeners_();\n\n    // setup segment loaders\n    // combined audio/video or just video when alternate audio track is selected\n    this.mainSegmentLoader_ = new _segmentLoader2['default'](_videoJs2['default'].mergeOptions(segmentLoaderSettings, {\n      segmentMetadataTrack: this.segmentMetadataTrack_,\n      loaderType: 'main'\n    }), options);\n\n    // alternate audio track\n    this.audioSegmentLoader_ = new _segmentLoader2['default'](_videoJs2['default'].mergeOptions(segmentLoaderSettings, {\n      loaderType: 'audio'\n    }), options);\n\n    this.subtitleSegmentLoader_ = new _vttSegmentLoader2['default'](_videoJs2['default'].mergeOptions(segmentLoaderSettings, {\n      loaderType: 'vtt'\n    }), options);\n\n    this.setupSegmentLoaderListeners_();\n\n    // Create SegmentLoader stat-getters\n    loaderStats.forEach(function (stat) {\n      _this[stat + '_'] = sumLoaderStat.bind(_this, stat);\n    });\n\n    this.masterPlaylistLoader_.load();\n  }\n\n  /**\n   * Register event handlers on the master playlist loader. A helper\n   * function for construction time.\n   *\n   * @private\n   */\n\n  _createClass(MasterPlaylistController, [{\n    key: 'setupMasterPlaylistLoaderListeners_',\n    value: function setupMasterPlaylistLoaderListeners_() {\n      var _this2 = this;\n\n      this.masterPlaylistLoader_.on('loadedmetadata', function () {\n        var media = _this2.masterPlaylistLoader_.media();\n        var requestTimeout = _this2.masterPlaylistLoader_.targetDuration * 1.5 * 1000;\n\n        // If we don't have any more available playlists, we don't want to\n        // timeout the request.\n        if ((0, _playlistJs.isLowestEnabledRendition)(_this2.masterPlaylistLoader_.master, _this2.masterPlaylistLoader_.media())) {\n          _this2.requestOptions_.timeout = 0;\n        } else {\n          _this2.requestOptions_.timeout = requestTimeout;\n        }\n\n        // if this isn't a live video and preload permits, start\n        // downloading segments\n        if (media.endList && _this2.tech_.preload() !== 'none') {\n          _this2.mainSegmentLoader_.playlist(media, _this2.requestOptions_);\n          _this2.mainSegmentLoader_.load();\n        }\n\n        (0, _mediaGroups.setupMediaGroups)({\n          segmentLoaders: {\n            AUDIO: _this2.audioSegmentLoader_,\n            SUBTITLES: _this2.subtitleSegmentLoader_,\n            main: _this2.mainSegmentLoader_\n          },\n          tech: _this2.tech_,\n          requestOptions: _this2.requestOptions_,\n          masterPlaylistLoader: _this2.masterPlaylistLoader_,\n          mode: _this2.mode_,\n          hls: _this2.hls_,\n          master: _this2.master(),\n          mediaTypes: _this2.mediaTypes_,\n          blacklistCurrentPlaylist: _this2.blacklistCurrentPlaylist.bind(_this2)\n        });\n\n        _this2.triggerPresenceUsage_(_this2.master(), media);\n\n        try {\n          _this2.setupSourceBuffers_();\n        } catch (e) {\n          _videoJs2['default'].log.warn('Failed to create SourceBuffers', e);\n          return _this2.mediaSource.endOfStream('decode');\n        }\n        _this2.setupFirstPlay();\n\n        _this2.trigger('selectedinitialmedia');\n      });\n\n      this.masterPlaylistLoader_.on('loadedplaylist', function () {\n        var updatedPlaylist = _this2.masterPlaylistLoader_.media();\n\n        if (!updatedPlaylist) {\n          var selectedMedia = undefined;\n\n          if (_this2.enableLowInitialPlaylist) {\n            selectedMedia = _this2.selectInitialPlaylist();\n          }\n\n          if (!selectedMedia) {\n            selectedMedia = _this2.selectPlaylist();\n          }\n\n          _this2.initialMedia_ = selectedMedia;\n          _this2.masterPlaylistLoader_.media(_this2.initialMedia_);\n          return;\n        }\n\n        if (_this2.useCueTags_) {\n          _this2.updateAdCues_(updatedPlaylist);\n        }\n\n        // TODO: Create a new event on the PlaylistLoader that signals\n        // that the segments have changed in some way and use that to\n        // update the SegmentLoader instead of doing it twice here and\n        // on `mediachange`\n        _this2.mainSegmentLoader_.playlist(updatedPlaylist, _this2.requestOptions_);\n        _this2.updateDuration();\n\n        // If the player isn't paused, ensure that the segment loader is running,\n        // as it is possible that it was temporarily stopped while waiting for\n        // a playlist (e.g., in case the playlist errored and we re-requested it).\n        if (!_this2.tech_.paused()) {\n          _this2.mainSegmentLoader_.load();\n        }\n\n        if (!updatedPlaylist.endList) {\n          (function () {\n            var addSeekableRange = function addSeekableRange() {\n              var seekable = _this2.seekable();\n\n              if (seekable.length !== 0) {\n                _this2.mediaSource.addSeekableRange_(seekable.start(0), seekable.end(0));\n              }\n            };\n\n            if (_this2.duration() !== Infinity) {\n              (function () {\n                var onDurationchange = function onDurationchange() {\n                  if (_this2.duration() === Infinity) {\n                    addSeekableRange();\n                  } else {\n                    _this2.tech_.one('durationchange', onDurationchange);\n                  }\n                };\n\n                _this2.tech_.one('durationchange', onDurationchange);\n              })();\n            } else {\n              addSeekableRange();\n            }\n          })();\n        }\n      });\n\n      this.masterPlaylistLoader_.on('error', function () {\n        _this2.blacklistCurrentPlaylist(_this2.masterPlaylistLoader_.error);\n      });\n\n      this.masterPlaylistLoader_.on('mediachanging', function () {\n        _this2.mainSegmentLoader_.abort();\n        _this2.mainSegmentLoader_.pause();\n      });\n\n      this.masterPlaylistLoader_.on('mediachange', function () {\n        var media = _this2.masterPlaylistLoader_.media();\n        var requestTimeout = _this2.masterPlaylistLoader_.targetDuration * 1.5 * 1000;\n\n        // If we don't have any more available playlists, we don't want to\n        // timeout the request.\n        if ((0, _playlistJs.isLowestEnabledRendition)(_this2.masterPlaylistLoader_.master, _this2.masterPlaylistLoader_.media())) {\n          _this2.requestOptions_.timeout = 0;\n        } else {\n          _this2.requestOptions_.timeout = requestTimeout;\n        }\n\n        // TODO: Create a new event on the PlaylistLoader that signals\n        // that the segments have changed in some way and use that to\n        // update the SegmentLoader instead of doing it twice here and\n        // on `loadedplaylist`\n        _this2.mainSegmentLoader_.playlist(media, _this2.requestOptions_);\n        _this2.mainSegmentLoader_.load();\n\n        _this2.tech_.trigger({\n          type: 'mediachange',\n          bubbles: true\n        });\n      });\n\n      this.masterPlaylistLoader_.on('playlistunchanged', function () {\n        var updatedPlaylist = _this2.masterPlaylistLoader_.media();\n        var playlistOutdated = _this2.stuckAtPlaylistEnd_(updatedPlaylist);\n\n        if (playlistOutdated) {\n          // Playlist has stopped updating and we're stuck at its end. Try to\n          // blacklist it and switch to another playlist in the hope that that\n          // one is updating (and give the player a chance to re-adjust to the\n          // safe live point).\n          _this2.blacklistCurrentPlaylist({\n            message: 'Playlist no longer updating.'\n          });\n          // useful for monitoring QoS\n          _this2.tech_.trigger('playliststuck');\n        }\n      });\n\n      this.masterPlaylistLoader_.on('renditiondisabled', function () {\n        _this2.tech_.trigger({ type: 'usage', name: 'hls-rendition-disabled' });\n      });\n      this.masterPlaylistLoader_.on('renditionenabled', function () {\n        _this2.tech_.trigger({ type: 'usage', name: 'hls-rendition-enabled' });\n      });\n    }\n\n    /**\n     * A helper function for triggerring presence usage events once per source\n     *\n     * @private\n     */\n  }, {\n    key: 'triggerPresenceUsage_',\n    value: function triggerPresenceUsage_(master, media) {\n      var mediaGroups = master.mediaGroups || {};\n      var defaultDemuxed = true;\n      var audioGroupKeys = Object.keys(mediaGroups.AUDIO);\n\n      for (var mediaGroup in mediaGroups.AUDIO) {\n        for (var label in mediaGroups.AUDIO[mediaGroup]) {\n          var properties = mediaGroups.AUDIO[mediaGroup][label];\n\n          if (!properties.uri) {\n            defaultDemuxed = false;\n          }\n        }\n      }\n\n      if (defaultDemuxed) {\n        this.tech_.trigger({ type: 'usage', name: 'hls-demuxed' });\n      }\n\n      if (Object.keys(mediaGroups.SUBTITLES).length) {\n        this.tech_.trigger({ type: 'usage', name: 'hls-webvtt' });\n      }\n\n      if (Hls.Playlist.isAes(media)) {\n        this.tech_.trigger({ type: 'usage', name: 'hls-aes' });\n      }\n\n      if (Hls.Playlist.isFmp4(media)) {\n        this.tech_.trigger({ type: 'usage', name: 'hls-fmp4' });\n      }\n\n      if (audioGroupKeys.length && Object.keys(mediaGroups.AUDIO[audioGroupKeys[0]]).length > 1) {\n        this.tech_.trigger({ type: 'usage', name: 'hls-alternate-audio' });\n      }\n\n      if (this.useCueTags_) {\n        this.tech_.trigger({ type: 'usage', name: 'hls-playlist-cue-tags' });\n      }\n    }\n\n    /**\n     * Register event handlers on the segment loaders. A helper function\n     * for construction time.\n     *\n     * @private\n     */\n  }, {\n    key: 'setupSegmentLoaderListeners_',\n    value: function setupSegmentLoaderListeners_() {\n      var _this3 = this;\n\n      this.mainSegmentLoader_.on('bandwidthupdate', function () {\n        var nextPlaylist = _this3.selectPlaylist();\n        var currentPlaylist = _this3.masterPlaylistLoader_.media();\n        var buffered = _this3.tech_.buffered();\n        var forwardBuffer = buffered.length ? buffered.end(buffered.length - 1) - _this3.tech_.currentTime() : 0;\n\n        var bufferLowWaterLine = _this3.bufferLowWaterLine();\n\n        // If the playlist is live, then we want to not take low water line into account.\n        // This is because in LIVE, the player plays 3 segments from the end of the\n        // playlist, and if `BUFFER_LOW_WATER_LINE` is greater than the duration availble\n        // in those segments, a viewer will never experience a rendition upswitch.\n        if (!currentPlaylist.endList ||\n        // For the same reason as LIVE, we ignore the low water line when the VOD\n        // duration is below the max potential low water line\n        _this3.duration() < _config2['default'].MAX_BUFFER_LOW_WATER_LINE ||\n        // we want to switch down to lower resolutions quickly to continue playback, but\n        nextPlaylist.attributes.BANDWIDTH < currentPlaylist.attributes.BANDWIDTH ||\n        // ensure we have some buffer before we switch up to prevent us running out of\n        // buffer while loading a higher rendition.\n        forwardBuffer >= bufferLowWaterLine) {\n          _this3.masterPlaylistLoader_.media(nextPlaylist);\n        }\n\n        _this3.tech_.trigger('bandwidthupdate');\n      });\n      this.mainSegmentLoader_.on('progress', function () {\n        _this3.trigger('progress');\n      });\n\n      this.mainSegmentLoader_.on('error', function () {\n        _this3.blacklistCurrentPlaylist(_this3.mainSegmentLoader_.error());\n      });\n\n      this.mainSegmentLoader_.on('syncinfoupdate', function () {\n        _this3.onSyncInfoUpdate_();\n      });\n\n      this.mainSegmentLoader_.on('timestampoffset', function () {\n        _this3.tech_.trigger({ type: 'usage', name: 'hls-timestamp-offset' });\n      });\n      this.audioSegmentLoader_.on('syncinfoupdate', function () {\n        _this3.onSyncInfoUpdate_();\n      });\n\n      this.mainSegmentLoader_.on('ended', function () {\n        _this3.onEndOfStream();\n      });\n\n      this.mainSegmentLoader_.on('earlyabort', function () {\n        _this3.blacklistCurrentPlaylist({\n          message: 'Aborted early because there isn\\'t enough bandwidth to complete the ' + 'request without rebuffering.'\n        }, ABORT_EARLY_BLACKLIST_SECONDS);\n      });\n\n      this.mainSegmentLoader_.on('reseteverything', function () {\n        // If playing an MTS stream, a videojs.MediaSource is listening for\n        // hls-reset to reset caption parsing state in the transmuxer\n        _this3.tech_.trigger('hls-reset');\n      });\n\n      this.mainSegmentLoader_.on('segmenttimemapping', function (event) {\n        // If playing an MTS stream in html, a videojs.MediaSource is listening for\n        // hls-segment-time-mapping update its internal mapping of stream to display time\n        _this3.tech_.trigger({\n          type: 'hls-segment-time-mapping',\n          mapping: event.mapping\n        });\n      });\n\n      this.audioSegmentLoader_.on('ended', function () {\n        _this3.onEndOfStream();\n      });\n    }\n  }, {\n    key: 'mediaSecondsLoaded_',\n    value: function mediaSecondsLoaded_() {\n      return Math.max(this.audioSegmentLoader_.mediaSecondsLoaded + this.mainSegmentLoader_.mediaSecondsLoaded);\n    }\n\n    /**\n     * Call load on our SegmentLoaders\n     */\n  }, {\n    key: 'load',\n    value: function load() {\n      this.mainSegmentLoader_.load();\n      if (this.mediaTypes_.AUDIO.activePlaylistLoader) {\n        this.audioSegmentLoader_.load();\n      }\n      if (this.mediaTypes_.SUBTITLES.activePlaylistLoader) {\n        this.subtitleSegmentLoader_.load();\n      }\n    }\n\n    /**\n     * Re-tune playback quality level for the current player\n     * conditions. This method may perform destructive actions, like\n     * removing already buffered content, to readjust the currently\n     * active playlist quickly.\n     *\n     * @private\n     */\n  }, {\n    key: 'fastQualityChange_',\n    value: function fastQualityChange_() {\n      var media = this.selectPlaylist();\n\n      if (media !== this.masterPlaylistLoader_.media()) {\n        this.masterPlaylistLoader_.media(media);\n\n        this.mainSegmentLoader_.resetLoader();\n        // don't need to reset audio as it is reset when media changes\n      }\n    }\n\n    /**\n     * Begin playback.\n     */\n  }, {\n    key: 'play',\n    value: function play() {\n      if (this.setupFirstPlay()) {\n        return;\n      }\n\n      if (this.tech_.ended()) {\n        this.tech_.setCurrentTime(0);\n      }\n\n      if (this.hasPlayed_()) {\n        this.load();\n      }\n\n      var seekable = this.tech_.seekable();\n\n      // if the viewer has paused and we fell out of the live window,\n      // seek forward to the live point\n      if (this.tech_.duration() === Infinity) {\n        if (this.tech_.currentTime() < seekable.start(0)) {\n          return this.tech_.setCurrentTime(seekable.end(seekable.length - 1));\n        }\n      }\n    }\n\n    /**\n     * Seek to the latest media position if this is a live video and the\n     * player and video are loaded and initialized.\n     */\n  }, {\n    key: 'setupFirstPlay',\n    value: function setupFirstPlay() {\n      var _this4 = this;\n\n      var media = this.masterPlaylistLoader_.media();\n\n      // Check that everything is ready to begin buffering for the first call to play\n      //  If 1) there is no active media\n      //     2) the player is paused\n      //     3) the first play has already been setup\n      // then exit early\n      if (!media || this.tech_.paused() || this.hasPlayed_()) {\n        return false;\n      }\n\n      // when the video is a live stream\n      if (!media.endList) {\n        var _ret3 = (function () {\n          var seekable = _this4.seekable();\n\n          if (!seekable.length) {\n            // without a seekable range, the player cannot seek to begin buffering at the live\n            // point\n            return {\n              v: false\n            };\n          }\n\n          if (_videoJs2['default'].browser.IE_VERSION && _this4.mode_ === 'html5' && _this4.tech_.readyState() === 0) {\n            // IE11 throws an InvalidStateError if you try to set currentTime while the\n            // readyState is 0, so it must be delayed until the tech fires loadedmetadata.\n            _this4.tech_.one('loadedmetadata', function () {\n              _this4.trigger('firstplay');\n              _this4.tech_.setCurrentTime(seekable.end(0));\n              _this4.hasPlayed_ = function () {\n                return true;\n              };\n            });\n\n            return {\n              v: false\n            };\n          }\n\n          // trigger firstplay to inform the source handler to ignore the next seek event\n          _this4.trigger('firstplay');\n          // seek to the live point\n          _this4.tech_.setCurrentTime(seekable.end(0));\n        })();\n\n        if (typeof _ret3 === 'object') return _ret3.v;\n      }\n\n      this.hasPlayed_ = function () {\n        return true;\n      };\n      // we can begin loading now that everything is ready\n      this.load();\n      return true;\n    }\n\n    /**\n     * handle the sourceopen event on the MediaSource\n     *\n     * @private\n     */\n  }, {\n    key: 'handleSourceOpen_',\n    value: function handleSourceOpen_() {\n      // Only attempt to create the source buffer if none already exist.\n      // handleSourceOpen is also called when we are \"re-opening\" a source buffer\n      // after `endOfStream` has been called (in response to a seek for instance)\n      try {\n        this.setupSourceBuffers_();\n      } catch (e) {\n        _videoJs2['default'].log.warn('Failed to create Source Buffers', e);\n        return this.mediaSource.endOfStream('decode');\n      }\n\n      // if autoplay is enabled, begin playback. This is duplicative of\n      // code in video.js but is required because play() must be invoked\n      // *after* the media source has opened.\n      if (this.tech_.autoplay()) {\n        var playPromise = this.tech_.play();\n\n        // Catch/silence error when a pause interrupts a play request\n        // on browsers which return a promise\n        if (typeof playPromise !== 'undefined' && typeof playPromise.then === 'function') {\n          playPromise.then(null, function (e) {});\n        }\n      }\n\n      this.trigger('sourceopen');\n    }\n\n    /**\n     * Calls endOfStream on the media source when all active stream types have called\n     * endOfStream\n     *\n     * @param {string} streamType\n     *        Stream type of the segment loader that called endOfStream\n     * @private\n     */\n  }, {\n    key: 'onEndOfStream',\n    value: function onEndOfStream() {\n      var isEndOfStream = this.mainSegmentLoader_.ended_;\n\n      if (this.mediaTypes_.AUDIO.activePlaylistLoader) {\n        // if the audio playlist loader exists, then alternate audio is active, so we need\n        // to wait for both the main and audio segment loaders to call endOfStream\n        isEndOfStream = isEndOfStream && this.audioSegmentLoader_.ended_;\n      }\n\n      if (isEndOfStream) {\n        this.mediaSource.endOfStream();\n      }\n    }\n\n    /**\n     * Check if a playlist has stopped being updated\n     * @param {Object} playlist the media playlist object\n     * @return {boolean} whether the playlist has stopped being updated or not\n     */\n  }, {\n    key: 'stuckAtPlaylistEnd_',\n    value: function stuckAtPlaylistEnd_(playlist) {\n      var seekable = this.seekable();\n\n      if (!seekable.length) {\n        // playlist doesn't have enough information to determine whether we are stuck\n        return false;\n      }\n\n      var expired = this.syncController_.getExpiredTime(playlist, this.mediaSource.duration);\n\n      if (expired === null) {\n        return false;\n      }\n\n      // does not use the safe live end to calculate playlist end, since we\n      // don't want to say we are stuck while there is still content\n      var absolutePlaylistEnd = Hls.Playlist.playlistEnd(playlist, expired);\n      var currentTime = this.tech_.currentTime();\n      var buffered = this.tech_.buffered();\n\n      if (!buffered.length) {\n        // return true if the playhead reached the absolute end of the playlist\n        return absolutePlaylistEnd - currentTime <= _ranges2['default'].SAFE_TIME_DELTA;\n      }\n      var bufferedEnd = buffered.end(buffered.length - 1);\n\n      // return true if there is too little buffer left and buffer has reached absolute\n      // end of playlist\n      return bufferedEnd - currentTime <= _ranges2['default'].SAFE_TIME_DELTA && absolutePlaylistEnd - bufferedEnd <= _ranges2['default'].SAFE_TIME_DELTA;\n    }\n\n    /**\n     * Blacklists a playlist when an error occurs for a set amount of time\n     * making it unavailable for selection by the rendition selection algorithm\n     * and then forces a new playlist (rendition) selection.\n     *\n     * @param {Object=} error an optional error that may include the playlist\n     * to blacklist\n     * @param {Number=} blacklistDuration an optional number of seconds to blacklist the\n     * playlist\n     */\n  }, {\n    key: 'blacklistCurrentPlaylist',\n    value: function blacklistCurrentPlaylist(error, blacklistDuration) {\n      if (error === undefined) error = {};\n\n      var currentPlaylist = undefined;\n      var nextPlaylist = undefined;\n\n      // If the `error` was generated by the playlist loader, it will contain\n      // the playlist we were trying to load (but failed) and that should be\n      // blacklisted instead of the currently selected playlist which is likely\n      // out-of-date in this scenario\n      currentPlaylist = error.playlist || this.masterPlaylistLoader_.media();\n\n      blacklistDuration = blacklistDuration || error.blacklistDuration || this.blacklistDuration;\n\n      // If there is no current playlist, then an error occurred while we were\n      // trying to load the master OR while we were disposing of the tech\n      if (!currentPlaylist) {\n        this.error = error;\n\n        try {\n          return this.mediaSource.endOfStream('network');\n        } catch (e) {\n          return this.trigger('error');\n        }\n      }\n\n      var isFinalRendition = this.masterPlaylistLoader_.master.playlists.filter(_playlistJs.isEnabled).length === 1;\n\n      if (isFinalRendition) {\n        // Never blacklisting this playlist because it's final rendition\n        _videoJs2['default'].log.warn('Problem encountered with the current ' + 'HLS playlist. Trying again since it is the final playlist.');\n\n        this.tech_.trigger('retryplaylist');\n        return this.masterPlaylistLoader_.load(isFinalRendition);\n      }\n      // Blacklist this playlist\n      currentPlaylist.excludeUntil = Date.now() + blacklistDuration * 1000;\n      this.tech_.trigger('blacklistplaylist');\n      this.tech_.trigger({ type: 'usage', name: 'hls-rendition-blacklisted' });\n\n      // Select a new playlist\n      nextPlaylist = this.selectPlaylist();\n      _videoJs2['default'].log.warn('Problem encountered with the current HLS playlist.' + (error.message ? ' ' + error.message : '') + ' Switching to another playlist.');\n\n      return this.masterPlaylistLoader_.media(nextPlaylist);\n    }\n\n    /**\n     * Pause all segment loaders\n     */\n  }, {\n    key: 'pauseLoading',\n    value: function pauseLoading() {\n      this.mainSegmentLoader_.pause();\n      if (this.mediaTypes_.AUDIO.activePlaylistLoader) {\n        this.audioSegmentLoader_.pause();\n      }\n      if (this.mediaTypes_.SUBTITLES.activePlaylistLoader) {\n        this.subtitleSegmentLoader_.pause();\n      }\n    }\n\n    /**\n     * set the current time on all segment loaders\n     *\n     * @param {TimeRange} currentTime the current time to set\n     * @return {TimeRange} the current time\n     */\n  }, {\n    key: 'setCurrentTime',\n    value: function setCurrentTime(currentTime) {\n      var buffered = _ranges2['default'].findRange(this.tech_.buffered(), currentTime);\n\n      if (!(this.masterPlaylistLoader_ && this.masterPlaylistLoader_.media())) {\n        // return immediately if the metadata is not ready yet\n        return 0;\n      }\n\n      // it's clearly an edge-case but don't thrown an error if asked to\n      // seek within an empty playlist\n      if (!this.masterPlaylistLoader_.media().segments) {\n        return 0;\n      }\n\n      // In flash playback, the segment loaders should be reset on every seek, even\n      // in buffer seeks. If the seek location is already buffered, continue buffering as\n      // usual\n      if (buffered && buffered.length && this.mode_ !== 'flash') {\n        return currentTime;\n      }\n\n      // cancel outstanding requests so we begin buffering at the new\n      // location\n      this.mainSegmentLoader_.resetEverything();\n      this.mainSegmentLoader_.abort();\n      if (this.mediaTypes_.AUDIO.activePlaylistLoader) {\n        this.audioSegmentLoader_.resetEverything();\n        this.audioSegmentLoader_.abort();\n      }\n      if (this.mediaTypes_.SUBTITLES.activePlaylistLoader) {\n        this.subtitleSegmentLoader_.resetEverything();\n        this.subtitleSegmentLoader_.abort();\n      }\n\n      // start segment loader loading in case they are paused\n      this.load();\n    }\n\n    /**\n     * get the current duration\n     *\n     * @return {TimeRange} the duration\n     */\n  }, {\n    key: 'duration',\n    value: function duration() {\n      if (!this.masterPlaylistLoader_) {\n        return 0;\n      }\n\n      if (this.mediaSource) {\n        return this.mediaSource.duration;\n      }\n\n      return Hls.Playlist.duration(this.masterPlaylistLoader_.media());\n    }\n\n    /**\n     * check the seekable range\n     *\n     * @return {TimeRange} the seekable range\n     */\n  }, {\n    key: 'seekable',\n    value: function seekable() {\n      return this.seekable_;\n    }\n  }, {\n    key: 'onSyncInfoUpdate_',\n    value: function onSyncInfoUpdate_() {\n      var mainSeekable = undefined;\n      var audioSeekable = undefined;\n\n      if (!this.masterPlaylistLoader_) {\n        return;\n      }\n\n      var media = this.masterPlaylistLoader_.media();\n\n      if (!media) {\n        return;\n      }\n\n      var expired = this.syncController_.getExpiredTime(media, this.mediaSource.duration);\n\n      if (expired === null) {\n        // not enough information to update seekable\n        return;\n      }\n\n      mainSeekable = Hls.Playlist.seekable(media, expired);\n\n      if (mainSeekable.length === 0) {\n        return;\n      }\n\n      if (this.mediaTypes_.AUDIO.activePlaylistLoader) {\n        media = this.mediaTypes_.AUDIO.activePlaylistLoader.media();\n        expired = this.syncController_.getExpiredTime(media, this.mediaSource.duration);\n\n        if (expired === null) {\n          return;\n        }\n\n        audioSeekable = Hls.Playlist.seekable(media, expired);\n\n        if (audioSeekable.length === 0) {\n          return;\n        }\n      }\n\n      if (!audioSeekable) {\n        // seekable has been calculated based on buffering video data so it\n        // can be returned directly\n        this.seekable_ = mainSeekable;\n      } else if (audioSeekable.start(0) > mainSeekable.end(0) || mainSeekable.start(0) > audioSeekable.end(0)) {\n        // seekables are pretty far off, rely on main\n        this.seekable_ = mainSeekable;\n      } else {\n        this.seekable_ = _videoJs2['default'].createTimeRanges([[audioSeekable.start(0) > mainSeekable.start(0) ? audioSeekable.start(0) : mainSeekable.start(0), audioSeekable.end(0) < mainSeekable.end(0) ? audioSeekable.end(0) : mainSeekable.end(0)]]);\n      }\n\n      this.tech_.trigger('seekablechanged');\n    }\n\n    /**\n     * Update the player duration\n     */\n  }, {\n    key: 'updateDuration',\n    value: function updateDuration() {\n      var _this5 = this;\n\n      var oldDuration = this.mediaSource.duration;\n      var newDuration = Hls.Playlist.duration(this.masterPlaylistLoader_.media());\n      var buffered = this.tech_.buffered();\n      var setDuration = function setDuration() {\n        _this5.mediaSource.duration = newDuration;\n        _this5.tech_.trigger('durationchange');\n\n        _this5.mediaSource.removeEventListener('sourceopen', setDuration);\n      };\n\n      if (buffered.length > 0) {\n        newDuration = Math.max(newDuration, buffered.end(buffered.length - 1));\n      }\n\n      // if the duration has changed, invalidate the cached value\n      if (oldDuration !== newDuration) {\n        // update the duration\n        if (this.mediaSource.readyState !== 'open') {\n          this.mediaSource.addEventListener('sourceopen', setDuration);\n        } else {\n          setDuration();\n        }\n      }\n    }\n\n    /**\n     * dispose of the MasterPlaylistController and everything\n     * that it controls\n     */\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      var _this6 = this;\n\n      this.decrypter_.terminate();\n      this.masterPlaylistLoader_.dispose();\n      this.mainSegmentLoader_.dispose();\n\n      ['AUDIO', 'SUBTITLES'].forEach(function (type) {\n        var groups = _this6.mediaTypes_[type].groups;\n\n        for (var id in groups) {\n          groups[id].forEach(function (group) {\n            if (group.playlistLoader) {\n              group.playlistLoader.dispose();\n            }\n          });\n        }\n      });\n\n      this.audioSegmentLoader_.dispose();\n      this.subtitleSegmentLoader_.dispose();\n    }\n\n    /**\n     * return the master playlist object if we have one\n     *\n     * @return {Object} the master playlist object that we parsed\n     */\n  }, {\n    key: 'master',\n    value: function master() {\n      return this.masterPlaylistLoader_.master;\n    }\n\n    /**\n     * return the currently selected playlist\n     *\n     * @return {Object} the currently selected playlist object that we parsed\n     */\n  }, {\n    key: 'media',\n    value: function media() {\n      // playlist loader will not return media if it has not been fully loaded\n      return this.masterPlaylistLoader_.media() || this.initialMedia_;\n    }\n\n    /**\n     * setup our internal source buffers on our segment Loaders\n     *\n     * @private\n     */\n  }, {\n    key: 'setupSourceBuffers_',\n    value: function setupSourceBuffers_() {\n      var media = this.masterPlaylistLoader_.media();\n      var mimeTypes = undefined;\n\n      // wait until a media playlist is available and the Media Source is\n      // attached\n      if (!media || this.mediaSource.readyState !== 'open') {\n        return;\n      }\n\n      mimeTypes = mimeTypesForPlaylist_(this.masterPlaylistLoader_.master, media);\n      if (mimeTypes.length < 1) {\n        this.error = 'No compatible SourceBuffer configuration for the variant stream:' + media.resolvedUri;\n        return this.mediaSource.endOfStream('decode');\n      }\n      this.mainSegmentLoader_.mimeType(mimeTypes[0]);\n      if (mimeTypes[1]) {\n        this.audioSegmentLoader_.mimeType(mimeTypes[1]);\n      }\n\n      // exclude any incompatible variant streams from future playlist\n      // selection\n      this.excludeIncompatibleVariants_(media);\n    }\n\n    /**\n     * Blacklist playlists that are known to be codec or\n     * stream-incompatible with the SourceBuffer configuration. For\n     * instance, Media Source Extensions would cause the video element to\n     * stall waiting for video data if you switched from a variant with\n     * video and audio to an audio-only one.\n     *\n     * @param {Object} media a media playlist compatible with the current\n     * set of SourceBuffers. Variants in the current master playlist that\n     * do not appear to have compatible codec or stream configurations\n     * will be excluded from the default playlist selection algorithm\n     * indefinitely.\n     * @private\n     */\n  }, {\n    key: 'excludeIncompatibleVariants_',\n    value: function excludeIncompatibleVariants_(media) {\n      var master = this.masterPlaylistLoader_.master;\n      var codecCount = 2;\n      var videoCodec = null;\n      var codecs = undefined;\n\n      if (media.attributes.CODECS) {\n        codecs = (0, _utilCodecsJs.parseCodecs)(media.attributes.CODECS);\n        videoCodec = codecs.videoCodec;\n        codecCount = codecs.codecCount;\n      }\n      master.playlists.forEach(function (variant) {\n        var variantCodecs = {\n          codecCount: 2,\n          videoCodec: null\n        };\n\n        if (variant.attributes.CODECS) {\n          var codecString = variant.attributes.CODECS;\n\n          variantCodecs = (0, _utilCodecsJs.parseCodecs)(codecString);\n\n          if (window.MediaSource && window.MediaSource.isTypeSupported && !window.MediaSource.isTypeSupported('video/mp4; codecs=\"' + mapLegacyAvcCodecs_(codecString) + '\"')) {\n            variant.excludeUntil = Infinity;\n          }\n        }\n\n        // if the streams differ in the presence or absence of audio or\n        // video, they are incompatible\n        if (variantCodecs.codecCount !== codecCount) {\n          variant.excludeUntil = Infinity;\n        }\n\n        // if h.264 is specified on the current playlist, some flavor of\n        // it must be specified on all compatible variants\n        if (variantCodecs.videoCodec !== videoCodec) {\n          variant.excludeUntil = Infinity;\n        }\n      });\n    }\n  }, {\n    key: 'updateAdCues_',\n    value: function updateAdCues_(media) {\n      var offset = 0;\n      var seekable = this.seekable();\n\n      if (seekable.length) {\n        offset = seekable.start(0);\n      }\n\n      _adCueTags2['default'].updateAdCues(media, this.cueTagsTrack_, offset);\n    }\n\n    /**\n     * Calculates the desired forward buffer length based on current time\n     *\n     * @return {Number} Desired forward buffer length in seconds\n     */\n  }, {\n    key: 'goalBufferLength',\n    value: function goalBufferLength() {\n      var currentTime = this.tech_.currentTime();\n      var initial = _config2['default'].GOAL_BUFFER_LENGTH;\n      var rate = _config2['default'].GOAL_BUFFER_LENGTH_RATE;\n      var max = Math.max(initial, _config2['default'].MAX_GOAL_BUFFER_LENGTH);\n\n      return Math.min(initial + currentTime * rate, max);\n    }\n\n    /**\n     * Calculates the desired buffer low water line based on current time\n     *\n     * @return {Number} Desired buffer low water line in seconds\n     */\n  }, {\n    key: 'bufferLowWaterLine',\n    value: function bufferLowWaterLine() {\n      var currentTime = this.tech_.currentTime();\n      var initial = _config2['default'].BUFFER_LOW_WATER_LINE;\n      var rate = _config2['default'].BUFFER_LOW_WATER_LINE_RATE;\n      var max = Math.max(initial, _config2['default'].MAX_BUFFER_LOW_WATER_LINE);\n\n      return Math.min(initial + currentTime * rate, max);\n    }\n  }]);\n\n  return MasterPlaylistController;\n})(_videoJs2['default'].EventTarget);\n\nexports.MasterPlaylistController = MasterPlaylistController;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/master-playlist-controller.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/media-groups.js":
/*!**************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/media-groups.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _playlistLoader = __webpack_require__(/*! ./playlist-loader */ \"./node_modules/videojs-contrib-hls/es5/playlist-loader.js\");\n\nvar _playlistLoader2 = _interopRequireDefault(_playlistLoader);\n\nvar noop = function noop() {};\n\n/**\n * Convert the properties of an HLS track into an audioTrackKind.\n *\n * @private\n */\nvar audioTrackKind_ = function audioTrackKind_(properties) {\n  var kind = properties['default'] ? 'main' : 'alternative';\n\n  if (properties.characteristics && properties.characteristics.indexOf('public.accessibility.describes-video') >= 0) {\n    kind = 'main-desc';\n  }\n\n  return kind;\n};\n\n/**\n * Pause provided segment loader and playlist loader if active\n *\n * @param {SegmentLoader} segmentLoader\n *        SegmentLoader to pause\n * @param {Object} mediaType\n *        Active media type\n * @function stopLoaders\n */\nvar stopLoaders = function stopLoaders(segmentLoader, mediaType) {\n  segmentLoader.abort();\n  segmentLoader.pause();\n\n  if (mediaType && mediaType.activePlaylistLoader) {\n    mediaType.activePlaylistLoader.pause();\n    mediaType.activePlaylistLoader = null;\n  }\n};\n\nexports.stopLoaders = stopLoaders;\n/**\n * Start loading provided segment loader and playlist loader\n *\n * @param {PlaylistLoader} playlistLoader\n *        PlaylistLoader to start loading\n * @param {Object} mediaType\n *        Active media type\n * @function startLoaders\n */\nvar startLoaders = function startLoaders(playlistLoader, mediaType) {\n  // Segment loader will be started after `loadedmetadata` or `loadedplaylist` from the\n  // playlist loader\n  mediaType.activePlaylistLoader = playlistLoader;\n  playlistLoader.load();\n};\n\nexports.startLoaders = startLoaders;\n/**\n * Returns a function to be called when the media group changes. It performs a\n * non-destructive (preserve the buffer) resync of the SegmentLoader. This is because a\n * change of group is merely a rendition switch of the same content at another encoding,\n * rather than a change of content, such as switching audio from English to Spanish.\n *\n * @param {String} type\n *        MediaGroup type\n * @param {Object} settings\n *        Object containing required information for media groups\n * @return {Function}\n *         Handler for a non-destructive resync of SegmentLoader when the active media\n *         group changes.\n * @function onGroupChanged\n */\nvar onGroupChanged = function onGroupChanged(type, settings) {\n  return function () {\n    var _settings$segmentLoaders = settings.segmentLoaders;\n    var segmentLoader = _settings$segmentLoaders[type];\n    var mainSegmentLoader = _settings$segmentLoaders.main;\n    var mediaType = settings.mediaTypes[type];\n\n    var activeTrack = mediaType.activeTrack();\n    var activeGroup = mediaType.activeGroup(activeTrack);\n    var previousActiveLoader = mediaType.activePlaylistLoader;\n\n    stopLoaders(segmentLoader, mediaType);\n\n    if (!activeGroup) {\n      // there is no group active\n      return;\n    }\n\n    if (!activeGroup.playlistLoader) {\n      if (previousActiveLoader) {\n        // The previous group had a playlist loader but the new active group does not\n        // this means we are switching from demuxed to muxed audio. In this case we want to\n        // do a destructive reset of the main segment loader and not restart the audio\n        // loaders.\n        mainSegmentLoader.resetEverything();\n      }\n      return;\n    }\n\n    // Non-destructive resync\n    segmentLoader.resyncLoader();\n\n    startLoaders(activeGroup.playlistLoader, mediaType);\n  };\n};\n\nexports.onGroupChanged = onGroupChanged;\n/**\n * Returns a function to be called when the media track changes. It performs a\n * destructive reset of the SegmentLoader to ensure we start loading as close to\n * currentTime as possible.\n *\n * @param {String} type\n *        MediaGroup type\n * @param {Object} settings\n *        Object containing required information for media groups\n * @return {Function}\n *         Handler for a destructive reset of SegmentLoader when the active media\n *         track changes.\n * @function onTrackChanged\n */\nvar onTrackChanged = function onTrackChanged(type, settings) {\n  return function () {\n    var _settings$segmentLoaders2 = settings.segmentLoaders;\n    var segmentLoader = _settings$segmentLoaders2[type];\n    var mainSegmentLoader = _settings$segmentLoaders2.main;\n    var mediaType = settings.mediaTypes[type];\n\n    var activeTrack = mediaType.activeTrack();\n    var activeGroup = mediaType.activeGroup(activeTrack);\n    var previousActiveLoader = mediaType.activePlaylistLoader;\n\n    stopLoaders(segmentLoader, mediaType);\n\n    if (!activeGroup) {\n      // there is no group active so we do not want to restart loaders\n      return;\n    }\n\n    if (!activeGroup.playlistLoader) {\n      // when switching from demuxed audio/video to muxed audio/video (noted by no playlist\n      // loader for the audio group), we want to do a destructive reset of the main segment\n      // loader and not restart the audio loaders\n      mainSegmentLoader.resetEverything();\n      return;\n    }\n\n    if (previousActiveLoader === activeGroup.playlistLoader) {\n      // Nothing has actually changed. This can happen because track change events can fire\n      // multiple times for a \"single\" change. One for enabling the new active track, and\n      // one for disabling the track that was active\n      startLoaders(activeGroup.playlistLoader, mediaType);\n      return;\n    }\n\n    if (segmentLoader.track) {\n      // For WebVTT, set the new text track in the segmentloader\n      segmentLoader.track(activeTrack);\n    }\n\n    // destructive reset\n    segmentLoader.resetEverything();\n\n    startLoaders(activeGroup.playlistLoader, mediaType);\n  };\n};\n\nexports.onTrackChanged = onTrackChanged;\nvar onError = {\n  /**\n   * Returns a function to be called when a SegmentLoader or PlaylistLoader encounters\n   * an error.\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @return {Function}\n   *         Error handler. Logs warning (or error if the playlist is blacklisted) to\n   *         console and switches back to default audio track.\n   * @function onError.AUDIO\n   */\n  AUDIO: function AUDIO(type, settings) {\n    return function () {\n      var segmentLoader = settings.segmentLoaders[type];\n      var mediaType = settings.mediaTypes[type];\n      var blacklistCurrentPlaylist = settings.blacklistCurrentPlaylist;\n\n      stopLoaders(segmentLoader, mediaType);\n\n      // switch back to default audio track\n      var activeTrack = mediaType.activeTrack();\n      var activeGroup = mediaType.activeGroup();\n      var id = (activeGroup.filter(function (group) {\n        return group['default'];\n      })[0] || activeGroup[0]).id;\n      var defaultTrack = mediaType.tracks[id];\n\n      if (activeTrack === defaultTrack) {\n        // Default track encountered an error. All we can do now is blacklist the current\n        // rendition and hope another will switch audio groups\n        blacklistCurrentPlaylist({\n          message: 'Problem encountered loading the default audio track.'\n        });\n        return;\n      }\n\n      _videoJs2['default'].log.warn('Problem encountered loading the alternate audio track.' + 'Switching back to default.');\n\n      for (var trackId in mediaType.tracks) {\n        mediaType.tracks[trackId].enabled = mediaType.tracks[trackId] === defaultTrack;\n      }\n\n      mediaType.onTrackChanged();\n    };\n  },\n  /**\n   * Returns a function to be called when a SegmentLoader or PlaylistLoader encounters\n   * an error.\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @return {Function}\n   *         Error handler. Logs warning to console and disables the active subtitle track\n   * @function onError.SUBTITLES\n   */\n  SUBTITLES: function SUBTITLES(type, settings) {\n    return function () {\n      var segmentLoader = settings.segmentLoaders[type];\n      var mediaType = settings.mediaTypes[type];\n\n      _videoJs2['default'].log.warn('Problem encountered loading the subtitle track.' + 'Disabling subtitle track.');\n\n      stopLoaders(segmentLoader, mediaType);\n\n      var track = mediaType.activeTrack();\n\n      if (track) {\n        track.mode = 'disabled';\n      }\n\n      mediaType.onTrackChanged();\n    };\n  }\n};\n\nexports.onError = onError;\nvar setupListeners = {\n  /**\n   * Setup event listeners for audio playlist loader\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {PlaylistLoader|null} playlistLoader\n   *        PlaylistLoader to register listeners on\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @function setupListeners.AUDIO\n   */\n  AUDIO: function AUDIO(type, playlistLoader, settings) {\n    if (!playlistLoader) {\n      // no playlist loader means audio will be muxed with the video\n      return;\n    }\n\n    var tech = settings.tech;\n    var requestOptions = settings.requestOptions;\n    var segmentLoader = settings.segmentLoaders[type];\n\n    playlistLoader.on('loadedmetadata', function () {\n      var media = playlistLoader.media();\n\n      segmentLoader.playlist(media, requestOptions);\n\n      // if the video is already playing, or if this isn't a live video and preload\n      // permits, start downloading segments\n      if (!tech.paused() || media.endList && tech.preload() !== 'none') {\n        segmentLoader.load();\n      }\n    });\n\n    playlistLoader.on('loadedplaylist', function () {\n      segmentLoader.playlist(playlistLoader.media(), requestOptions);\n\n      // If the player isn't paused, ensure that the segment loader is running\n      if (!tech.paused()) {\n        segmentLoader.load();\n      }\n    });\n\n    playlistLoader.on('error', onError[type](type, settings));\n  },\n  /**\n   * Setup event listeners for subtitle playlist loader\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {PlaylistLoader|null} playlistLoader\n   *        PlaylistLoader to register listeners on\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @function setupListeners.SUBTITLES\n   */\n  SUBTITLES: function SUBTITLES(type, playlistLoader, settings) {\n    var tech = settings.tech;\n    var requestOptions = settings.requestOptions;\n    var segmentLoader = settings.segmentLoaders[type];\n    var mediaType = settings.mediaTypes[type];\n\n    playlistLoader.on('loadedmetadata', function () {\n      var media = playlistLoader.media();\n\n      segmentLoader.playlist(media, requestOptions);\n      segmentLoader.track(mediaType.activeTrack());\n\n      // if the video is already playing, or if this isn't a live video and preload\n      // permits, start downloading segments\n      if (!tech.paused() || media.endList && tech.preload() !== 'none') {\n        segmentLoader.load();\n      }\n    });\n\n    playlistLoader.on('loadedplaylist', function () {\n      segmentLoader.playlist(playlistLoader.media(), requestOptions);\n\n      // If the player isn't paused, ensure that the segment loader is running\n      if (!tech.paused()) {\n        segmentLoader.load();\n      }\n    });\n\n    playlistLoader.on('error', onError[type](type, settings));\n  }\n};\n\nexports.setupListeners = setupListeners;\nvar initialize = {\n  /**\n   * Setup PlaylistLoaders and AudioTracks for the audio groups\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @function initialize.AUDIO\n   */\n  'AUDIO': function AUDIO(type, settings) {\n    var mode = settings.mode;\n    var hls = settings.hls;\n    var segmentLoader = settings.segmentLoaders[type];\n    var requestOptions = settings.requestOptions;\n    var mediaGroups = settings.master.mediaGroups;\n    var _settings$mediaTypes$type = settings.mediaTypes[type];\n    var groups = _settings$mediaTypes$type.groups;\n    var tracks = _settings$mediaTypes$type.tracks;\n\n    // force a default if we have none or we are not\n    // in html5 mode (the only mode to support more than one\n    // audio track)\n    if (!mediaGroups[type] || Object.keys(mediaGroups[type]).length === 0 || mode !== 'html5') {\n      mediaGroups[type] = { main: { 'default': { 'default': true } } };\n    }\n\n    for (var groupId in mediaGroups[type]) {\n      if (!groups[groupId]) {\n        groups[groupId] = [];\n      }\n\n      for (var variantLabel in mediaGroups[type][groupId]) {\n        var properties = mediaGroups[type][groupId][variantLabel];\n        var playlistLoader = undefined;\n\n        if (properties.resolvedUri) {\n          playlistLoader = new _playlistLoader2['default'](properties.resolvedUri, hls, requestOptions);\n        } else {\n          // no resolvedUri means the audio is muxed with the video when using this\n          // audio track\n          playlistLoader = null;\n        }\n\n        properties = _videoJs2['default'].mergeOptions({ id: variantLabel, playlistLoader: playlistLoader }, properties);\n\n        setupListeners[type](type, properties.playlistLoader, settings);\n\n        groups[groupId].push(properties);\n\n        if (typeof tracks[variantLabel] === 'undefined') {\n          var track = new _videoJs2['default'].AudioTrack({\n            id: variantLabel,\n            kind: audioTrackKind_(properties),\n            enabled: false,\n            language: properties.language,\n            'default': properties['default'],\n            label: variantLabel\n          });\n\n          tracks[variantLabel] = track;\n        }\n      }\n    }\n\n    // setup single error event handler for the segment loader\n    segmentLoader.on('error', onError[type](type, settings));\n  },\n  /**\n   * Setup PlaylistLoaders and TextTracks for the subtitle groups\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @function initialize.SUBTITLES\n   */\n  'SUBTITLES': function SUBTITLES(type, settings) {\n    var tech = settings.tech;\n    var hls = settings.hls;\n    var segmentLoader = settings.segmentLoaders[type];\n    var requestOptions = settings.requestOptions;\n    var mediaGroups = settings.master.mediaGroups;\n    var _settings$mediaTypes$type2 = settings.mediaTypes[type];\n    var groups = _settings$mediaTypes$type2.groups;\n    var tracks = _settings$mediaTypes$type2.tracks;\n\n    for (var groupId in mediaGroups[type]) {\n      if (!groups[groupId]) {\n        groups[groupId] = [];\n      }\n\n      for (var variantLabel in mediaGroups[type][groupId]) {\n        if (mediaGroups[type][groupId][variantLabel].forced) {\n          // Subtitle playlists with the forced attribute are not selectable in Safari.\n          // According to Apple's HLS Authoring Specification:\n          //   If content has forced subtitles and regular subtitles in a given language,\n          //   the regular subtitles track in that language MUST contain both the forced\n          //   subtitles and the regular subtitles for that language.\n          // Because of this requirement and that Safari does not add forced subtitles,\n          // forced subtitles are skipped here to maintain consistent experience across\n          // all platforms\n          continue;\n        }\n\n        var properties = mediaGroups[type][groupId][variantLabel];\n\n        properties = _videoJs2['default'].mergeOptions({\n          id: variantLabel,\n          playlistLoader: new _playlistLoader2['default'](properties.resolvedUri, hls, requestOptions)\n        }, properties);\n\n        setupListeners[type](type, properties.playlistLoader, settings);\n\n        groups[groupId].push(properties);\n\n        if (typeof tracks[variantLabel] === 'undefined') {\n          var track = tech.addRemoteTextTrack({\n            id: variantLabel,\n            kind: 'subtitles',\n            enabled: false,\n            language: properties.language,\n            label: variantLabel\n          }, false).track;\n\n          tracks[variantLabel] = track;\n        }\n      }\n    }\n\n    // setup single error event handler for the segment loader\n    segmentLoader.on('error', onError[type](type, settings));\n  },\n  /**\n   * Setup TextTracks for the closed-caption groups\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @function initialize['CLOSED-CAPTIONS']\n   */\n  'CLOSED-CAPTIONS': function CLOSEDCAPTIONS(type, settings) {\n    var tech = settings.tech;\n    var mediaGroups = settings.master.mediaGroups;\n    var _settings$mediaTypes$type3 = settings.mediaTypes[type];\n    var groups = _settings$mediaTypes$type3.groups;\n    var tracks = _settings$mediaTypes$type3.tracks;\n\n    for (var groupId in mediaGroups[type]) {\n      if (!groups[groupId]) {\n        groups[groupId] = [];\n      }\n\n      for (var variantLabel in mediaGroups[type][groupId]) {\n        var properties = mediaGroups[type][groupId][variantLabel];\n\n        // We only support CEA608 captions for now, so ignore anything that\n        // doesn't use a CCx INSTREAM-ID\n        if (!properties.instreamId.match(/CC\\d/)) {\n          continue;\n        }\n\n        // No PlaylistLoader is required for Closed-Captions because the captions are\n        // embedded within the video stream\n        groups[groupId].push(_videoJs2['default'].mergeOptions({ id: variantLabel }, properties));\n\n        if (typeof tracks[variantLabel] === 'undefined') {\n          var track = tech.addRemoteTextTrack({\n            id: properties.instreamId,\n            kind: 'captions',\n            enabled: false,\n            language: properties.language,\n            label: variantLabel\n          }, false).track;\n\n          tracks[variantLabel] = track;\n        }\n      }\n    }\n  }\n};\n\nexports.initialize = initialize;\n/**\n * Returns a function used to get the active group of the provided type\n *\n * @param {String} type\n *        MediaGroup type\n * @param {Object} settings\n *        Object containing required information for media groups\n * @return {Function}\n *         Function that returns the active media group for the provided type. Takes an\n *         optional parameter {TextTrack} track. If no track is provided, a list of all\n *         variants in the group, otherwise the variant corresponding to the provided\n *         track is returned.\n * @function activeGroup\n */\nvar activeGroup = function activeGroup(type, settings) {\n  return function (track) {\n    var masterPlaylistLoader = settings.masterPlaylistLoader;\n    var groups = settings.mediaTypes[type].groups;\n\n    var media = masterPlaylistLoader.media();\n\n    if (!media) {\n      return null;\n    }\n\n    var variants = null;\n\n    if (media.attributes[type]) {\n      variants = groups[media.attributes[type]];\n    }\n\n    variants = variants || groups.main;\n\n    if (typeof track === 'undefined') {\n      return variants;\n    }\n\n    if (track === null) {\n      // An active track was specified so a corresponding group is expected. track === null\n      // means no track is currently active so there is no corresponding group\n      return null;\n    }\n\n    return variants.filter(function (props) {\n      return props.id === track.id;\n    })[0] || null;\n  };\n};\n\nexports.activeGroup = activeGroup;\nvar activeTrack = {\n  /**\n   * Returns a function used to get the active track of type provided\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @return {Function}\n   *         Function that returns the active media track for the provided type. Returns\n   *         null if no track is active\n   * @function activeTrack.AUDIO\n   */\n  AUDIO: function AUDIO(type, settings) {\n    return function () {\n      var tracks = settings.mediaTypes[type].tracks;\n\n      for (var id in tracks) {\n        if (tracks[id].enabled) {\n          return tracks[id];\n        }\n      }\n\n      return null;\n    };\n  },\n  /**\n   * Returns a function used to get the active track of type provided\n   *\n   * @param {String} type\n   *        MediaGroup type\n   * @param {Object} settings\n   *        Object containing required information for media groups\n   * @return {Function}\n   *         Function that returns the active media track for the provided type. Returns\n   *         null if no track is active\n   * @function activeTrack.SUBTITLES\n   */\n  SUBTITLES: function SUBTITLES(type, settings) {\n    return function () {\n      var tracks = settings.mediaTypes[type].tracks;\n\n      for (var id in tracks) {\n        if (tracks[id].mode === 'showing') {\n          return tracks[id];\n        }\n      }\n\n      return null;\n    };\n  }\n};\n\nexports.activeTrack = activeTrack;\n/**\n * Setup PlaylistLoaders and Tracks for media groups (Audio, Subtitles,\n * Closed-Captions) specified in the master manifest.\n *\n * @param {Object} settings\n *        Object containing required information for setting up the media groups\n * @param {SegmentLoader} settings.segmentLoaders.AUDIO\n *        Audio segment loader\n * @param {SegmentLoader} settings.segmentLoaders.SUBTITLES\n *        Subtitle segment loader\n * @param {SegmentLoader} settings.segmentLoaders.main\n *        Main segment loader\n * @param {Tech} settings.tech\n *        The tech of the player\n * @param {Object} settings.requestOptions\n *        XHR request options used by the segment loaders\n * @param {PlaylistLoader} settings.masterPlaylistLoader\n *        PlaylistLoader for the master source\n * @param {String} mode\n *        Mode of the hls source handler. Can be 'auto', 'html5', or 'flash'\n * @param {HlsHandler} settings.hls\n *        HLS SourceHandler\n * @param {Object} settings.master\n *        The parsed master manifest\n * @param {Object} settings.mediaTypes\n *        Object to store the loaders, tracks, and utility methods for each media type\n * @param {Function} settings.blacklistCurrentPlaylist\n *        Blacklists the current rendition and forces a rendition switch.\n * @function setupMediaGroups\n */\nvar setupMediaGroups = function setupMediaGroups(settings) {\n  ['AUDIO', 'SUBTITLES', 'CLOSED-CAPTIONS'].forEach(function (type) {\n    initialize[type](type, settings);\n  });\n\n  var mediaTypes = settings.mediaTypes;\n  var masterPlaylistLoader = settings.masterPlaylistLoader;\n  var tech = settings.tech;\n  var hls = settings.hls;\n\n  // setup active group and track getters and change event handlers\n  ['AUDIO', 'SUBTITLES'].forEach(function (type) {\n    mediaTypes[type].activeGroup = activeGroup(type, settings);\n    mediaTypes[type].activeTrack = activeTrack[type](type, settings);\n    mediaTypes[type].onGroupChanged = onGroupChanged(type, settings);\n    mediaTypes[type].onTrackChanged = onTrackChanged(type, settings);\n  });\n\n  // DO NOT enable the default subtitle or caption track.\n  // DO enable the default audio track\n  var audioGroup = mediaTypes.AUDIO.activeGroup();\n  var groupId = (audioGroup.filter(function (group) {\n    return group['default'];\n  })[0] || audioGroup[0]).id;\n\n  mediaTypes.AUDIO.tracks[groupId].enabled = true;\n  mediaTypes.AUDIO.onTrackChanged();\n\n  masterPlaylistLoader.on('mediachange', function () {\n    ['AUDIO', 'SUBTITLES'].forEach(function (type) {\n      return mediaTypes[type].onGroupChanged();\n    });\n  });\n\n  // custom audio track change event handler for usage event\n  var onAudioTrackChanged = function onAudioTrackChanged() {\n    mediaTypes.AUDIO.onTrackChanged();\n    tech.trigger({ type: 'usage', name: 'hls-audio-change' });\n  };\n\n  tech.audioTracks().addEventListener('change', onAudioTrackChanged);\n  tech.remoteTextTracks().addEventListener('change', mediaTypes.SUBTITLES.onTrackChanged);\n\n  hls.on('dispose', function () {\n    tech.audioTracks().removeEventListener('change', onAudioTrackChanged);\n    tech.remoteTextTracks().removeEventListener('change', mediaTypes.SUBTITLES.onTrackChanged);\n  });\n\n  // clear existing audio tracks and add the ones we just created\n  tech.clearTracks('audio');\n\n  for (var id in mediaTypes.AUDIO.tracks) {\n    tech.audioTracks().addTrack(mediaTypes.AUDIO.tracks[id]);\n  }\n};\n\nexports.setupMediaGroups = setupMediaGroups;\n/**\n * Creates skeleton object used to store the loaders, tracks, and utility methods for each\n * media type\n *\n * @return {Object}\n *         Object to store the loaders, tracks, and utility methods for each media type\n * @function createMediaTypes\n */\nvar createMediaTypes = function createMediaTypes() {\n  var mediaTypes = {};\n\n  ['AUDIO', 'SUBTITLES', 'CLOSED-CAPTIONS'].forEach(function (type) {\n    mediaTypes[type] = {\n      groups: {},\n      tracks: {},\n      activePlaylistLoader: null,\n      activeGroup: noop,\n      activeTrack: noop,\n      onGroupChanged: noop,\n      onTrackChanged: noop\n    };\n  });\n\n  return mediaTypes;\n};\nexports.createMediaTypes = createMediaTypes;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/media-groups.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/media-segment-request.js":
/*!***********************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/media-segment-request.js ***!
  \***********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _binUtils = __webpack_require__(/*! ./bin-utils */ \"./node_modules/videojs-contrib-hls/es5/bin-utils.js\");\n\nvar REQUEST_ERRORS = {\n  FAILURE: 2,\n  TIMEOUT: -101,\n  ABORTED: -102\n};\n\nexports.REQUEST_ERRORS = REQUEST_ERRORS;\n/**\n * Turns segment byterange into a string suitable for use in\n * HTTP Range requests\n *\n * @param {Object} byterange - an object with two values defining the start and end\n *                             of a byte-range\n */\nvar byterangeStr = function byterangeStr(byterange) {\n  var byterangeStart = undefined;\n  var byterangeEnd = undefined;\n\n  // `byterangeEnd` is one less than `offset + length` because the HTTP range\n  // header uses inclusive ranges\n  byterangeEnd = byterange.offset + byterange.length - 1;\n  byterangeStart = byterange.offset;\n  return 'bytes=' + byterangeStart + '-' + byterangeEnd;\n};\n\n/**\n * Defines headers for use in the xhr request for a particular segment.\n *\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n */\nvar segmentXhrHeaders = function segmentXhrHeaders(segment) {\n  var headers = {};\n\n  if (segment.byterange) {\n    headers.Range = byterangeStr(segment.byterange);\n  }\n  return headers;\n};\n\n/**\n * Abort all requests\n *\n * @param {Object} activeXhrs - an object that tracks all XHR requests\n */\nvar abortAll = function abortAll(activeXhrs) {\n  activeXhrs.forEach(function (xhr) {\n    xhr.abort();\n  });\n};\n\n/**\n * Gather important bandwidth stats once a request has completed\n *\n * @param {Object} request - the XHR request from which to gather stats\n */\nvar getRequestStats = function getRequestStats(request) {\n  return {\n    bandwidth: request.bandwidth,\n    bytesReceived: request.bytesReceived || 0,\n    roundTripTime: request.roundTripTime || 0\n  };\n};\n\n/**\n * If possible gather bandwidth stats as a request is in\n * progress\n *\n * @param {Event} progressEvent - an event object from an XHR's progress event\n */\nvar getProgressStats = function getProgressStats(progressEvent) {\n  var request = progressEvent.target;\n  var roundTripTime = Date.now() - request.requestTime;\n  var stats = {\n    bandwidth: Infinity,\n    bytesReceived: 0,\n    roundTripTime: roundTripTime || 0\n  };\n\n  stats.bytesReceived = progressEvent.loaded;\n  // This can result in Infinity if stats.roundTripTime is 0 but that is ok\n  // because we should only use bandwidth stats on progress to determine when\n  // abort a request early due to insufficient bandwidth\n  stats.bandwidth = Math.floor(stats.bytesReceived / stats.roundTripTime * 8 * 1000);\n\n  return stats;\n};\n\n/**\n * Handle all error conditions in one place and return an object\n * with all the information\n *\n * @param {Error|null} error - if non-null signals an error occured with the XHR\n * @param {Object} request -  the XHR request that possibly generated the error\n */\nvar handleErrors = function handleErrors(error, request) {\n  if (request.timedout) {\n    return {\n      status: request.status,\n      message: 'HLS request timed-out at URL: ' + request.uri,\n      code: REQUEST_ERRORS.TIMEOUT,\n      xhr: request\n    };\n  }\n\n  if (request.aborted) {\n    return {\n      status: request.status,\n      message: 'HLS request aborted at URL: ' + request.uri,\n      code: REQUEST_ERRORS.ABORTED,\n      xhr: request\n    };\n  }\n\n  if (error) {\n    return {\n      status: request.status,\n      message: 'HLS request errored at URL: ' + request.uri,\n      code: REQUEST_ERRORS.FAILURE,\n      xhr: request\n    };\n  }\n\n  return null;\n};\n\n/**\n * Handle responses for key data and convert the key data to the correct format\n * for the decryption step later\n *\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n * @param {Function} finishProcessingFn - a callback to execute to continue processing\n *                                        this request\n */\nvar handleKeyResponse = function handleKeyResponse(segment, finishProcessingFn) {\n  return function (error, request) {\n    var response = request.response;\n    var errorObj = handleErrors(error, request);\n\n    if (errorObj) {\n      return finishProcessingFn(errorObj, segment);\n    }\n\n    if (response.byteLength !== 16) {\n      return finishProcessingFn({\n        status: request.status,\n        message: 'Invalid HLS key at URL: ' + request.uri,\n        code: REQUEST_ERRORS.FAILURE,\n        xhr: request\n      }, segment);\n    }\n\n    var view = new DataView(response);\n\n    segment.key.bytes = new Uint32Array([view.getUint32(0), view.getUint32(4), view.getUint32(8), view.getUint32(12)]);\n    return finishProcessingFn(null, segment);\n  };\n};\n\n/**\n * Handle init-segment responses\n *\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n * @param {Function} finishProcessingFn - a callback to execute to continue processing\n *                                        this request\n */\nvar handleInitSegmentResponse = function handleInitSegmentResponse(segment, finishProcessingFn) {\n  return function (error, request) {\n    var response = request.response;\n    var errorObj = handleErrors(error, request);\n\n    if (errorObj) {\n      return finishProcessingFn(errorObj, segment);\n    }\n\n    // stop processing if received empty content\n    if (response.byteLength === 0) {\n      return finishProcessingFn({\n        status: request.status,\n        message: 'Empty HLS segment content at URL: ' + request.uri,\n        code: REQUEST_ERRORS.FAILURE,\n        xhr: request\n      }, segment);\n    }\n\n    segment.map.bytes = new Uint8Array(request.response);\n    return finishProcessingFn(null, segment);\n  };\n};\n\n/**\n * Response handler for segment-requests being sure to set the correct\n * property depending on whether the segment is encryped or not\n * Also records and keeps track of stats that are used for ABR purposes\n *\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n * @param {Function} finishProcessingFn - a callback to execute to continue processing\n *                                        this request\n */\nvar handleSegmentResponse = function handleSegmentResponse(segment, finishProcessingFn) {\n  return function (error, request) {\n    var response = request.response;\n    var errorObj = handleErrors(error, request);\n\n    if (errorObj) {\n      return finishProcessingFn(errorObj, segment);\n    }\n\n    // stop processing if received empty content\n    if (response.byteLength === 0) {\n      return finishProcessingFn({\n        status: request.status,\n        message: 'Empty HLS segment content at URL: ' + request.uri,\n        code: REQUEST_ERRORS.FAILURE,\n        xhr: request\n      }, segment);\n    }\n\n    segment.stats = getRequestStats(request);\n\n    if (segment.key) {\n      segment.encryptedBytes = new Uint8Array(request.response);\n    } else {\n      segment.bytes = new Uint8Array(request.response);\n    }\n\n    return finishProcessingFn(null, segment);\n  };\n};\n\n/**\n * Decrypt the segment via the decryption web worker\n *\n * @param {WebWorker} decrypter - a WebWorker interface to AES-128 decryption routines\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n * @param {Function} doneFn - a callback that is executed after decryption has completed\n */\nvar decryptSegment = function decryptSegment(decrypter, segment, doneFn) {\n  var decryptionHandler = function decryptionHandler(event) {\n    if (event.data.source === segment.requestId) {\n      decrypter.removeEventListener('message', decryptionHandler);\n      var decrypted = event.data.decrypted;\n\n      segment.bytes = new Uint8Array(decrypted.bytes, decrypted.byteOffset, decrypted.byteLength);\n      return doneFn(null, segment);\n    }\n  };\n\n  decrypter.addEventListener('message', decryptionHandler);\n\n  // this is an encrypted segment\n  // incrementally decrypt the segment\n  decrypter.postMessage((0, _binUtils.createTransferableMessage)({\n    source: segment.requestId,\n    encrypted: segment.encryptedBytes,\n    key: segment.key.bytes,\n    iv: segment.key.iv\n  }), [segment.encryptedBytes.buffer, segment.key.bytes.buffer]);\n};\n\n/**\n * The purpose of this function is to get the most pertinent error from the\n * array of errors.\n * For instance if a timeout and two aborts occur, then the aborts were\n * likely triggered by the timeout so return that error object.\n */\nvar getMostImportantError = function getMostImportantError(errors) {\n  return errors.reduce(function (prev, err) {\n    return err.code > prev.code ? err : prev;\n  });\n};\n\n/**\n * This function waits for all XHRs to finish (with either success or failure)\n * before continueing processing via it's callback. The function gathers errors\n * from each request into a single errors array so that the error status for\n * each request can be examined later.\n *\n * @param {Object} activeXhrs - an object that tracks all XHR requests\n * @param {WebWorker} decrypter - a WebWorker interface to AES-128 decryption routines\n * @param {Function} doneFn - a callback that is executed after all resources have been\n *                            downloaded and any decryption completed\n */\nvar waitForCompletion = function waitForCompletion(activeXhrs, decrypter, doneFn) {\n  var errors = [];\n  var count = 0;\n\n  return function (error, segment) {\n    if (error) {\n      // If there are errors, we have to abort any outstanding requests\n      abortAll(activeXhrs);\n      errors.push(error);\n    }\n    count += 1;\n\n    if (count === activeXhrs.length) {\n      // Keep track of when *all* of the requests have completed\n      segment.endOfAllRequests = Date.now();\n\n      if (errors.length > 0) {\n        var worstError = getMostImportantError(errors);\n\n        return doneFn(worstError, segment);\n      }\n      if (segment.encryptedBytes) {\n        return decryptSegment(decrypter, segment, doneFn);\n      }\n      // Otherwise, everything is ready just continue\n      return doneFn(null, segment);\n    }\n  };\n};\n\n/**\n * Simple progress event callback handler that gathers some stats before\n * executing a provided callback with the `segment` object\n *\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n * @param {Function} progressFn - a callback that is executed each time a progress event\n *                                is received\n * @param {Event} event - the progress event object from XMLHttpRequest\n */\nvar handleProgress = function handleProgress(segment, progressFn) {\n  return function (event) {\n    segment.stats = _videoJs2['default'].mergeOptions(segment.stats, getProgressStats(event));\n\n    // record the time that we receive the first byte of data\n    if (!segment.stats.firstBytesReceivedAt && segment.stats.bytesReceived) {\n      segment.stats.firstBytesReceivedAt = Date.now();\n    }\n\n    return progressFn(event, segment);\n  };\n};\n\n/**\n * Load all resources and does any processing necessary for a media-segment\n *\n * Features:\n *   decrypts the media-segment if it has a key uri and an iv\n *   aborts *all* requests if *any* one request fails\n *\n * The segment object, at minimum, has the following format:\n * {\n *   resolvedUri: String,\n *   [byterange]: {\n *     offset: Number,\n *     length: Number\n *   },\n *   [key]: {\n *     resolvedUri: String\n *     [byterange]: {\n *       offset: Number,\n *       length: Number\n *     },\n *     iv: {\n *       bytes: Uint32Array\n *     }\n *   },\n *   [map]: {\n *     resolvedUri: String,\n *     [byterange]: {\n *       offset: Number,\n *       length: Number\n *     },\n *     [bytes]: Uint8Array\n *   }\n * }\n * ...where [name] denotes optional properties\n *\n * @param {Function} xhr - an instance of the xhr wrapper in xhr.js\n * @param {Object} xhrOptions - the base options to provide to all xhr requests\n * @param {WebWorker} decryptionWorker - a WebWorker interface to AES-128\n *                                       decryption routines\n * @param {Object} segment - a simplified copy of the segmentInfo object\n *                           from SegmentLoader\n * @param {Function} progressFn - a callback that receives progress events from the main\n *                                segment's xhr request\n * @param {Function} doneFn - a callback that is executed only once all requests have\n *                            succeeded or failed\n * @returns {Function} a function that, when invoked, immediately aborts all\n *                     outstanding requests\n */\nvar mediaSegmentRequest = function mediaSegmentRequest(xhr, xhrOptions, decryptionWorker, segment, progressFn, doneFn) {\n  var activeXhrs = [];\n  var finishProcessingFn = waitForCompletion(activeXhrs, decryptionWorker, doneFn);\n\n  // optionally, request the decryption key\n  if (segment.key) {\n    var keyRequestOptions = _videoJs2['default'].mergeOptions(xhrOptions, {\n      uri: segment.key.resolvedUri,\n      responseType: 'arraybuffer'\n    });\n    var keyRequestCallback = handleKeyResponse(segment, finishProcessingFn);\n    var keyXhr = xhr(keyRequestOptions, keyRequestCallback);\n\n    activeXhrs.push(keyXhr);\n  }\n\n  // optionally, request the associated media init segment\n  if (segment.map && !segment.map.bytes) {\n    var initSegmentOptions = _videoJs2['default'].mergeOptions(xhrOptions, {\n      uri: segment.map.resolvedUri,\n      responseType: 'arraybuffer',\n      headers: segmentXhrHeaders(segment.map)\n    });\n    var initSegmentRequestCallback = handleInitSegmentResponse(segment, finishProcessingFn);\n    var initSegmentXhr = xhr(initSegmentOptions, initSegmentRequestCallback);\n\n    activeXhrs.push(initSegmentXhr);\n  }\n\n  var segmentRequestOptions = _videoJs2['default'].mergeOptions(xhrOptions, {\n    uri: segment.resolvedUri,\n    responseType: 'arraybuffer',\n    headers: segmentXhrHeaders(segment)\n  });\n  var segmentRequestCallback = handleSegmentResponse(segment, finishProcessingFn);\n  var segmentXhr = xhr(segmentRequestOptions, segmentRequestCallback);\n\n  segmentXhr.addEventListener('progress', handleProgress(segment, progressFn));\n  activeXhrs.push(segmentXhr);\n\n  return function () {\n    return abortAll(activeXhrs);\n  };\n};\nexports.mediaSegmentRequest = mediaSegmentRequest;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/media-segment-request.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/playback-watcher.js":
/*!******************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/playback-watcher.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file playback-watcher.js\n *\n * Playback starts, and now my watch begins. It shall not end until my death. I shall\n * take no wait, hold no uncleared timeouts, father no bad seeks. I shall wear no crowns\n * and win no glory. I shall live and die at my post. I am the corrector of the underflow.\n * I am the watcher of gaps. I am the shield that guards the realms of seekable. I pledge\n * my life and honor to the Playback Watch, for this Player and all the Players to come.\n */\n\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _ranges = __webpack_require__(/*! ./ranges */ \"./node_modules/videojs-contrib-hls/es5/ranges.js\");\n\nvar _ranges2 = _interopRequireDefault(_ranges);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\n// Set of events that reset the playback-watcher time check logic and clear the timeout\nvar timerCancelEvents = ['seeking', 'seeked', 'pause', 'playing', 'error'];\n\n/**\n * @class PlaybackWatcher\n */\n\nvar PlaybackWatcher = (function () {\n  /**\n   * Represents an PlaybackWatcher object.\n   * @constructor\n   * @param {object} options an object that includes the tech and settings\n   */\n\n  function PlaybackWatcher(options) {\n    var _this = this;\n\n    _classCallCheck(this, PlaybackWatcher);\n\n    this.tech_ = options.tech;\n    this.seekable = options.seekable;\n\n    this.consecutiveUpdates = 0;\n    this.lastRecordedTime = null;\n    this.timer_ = null;\n    this.checkCurrentTimeTimeout_ = null;\n\n    if (options.debug) {\n      this.logger_ = _videoJs2['default'].log.bind(_videoJs2['default'], 'playback-watcher ->');\n    }\n    this.logger_('initialize');\n\n    var canPlayHandler = function canPlayHandler() {\n      return _this.monitorCurrentTime_();\n    };\n    var waitingHandler = function waitingHandler() {\n      return _this.techWaiting_();\n    };\n    var cancelTimerHandler = function cancelTimerHandler() {\n      return _this.cancelTimer_();\n    };\n    var fixesBadSeeksHandler = function fixesBadSeeksHandler() {\n      return _this.fixesBadSeeks_();\n    };\n\n    this.tech_.on('seekablechanged', fixesBadSeeksHandler);\n    this.tech_.on('waiting', waitingHandler);\n    this.tech_.on(timerCancelEvents, cancelTimerHandler);\n    this.tech_.on('canplay', canPlayHandler);\n\n    // Define the dispose function to clean up our events\n    this.dispose = function () {\n      _this.logger_('dispose');\n      _this.tech_.off('seekablechanged', fixesBadSeeksHandler);\n      _this.tech_.off('waiting', waitingHandler);\n      _this.tech_.off(timerCancelEvents, cancelTimerHandler);\n      _this.tech_.off('canplay', canPlayHandler);\n      if (_this.checkCurrentTimeTimeout_) {\n        _globalWindow2['default'].clearTimeout(_this.checkCurrentTimeTimeout_);\n      }\n      _this.cancelTimer_();\n    };\n  }\n\n  /**\n   * Periodically check current time to see if playback stopped\n   *\n   * @private\n   */\n\n  _createClass(PlaybackWatcher, [{\n    key: 'monitorCurrentTime_',\n    value: function monitorCurrentTime_() {\n      this.checkCurrentTime_();\n\n      if (this.checkCurrentTimeTimeout_) {\n        _globalWindow2['default'].clearTimeout(this.checkCurrentTimeTimeout_);\n      }\n\n      // 42 = 24 fps // 250 is what Webkit uses // FF uses 15\n      this.checkCurrentTimeTimeout_ = _globalWindow2['default'].setTimeout(this.monitorCurrentTime_.bind(this), 250);\n    }\n\n    /**\n     * The purpose of this function is to emulate the \"waiting\" event on\n     * browsers that do not emit it when they are waiting for more\n     * data to continue playback\n     *\n     * @private\n     */\n  }, {\n    key: 'checkCurrentTime_',\n    value: function checkCurrentTime_() {\n      if (this.tech_.seeking() && this.fixesBadSeeks_()) {\n        this.consecutiveUpdates = 0;\n        this.lastRecordedTime = this.tech_.currentTime();\n        return;\n      }\n\n      if (this.tech_.paused() || this.tech_.seeking()) {\n        return;\n      }\n\n      var currentTime = this.tech_.currentTime();\n      var buffered = this.tech_.buffered();\n\n      if (this.lastRecordedTime === currentTime && (!buffered.length || currentTime + _ranges2['default'].SAFE_TIME_DELTA >= buffered.end(buffered.length - 1))) {\n        // If current time is at the end of the final buffered region, then any playback\n        // stall is most likely caused by buffering in a low bandwidth environment. The tech\n        // should fire a `waiting` event in this scenario, but due to browser and tech\n        // inconsistencies (e.g. The Flash tech does not fire a `waiting` event when the end\n        // of the buffer is reached and has fallen off the live window). Calling\n        // `techWaiting_` here allows us to simulate responding to a native `waiting` event\n        // when the tech fails to emit one.\n        return this.techWaiting_();\n      }\n\n      if (this.consecutiveUpdates >= 5 && currentTime === this.lastRecordedTime) {\n        this.consecutiveUpdates++;\n        this.waiting_();\n      } else if (currentTime === this.lastRecordedTime) {\n        this.consecutiveUpdates++;\n      } else {\n        this.consecutiveUpdates = 0;\n        this.lastRecordedTime = currentTime;\n      }\n    }\n\n    /**\n     * Cancels any pending timers and resets the 'timeupdate' mechanism\n     * designed to detect that we are stalled\n     *\n     * @private\n     */\n  }, {\n    key: 'cancelTimer_',\n    value: function cancelTimer_() {\n      this.consecutiveUpdates = 0;\n\n      if (this.timer_) {\n        this.logger_('cancelTimer_');\n        clearTimeout(this.timer_);\n      }\n\n      this.timer_ = null;\n    }\n\n    /**\n     * Fixes situations where there's a bad seek\n     *\n     * @return {Boolean} whether an action was taken to fix the seek\n     * @private\n     */\n  }, {\n    key: 'fixesBadSeeks_',\n    value: function fixesBadSeeks_() {\n      var seeking = this.tech_.seeking();\n      var seekable = this.seekable();\n      var currentTime = this.tech_.currentTime();\n      var seekTo = undefined;\n\n      if (seeking && this.afterSeekableWindow_(seekable, currentTime)) {\n        var seekableEnd = seekable.end(seekable.length - 1);\n\n        // sync to live point (if VOD, our seekable was updated and we're simply adjusting)\n        seekTo = seekableEnd;\n      }\n\n      if (seeking && this.beforeSeekableWindow_(seekable, currentTime)) {\n        var seekableStart = seekable.start(0);\n\n        // sync to the beginning of the live window\n        // provide a buffer of .1 seconds to handle rounding/imprecise numbers\n        seekTo = seekableStart + _ranges2['default'].SAFE_TIME_DELTA;\n      }\n\n      if (typeof seekTo !== 'undefined') {\n        this.logger_('Trying to seek outside of seekable at time ' + currentTime + ' with ' + ('seekable range ' + _ranges2['default'].printableRange(seekable) + '. Seeking to ') + (seekTo + '.'));\n\n        this.tech_.setCurrentTime(seekTo);\n        return true;\n      }\n\n      return false;\n    }\n\n    /**\n     * Handler for situations when we determine the player is waiting.\n     *\n     * @private\n     */\n  }, {\n    key: 'waiting_',\n    value: function waiting_() {\n      if (this.techWaiting_()) {\n        return;\n      }\n\n      // All tech waiting checks failed. Use last resort correction\n      var currentTime = this.tech_.currentTime();\n      var buffered = this.tech_.buffered();\n      var currentRange = _ranges2['default'].findRange(buffered, currentTime);\n\n      // Sometimes the player can stall for unknown reasons within a contiguous buffered\n      // region with no indication that anything is amiss (seen in Firefox). Seeking to\n      // currentTime is usually enough to kickstart the player. This checks that the player\n      // is currently within a buffered region before attempting a corrective seek.\n      // Chrome does not appear to continue `timeupdate` events after a `waiting` event\n      // until there is ~ 3 seconds of forward buffer available. PlaybackWatcher should also\n      // make sure there is ~3 seconds of forward buffer before taking any corrective action\n      // to avoid triggering an `unknownwaiting` event when the network is slow.\n      if (currentRange.length && currentTime + 3 <= currentRange.end(0)) {\n        this.cancelTimer_();\n        this.tech_.setCurrentTime(currentTime);\n\n        this.logger_('Stopped at ' + currentTime + ' while inside a buffered region ' + ('[' + currentRange.start(0) + ' -> ' + currentRange.end(0) + ']. Attempting to resume ') + 'playback by seeking to the current time.');\n\n        // unknown waiting corrections may be useful for monitoring QoS\n        this.tech_.trigger({ type: 'usage', name: 'hls-unknown-waiting' });\n        return;\n      }\n    }\n\n    /**\n     * Handler for situations when the tech fires a `waiting` event\n     *\n     * @return {Boolean}\n     *         True if an action (or none) was needed to correct the waiting. False if no\n     *         checks passed\n     * @private\n     */\n  }, {\n    key: 'techWaiting_',\n    value: function techWaiting_() {\n      var seekable = this.seekable();\n      var currentTime = this.tech_.currentTime();\n\n      if (this.tech_.seeking() && this.fixesBadSeeks_()) {\n        // Tech is seeking or bad seek fixed, no action needed\n        return true;\n      }\n\n      if (this.tech_.seeking() || this.timer_ !== null) {\n        // Tech is seeking or already waiting on another action, no action needed\n        return true;\n      }\n\n      if (this.beforeSeekableWindow_(seekable, currentTime)) {\n        var livePoint = seekable.end(seekable.length - 1);\n\n        this.logger_('Fell out of live window at time ' + currentTime + '. Seeking to ' + ('live point (seekable end) ' + livePoint));\n        this.cancelTimer_();\n        this.tech_.setCurrentTime(livePoint);\n\n        // live window resyncs may be useful for monitoring QoS\n        this.tech_.trigger({ type: 'usage', name: 'hls-live-resync' });\n        return true;\n      }\n\n      var buffered = this.tech_.buffered();\n      var nextRange = _ranges2['default'].findNextRange(buffered, currentTime);\n\n      if (this.videoUnderflow_(nextRange, buffered, currentTime)) {\n        // Even though the video underflowed and was stuck in a gap, the audio overplayed\n        // the gap, leading currentTime into a buffered range. Seeking to currentTime\n        // allows the video to catch up to the audio position without losing any audio\n        // (only suffering ~3 seconds of frozen video and a pause in audio playback).\n        this.cancelTimer_();\n        this.tech_.setCurrentTime(currentTime);\n\n        // video underflow may be useful for monitoring QoS\n        this.tech_.trigger({ type: 'usage', name: 'hls-video-underflow' });\n        return true;\n      }\n\n      // check for gap\n      if (nextRange.length > 0) {\n        var difference = nextRange.start(0) - currentTime;\n\n        this.logger_('Stopped at ' + currentTime + ', setting timer for ' + difference + ', seeking ' + ('to ' + nextRange.start(0)));\n\n        this.timer_ = setTimeout(this.skipTheGap_.bind(this), difference * 1000, currentTime);\n        return true;\n      }\n\n      // All checks failed. Returning false to indicate failure to correct waiting\n      return false;\n    }\n  }, {\n    key: 'afterSeekableWindow_',\n    value: function afterSeekableWindow_(seekable, currentTime) {\n      if (!seekable.length) {\n        // we can't make a solid case if there's no seekable, default to false\n        return false;\n      }\n\n      if (currentTime > seekable.end(seekable.length - 1) + _ranges2['default'].SAFE_TIME_DELTA) {\n        return true;\n      }\n\n      return false;\n    }\n  }, {\n    key: 'beforeSeekableWindow_',\n    value: function beforeSeekableWindow_(seekable, currentTime) {\n      if (seekable.length &&\n      // can't fall before 0 and 0 seekable start identifies VOD stream\n      seekable.start(0) > 0 && currentTime < seekable.start(0) - _ranges2['default'].SAFE_TIME_DELTA) {\n        return true;\n      }\n\n      return false;\n    }\n  }, {\n    key: 'videoUnderflow_',\n    value: function videoUnderflow_(nextRange, buffered, currentTime) {\n      if (nextRange.length === 0) {\n        // Even if there is no available next range, there is still a possibility we are\n        // stuck in a gap due to video underflow.\n        var gap = this.gapFromVideoUnderflow_(buffered, currentTime);\n\n        if (gap) {\n          this.logger_('Encountered a gap in video from ' + gap.start + ' to ' + gap.end + '. ' + ('Seeking to current time ' + currentTime));\n\n          return true;\n        }\n      }\n\n      return false;\n    }\n\n    /**\n     * Timer callback. If playback still has not proceeded, then we seek\n     * to the start of the next buffered region.\n     *\n     * @private\n     */\n  }, {\n    key: 'skipTheGap_',\n    value: function skipTheGap_(scheduledCurrentTime) {\n      var buffered = this.tech_.buffered();\n      var currentTime = this.tech_.currentTime();\n      var nextRange = _ranges2['default'].findNextRange(buffered, currentTime);\n\n      this.cancelTimer_();\n\n      if (nextRange.length === 0 || currentTime !== scheduledCurrentTime) {\n        return;\n      }\n\n      this.logger_('skipTheGap_:', 'currentTime:', currentTime, 'scheduled currentTime:', scheduledCurrentTime, 'nextRange start:', nextRange.start(0));\n\n      // only seek if we still have not played\n      this.tech_.setCurrentTime(nextRange.start(0) + _ranges2['default'].TIME_FUDGE_FACTOR);\n\n      this.tech_.trigger({ type: 'usage', name: 'hls-gap-skip' });\n    }\n  }, {\n    key: 'gapFromVideoUnderflow_',\n    value: function gapFromVideoUnderflow_(buffered, currentTime) {\n      // At least in Chrome, if there is a gap in the video buffer, the audio will continue\n      // playing for ~3 seconds after the video gap starts. This is done to account for\n      // video buffer underflow/underrun (note that this is not done when there is audio\n      // buffer underflow/underrun -- in that case the video will stop as soon as it\n      // encounters the gap, as audio stalls are more noticeable/jarring to a user than\n      // video stalls). The player's time will reflect the playthrough of audio, so the\n      // time will appear as if we are in a buffered region, even if we are stuck in a\n      // \"gap.\"\n      //\n      // Example:\n      // video buffer:   0 => 10.1, 10.2 => 20\n      // audio buffer:   0 => 20\n      // overall buffer: 0 => 10.1, 10.2 => 20\n      // current time: 13\n      //\n      // Chrome's video froze at 10 seconds, where the video buffer encountered the gap,\n      // however, the audio continued playing until it reached ~3 seconds past the gap\n      // (13 seconds), at which point it stops as well. Since current time is past the\n      // gap, findNextRange will return no ranges.\n      //\n      // To check for this issue, we see if there is a gap that starts somewhere within\n      // a 3 second range (3 seconds +/- 1 second) back from our current time.\n      var gaps = _ranges2['default'].findGaps(buffered);\n\n      for (var i = 0; i < gaps.length; i++) {\n        var start = gaps.start(i);\n        var end = gaps.end(i);\n\n        // gap is starts no more than 4 seconds back\n        if (currentTime - start < 4 && currentTime - start > 2) {\n          return {\n            start: start,\n            end: end\n          };\n        }\n      }\n\n      return null;\n    }\n\n    /**\n     * A debugging logger noop that is set to console.log only if debugging\n     * is enabled globally\n     *\n     * @private\n     */\n  }, {\n    key: 'logger_',\n    value: function logger_() {}\n  }]);\n\n  return PlaybackWatcher;\n})();\n\nexports['default'] = PlaybackWatcher;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/playback-watcher.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/playlist-loader.js":
/*!*****************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/playlist-loader.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @module playlist-loader\n *\n * @file A state machine that manages the loading, caching, and updating of\n * M3U8 playlists.\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _resolveUrl = __webpack_require__(/*! ./resolve-url */ \"./node_modules/videojs-contrib-hls/es5/resolve-url.js\");\n\nvar _resolveUrl2 = _interopRequireDefault(_resolveUrl);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _m3u8Parser = __webpack_require__(/*! m3u8-parser */ \"./node_modules/m3u8-parser/es5/index.js\");\n\nvar _m3u8Parser2 = _interopRequireDefault(_m3u8Parser);\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\n/**\n * Returns a new array of segments that is the result of merging\n * properties from an older list of segments onto an updated\n * list. No properties on the updated playlist will be overridden.\n *\n * @param {Array} original the outdated list of segments\n * @param {Array} update the updated list of segments\n * @param {Number=} offset the index of the first update\n * segment in the original segment list. For non-live playlists,\n * this should always be zero and does not need to be\n * specified. For live playlists, it should be the difference\n * between the media sequence numbers in the original and updated\n * playlists.\n * @return a list of merged segment objects\n */\nvar updateSegments = function updateSegments(original, update, offset) {\n  var result = update.slice();\n\n  offset = offset || 0;\n  var length = Math.min(original.length, update.length + offset);\n\n  for (var i = offset; i < length; i++) {\n    result[i - offset] = (0, _videoJs.mergeOptions)(original[i], result[i - offset]);\n  }\n  return result;\n};\n\nexports.updateSegments = updateSegments;\nvar resolveSegmentUris = function resolveSegmentUris(segment, baseUri) {\n  if (!segment.resolvedUri) {\n    segment.resolvedUri = (0, _resolveUrl2['default'])(baseUri, segment.uri);\n  }\n  if (segment.key && !segment.key.resolvedUri) {\n    segment.key.resolvedUri = (0, _resolveUrl2['default'])(baseUri, segment.key.uri);\n  }\n  if (segment.map && !segment.map.resolvedUri) {\n    segment.map.resolvedUri = (0, _resolveUrl2['default'])(baseUri, segment.map.uri);\n  }\n};\n\nexports.resolveSegmentUris = resolveSegmentUris;\n/**\n  * Returns a new master playlist that is the result of merging an\n  * updated media playlist into the original version. If the\n  * updated media playlist does not match any of the playlist\n  * entries in the original master playlist, null is returned.\n  *\n  * @param {Object} master a parsed master M3U8 object\n  * @param {Object} media a parsed media M3U8 object\n  * @return {Object} a new object that represents the original\n  * master playlist with the updated media playlist merged in, or\n  * null if the merge produced no change.\n  */\nvar updateMaster = function updateMaster(master, media) {\n  var result = (0, _videoJs.mergeOptions)(master, {});\n  var playlist = result.playlists.filter(function (p) {\n    return p.uri === media.uri;\n  })[0];\n\n  if (!playlist) {\n    return null;\n  }\n\n  // consider the playlist unchanged if the number of segments is equal and the media\n  // sequence number is unchanged\n  if (playlist.segments && media.segments && playlist.segments.length === media.segments.length && playlist.mediaSequence === media.mediaSequence) {\n    return null;\n  }\n\n  var mergedPlaylist = (0, _videoJs.mergeOptions)(playlist, media);\n\n  // if the update could overlap existing segment information, merge the two segment lists\n  if (playlist.segments) {\n    mergedPlaylist.segments = updateSegments(playlist.segments, media.segments, media.mediaSequence - playlist.mediaSequence);\n  }\n\n  // resolve any segment URIs to prevent us from having to do it later\n  mergedPlaylist.segments.forEach(function (segment) {\n    resolveSegmentUris(segment, mergedPlaylist.resolvedUri);\n  });\n\n  // TODO Right now in the playlists array there are two references to each playlist, one\n  // that is referenced by index, and one by URI. The index reference may no longer be\n  // necessary.\n  for (var i = 0; i < result.playlists.length; i++) {\n    if (result.playlists[i].uri === media.uri) {\n      result.playlists[i] = mergedPlaylist;\n    }\n  }\n  result.playlists[media.uri] = mergedPlaylist;\n\n  return result;\n};\n\nexports.updateMaster = updateMaster;\nvar setupMediaPlaylists = function setupMediaPlaylists(master) {\n  // setup by-URI lookups and resolve media playlist URIs\n  var i = master.playlists.length;\n\n  while (i--) {\n    var playlist = master.playlists[i];\n\n    master.playlists[playlist.uri] = playlist;\n    playlist.resolvedUri = (0, _resolveUrl2['default'])(master.uri, playlist.uri);\n\n    if (!playlist.attributes) {\n      // Although the spec states an #EXT-X-STREAM-INF tag MUST have a\n      // BANDWIDTH attribute, we can play the stream without it. This means a poorly\n      // formatted master playlist may not have an attribute list. An attributes\n      // property is added here to prevent undefined references when we encounter\n      // this scenario.\n      playlist.attributes = {};\n\n      _videoJs.log.warn('Invalid playlist STREAM-INF detected. Missing BANDWIDTH attribute.');\n    }\n  }\n};\n\nexports.setupMediaPlaylists = setupMediaPlaylists;\nvar resolveMediaGroupUris = function resolveMediaGroupUris(master) {\n  ['AUDIO', 'SUBTITLES'].forEach(function (mediaType) {\n    for (var groupKey in master.mediaGroups[mediaType]) {\n      for (var labelKey in master.mediaGroups[mediaType][groupKey]) {\n        var mediaProperties = master.mediaGroups[mediaType][groupKey][labelKey];\n\n        if (mediaProperties.uri) {\n          mediaProperties.resolvedUri = (0, _resolveUrl2['default'])(master.uri, mediaProperties.uri);\n        }\n      }\n    }\n  });\n};\n\nexports.resolveMediaGroupUris = resolveMediaGroupUris;\n/**\n * Calculates the time to wait before refreshing a live playlist\n *\n * @param {Object} media\n *        The current media\n * @param {Boolean} update\n *        True if there were any updates from the last refresh, false otherwise\n * @return {Number}\n *         The time in ms to wait before refreshing the live playlist\n */\nvar refreshDelay = function refreshDelay(media, update) {\n  var lastSegment = media.segments[media.segments.length - 1];\n  var delay = undefined;\n\n  if (update && lastSegment && lastSegment.duration) {\n    delay = lastSegment.duration * 1000;\n  } else {\n    // if the playlist is unchanged since the last reload or last segment duration\n    // cannot be determined, try again after half the target duration\n    delay = (media.targetDuration || 10) * 500;\n  }\n  return delay;\n};\n\nexports.refreshDelay = refreshDelay;\n/**\n * Load a playlist from a remote location\n *\n * @class PlaylistLoader\n * @extends videojs.EventTarget\n * @param {String} srcUrl the url to start with\n * @param {Object} hls\n * @param {Object} [options]\n * @param {Boolean} [options.withCredentials=false] the withCredentials xhr option\n * @param {Boolean} [options.handleManifestRedirects=false] whether to follow redirects, when any\n *        playlist request was redirected\n */\n\nvar PlaylistLoader = (function (_EventTarget) {\n  _inherits(PlaylistLoader, _EventTarget);\n\n  function PlaylistLoader(srcUrl, hls, options) {\n    var _this = this;\n\n    _classCallCheck(this, PlaylistLoader);\n\n    _get(Object.getPrototypeOf(PlaylistLoader.prototype), 'constructor', this).call(this);\n\n    options = options || {};\n\n    this.srcUrl = srcUrl;\n    this.hls_ = hls;\n    this.withCredentials = !!options.withCredentials;\n    this.handleManifestRedirects = !!options.handleManifestRedirects;\n\n    if (!this.srcUrl) {\n      throw new Error('A non-empty playlist URL is required');\n    }\n\n    // initialize the loader state\n    this.state = 'HAVE_NOTHING';\n\n    // live playlist staleness timeout\n    this.on('mediaupdatetimeout', function () {\n      if (_this.state !== 'HAVE_METADATA') {\n        // only refresh the media playlist if no other activity is going on\n        return;\n      }\n\n      _this.state = 'HAVE_CURRENT_METADATA';\n\n      _this.request = _this.hls_.xhr({\n        uri: (0, _resolveUrl2['default'])(_this.master.uri, _this.media().uri),\n        withCredentials: _this.withCredentials\n      }, function (error, req) {\n        // disposed\n        if (!_this.request) {\n          return;\n        }\n\n        if (error) {\n          return _this.playlistRequestError(_this.request, _this.media().uri, 'HAVE_METADATA');\n        }\n\n        _this.haveMetadata(_this.request, _this.media().uri);\n      });\n    });\n  }\n\n  _createClass(PlaylistLoader, [{\n    key: 'playlistRequestError',\n    value: function playlistRequestError(xhr, url, startingState) {\n      // any in-flight request is now finished\n      this.request = null;\n\n      if (startingState) {\n        this.state = startingState;\n      }\n\n      this.error = {\n        playlist: this.master.playlists[url],\n        status: xhr.status,\n        message: 'HLS playlist request error at URL: ' + url,\n        responseText: xhr.responseText,\n        code: xhr.status >= 500 ? 4 : 2\n      };\n\n      this.trigger('error');\n    }\n\n    // update the playlist loader's state in response to a new or\n    // updated playlist.\n  }, {\n    key: 'haveMetadata',\n    value: function haveMetadata(xhr, url) {\n      var _this2 = this;\n\n      // any in-flight request is now finished\n      this.request = null;\n      this.state = 'HAVE_METADATA';\n\n      var parser = new _m3u8Parser2['default'].Parser();\n\n      parser.push(xhr.responseText);\n      parser.end();\n      parser.manifest.uri = url;\n      // m3u8-parser does not attach an attributes property to media playlists so make\n      // sure that the property is attached to avoid undefined reference errors\n      parser.manifest.attributes = parser.manifest.attributes || {};\n\n      // merge this playlist into the master\n      var update = updateMaster(this.master, parser.manifest);\n\n      this.targetDuration = parser.manifest.targetDuration;\n\n      if (update) {\n        this.master = update;\n        this.media_ = this.master.playlists[parser.manifest.uri];\n      } else {\n        this.trigger('playlistunchanged');\n      }\n\n      // refresh live playlists after a target duration passes\n      if (!this.media().endList) {\n        _globalWindow2['default'].clearTimeout(this.mediaUpdateTimeout);\n        this.mediaUpdateTimeout = _globalWindow2['default'].setTimeout(function () {\n          _this2.trigger('mediaupdatetimeout');\n        }, refreshDelay(this.media(), !!update));\n      }\n\n      this.trigger('loadedplaylist');\n    }\n\n    /**\n     * Abort any outstanding work and clean up.\n     */\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      this.stopRequest();\n      _globalWindow2['default'].clearTimeout(this.mediaUpdateTimeout);\n    }\n  }, {\n    key: 'stopRequest',\n    value: function stopRequest() {\n      if (this.request) {\n        var oldRequest = this.request;\n\n        this.request = null;\n        oldRequest.onreadystatechange = null;\n        oldRequest.abort();\n      }\n    }\n\n    /**\n     * When called without any arguments, returns the currently\n     * active media playlist. When called with a single argument,\n     * triggers the playlist loader to asynchronously switch to the\n     * specified media playlist. Calling this method while the\n     * loader is in the HAVE_NOTHING causes an error to be emitted\n     * but otherwise has no effect.\n     *\n     * @param {Object=} playlist the parsed media playlist\n     * object to switch to\n     * @return {Playlist} the current loaded media\n     */\n  }, {\n    key: 'media',\n    value: function media(playlist) {\n      var _this3 = this;\n\n      // getter\n      if (!playlist) {\n        return this.media_;\n      }\n\n      // setter\n      if (this.state === 'HAVE_NOTHING') {\n        throw new Error('Cannot switch media playlist from ' + this.state);\n      }\n\n      var startingState = this.state;\n\n      // find the playlist object if the target playlist has been\n      // specified by URI\n      if (typeof playlist === 'string') {\n        if (!this.master.playlists[playlist]) {\n          throw new Error('Unknown playlist URI: ' + playlist);\n        }\n        playlist = this.master.playlists[playlist];\n      }\n\n      var mediaChange = !this.media_ || playlist.uri !== this.media_.uri;\n\n      // switch to fully loaded playlists immediately\n      if (this.master.playlists[playlist.uri].endList) {\n        // abort outstanding playlist requests\n        if (this.request) {\n          this.request.onreadystatechange = null;\n          this.request.abort();\n          this.request = null;\n        }\n        this.state = 'HAVE_METADATA';\n        this.media_ = playlist;\n\n        // trigger media change if the active media has been updated\n        if (mediaChange) {\n          this.trigger('mediachanging');\n          this.trigger('mediachange');\n        }\n        return;\n      }\n\n      // switching to the active playlist is a no-op\n      if (!mediaChange) {\n        return;\n      }\n\n      this.state = 'SWITCHING_MEDIA';\n\n      // there is already an outstanding playlist request\n      if (this.request) {\n        if (playlist.resolvedUri === this.request.url) {\n          // requesting to switch to the same playlist multiple times\n          // has no effect after the first\n          return;\n        }\n        this.request.onreadystatechange = null;\n        this.request.abort();\n        this.request = null;\n      }\n\n      // request the new playlist\n      if (this.media_) {\n        this.trigger('mediachanging');\n      }\n\n      this.request = this.hls_.xhr({\n        uri: playlist.resolvedUri,\n        withCredentials: this.withCredentials\n      }, function (error, req) {\n        // disposed\n        if (!_this3.request) {\n          return;\n        }\n\n        playlist.resolvedUri = _this3.resolveManifestRedirect(playlist.resolvedUri, req);\n\n        if (error) {\n          return _this3.playlistRequestError(_this3.request, playlist.uri, startingState);\n        }\n\n        _this3.haveMetadata(req, playlist.uri);\n\n        // fire loadedmetadata the first time a media playlist is loaded\n        if (startingState === 'HAVE_MASTER') {\n          _this3.trigger('loadedmetadata');\n        } else {\n          _this3.trigger('mediachange');\n        }\n      });\n    }\n\n    /**\n     * Checks whether xhr request was redirected and returns correct url depending\n     * on `handleManifestRedirects` option\n     *\n     * @api private\n     *\n     * @param  {String} url - an url being requested\n     * @param  {XMLHttpRequest} req - xhr request result\n     *\n     * @return {String}\n     */\n  }, {\n    key: 'resolveManifestRedirect',\n    value: function resolveManifestRedirect(url, req) {\n      if (this.handleManifestRedirects && req.responseURL && url !== req.responseURL) {\n        return req.responseURL;\n      }\n\n      return url;\n    }\n\n    /**\n     * pause loading of the playlist\n     */\n  }, {\n    key: 'pause',\n    value: function pause() {\n      this.stopRequest();\n      _globalWindow2['default'].clearTimeout(this.mediaUpdateTimeout);\n      if (this.state === 'HAVE_NOTHING') {\n        // If we pause the loader before any data has been retrieved, its as if we never\n        // started, so reset to an unstarted state.\n        this.started = false;\n      }\n      // Need to restore state now that no activity is happening\n      if (this.state === 'SWITCHING_MEDIA') {\n        // if the loader was in the process of switching media, it should either return to\n        // HAVE_MASTER or HAVE_METADATA depending on if the loader has loaded a media\n        // playlist yet. This is determined by the existence of loader.media_\n        if (this.media_) {\n          this.state = 'HAVE_METADATA';\n        } else {\n          this.state = 'HAVE_MASTER';\n        }\n      } else if (this.state === 'HAVE_CURRENT_METADATA') {\n        this.state = 'HAVE_METADATA';\n      }\n    }\n\n    /**\n     * start loading of the playlist\n     */\n  }, {\n    key: 'load',\n    value: function load(isFinalRendition) {\n      var _this4 = this;\n\n      _globalWindow2['default'].clearTimeout(this.mediaUpdateTimeout);\n\n      var media = this.media();\n\n      if (isFinalRendition) {\n        var delay = media ? media.targetDuration / 2 * 1000 : 5 * 1000;\n\n        this.mediaUpdateTimeout = _globalWindow2['default'].setTimeout(function () {\n          return _this4.load();\n        }, delay);\n        return;\n      }\n\n      if (!this.started) {\n        this.start();\n        return;\n      }\n\n      if (media && !media.endList) {\n        this.trigger('mediaupdatetimeout');\n      } else {\n        this.trigger('loadedplaylist');\n      }\n    }\n\n    /**\n     * start loading of the playlist\n     */\n  }, {\n    key: 'start',\n    value: function start() {\n      var _this5 = this;\n\n      this.started = true;\n\n      // request the specified URL\n      this.request = this.hls_.xhr({\n        uri: this.srcUrl,\n        withCredentials: this.withCredentials\n      }, function (error, req) {\n        // disposed\n        if (!_this5.request) {\n          return;\n        }\n\n        // clear the loader's request reference\n        _this5.request = null;\n\n        if (error) {\n          _this5.error = {\n            status: req.status,\n            message: 'HLS playlist request error at URL: ' + _this5.srcUrl,\n            responseText: req.responseText,\n            // MEDIA_ERR_NETWORK\n            code: 2\n          };\n          if (_this5.state === 'HAVE_NOTHING') {\n            _this5.started = false;\n          }\n          return _this5.trigger('error');\n        }\n\n        var parser = new _m3u8Parser2['default'].Parser();\n\n        parser.push(req.responseText);\n        parser.end();\n\n        _this5.state = 'HAVE_MASTER';\n\n        _this5.srcUrl = _this5.resolveManifestRedirect(_this5.srcUrl, req);\n\n        parser.manifest.uri = _this5.srcUrl;\n\n        // loaded a master playlist\n        if (parser.manifest.playlists) {\n          _this5.master = parser.manifest;\n\n          setupMediaPlaylists(_this5.master);\n          resolveMediaGroupUris(_this5.master);\n\n          _this5.trigger('loadedplaylist');\n          if (!_this5.request) {\n            // no media playlist was specifically selected so start\n            // from the first listed one\n            _this5.media(parser.manifest.playlists[0]);\n          }\n          return;\n        }\n\n        // loaded a media playlist\n        // infer a master playlist if none was previously requested\n        _this5.master = {\n          mediaGroups: {\n            'AUDIO': {},\n            'VIDEO': {},\n            'CLOSED-CAPTIONS': {},\n            'SUBTITLES': {}\n          },\n          uri: _globalWindow2['default'].location.href,\n          playlists: [{\n            uri: _this5.srcUrl,\n            resolvedUri: _this5.srcUrl,\n            // m3u8-parser does not attach an attributes property to media playlists so make\n            // sure that the property is attached to avoid undefined reference errors\n            attributes: {}\n          }]\n        };\n        _this5.master.playlists[_this5.srcUrl] = _this5.master.playlists[0];\n        _this5.haveMetadata(req, _this5.srcUrl);\n        return _this5.trigger('loadedmetadata');\n      });\n    }\n  }]);\n\n  return PlaylistLoader;\n})(_videoJs.EventTarget);\n\nexports['default'] = PlaylistLoader;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/playlist-loader.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/playlist-selectors.js":
/*!********************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/playlist-selectors.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _config = __webpack_require__(/*! ./config */ \"./node_modules/videojs-contrib-hls/es5/config.js\");\n\nvar _config2 = _interopRequireDefault(_config);\n\nvar _playlist = __webpack_require__(/*! ./playlist */ \"./node_modules/videojs-contrib-hls/es5/playlist.js\");\n\nvar _playlist2 = _interopRequireDefault(_playlist);\n\nvar _utilCodecsJs = __webpack_require__(/*! ./util/codecs.js */ \"./node_modules/videojs-contrib-hls/es5/util/codecs.js\");\n\n// Utilities\n\n/**\n * Returns the CSS value for the specified property on an element\n * using `getComputedStyle`. Firefox has a long-standing issue where\n * getComputedStyle() may return null when running in an iframe with\n * `display: none`.\n *\n * @see https://bugzilla.mozilla.org/show_bug.cgi?id=548397\n * @param {HTMLElement} el the htmlelement to work on\n * @param {string} the proprety to get the style for\n */\nvar safeGetComputedStyle = function safeGetComputedStyle(el, property) {\n  var result = undefined;\n\n  if (!el) {\n    return '';\n  }\n\n  result = window.getComputedStyle(el);\n  if (!result) {\n    return '';\n  }\n\n  return result[property];\n};\n\n/**\n * Resuable stable sort function\n *\n * @param {Playlists} array\n * @param {Function} sortFn Different comparators\n * @function stableSort\n */\nvar stableSort = function stableSort(array, sortFn) {\n  var newArray = array.slice();\n\n  array.sort(function (left, right) {\n    var cmp = sortFn(left, right);\n\n    if (cmp === 0) {\n      return newArray.indexOf(left) - newArray.indexOf(right);\n    }\n    return cmp;\n  });\n};\n\n/**\n * A comparator function to sort two playlist object by bandwidth.\n *\n * @param {Object} left a media playlist object\n * @param {Object} right a media playlist object\n * @return {Number} Greater than zero if the bandwidth attribute of\n * left is greater than the corresponding attribute of right. Less\n * than zero if the bandwidth of right is greater than left and\n * exactly zero if the two are equal.\n */\nvar comparePlaylistBandwidth = function comparePlaylistBandwidth(left, right) {\n  var leftBandwidth = undefined;\n  var rightBandwidth = undefined;\n\n  if (left.attributes.BANDWIDTH) {\n    leftBandwidth = left.attributes.BANDWIDTH;\n  }\n  leftBandwidth = leftBandwidth || window.Number.MAX_VALUE;\n  if (right.attributes.BANDWIDTH) {\n    rightBandwidth = right.attributes.BANDWIDTH;\n  }\n  rightBandwidth = rightBandwidth || window.Number.MAX_VALUE;\n\n  return leftBandwidth - rightBandwidth;\n};\n\nexports.comparePlaylistBandwidth = comparePlaylistBandwidth;\n/**\n * A comparator function to sort two playlist object by resolution (width).\n * @param {Object} left a media playlist object\n * @param {Object} right a media playlist object\n * @return {Number} Greater than zero if the resolution.width attribute of\n * left is greater than the corresponding attribute of right. Less\n * than zero if the resolution.width of right is greater than left and\n * exactly zero if the two are equal.\n */\nvar comparePlaylistResolution = function comparePlaylistResolution(left, right) {\n  var leftWidth = undefined;\n  var rightWidth = undefined;\n\n  if (left.attributes.RESOLUTION && left.attributes.RESOLUTION.width) {\n    leftWidth = left.attributes.RESOLUTION.width;\n  }\n\n  leftWidth = leftWidth || window.Number.MAX_VALUE;\n\n  if (right.attributes.RESOLUTION && right.attributes.RESOLUTION.width) {\n    rightWidth = right.attributes.RESOLUTION.width;\n  }\n\n  rightWidth = rightWidth || window.Number.MAX_VALUE;\n\n  // NOTE - Fallback to bandwidth sort as appropriate in cases where multiple renditions\n  // have the same media dimensions/ resolution\n  if (leftWidth === rightWidth && left.attributes.BANDWIDTH && right.attributes.BANDWIDTH) {\n    return left.attributes.BANDWIDTH - right.attributes.BANDWIDTH;\n  }\n  return leftWidth - rightWidth;\n};\n\nexports.comparePlaylistResolution = comparePlaylistResolution;\n/**\n * Chooses the appropriate media playlist based on bandwidth and player size\n *\n * @param {Object} master\n *        Object representation of the master manifest\n * @param {Number} playerBandwidth\n *        Current calculated bandwidth of the player\n * @param {Number} playerWidth\n *        Current width of the player element\n * @param {Number} playerHeight\n *        Current height of the player element\n * @return {Playlist} the highest bitrate playlist less than the\n * currently detected bandwidth, accounting for some amount of\n * bandwidth variance\n */\nvar simpleSelector = function simpleSelector(master, playerBandwidth, playerWidth, playerHeight) {\n  // convert the playlists to an intermediary representation to make comparisons easier\n  var sortedPlaylistReps = master.playlists.map(function (playlist) {\n    var width = undefined;\n    var height = undefined;\n    var bandwidth = undefined;\n\n    width = playlist.attributes.RESOLUTION && playlist.attributes.RESOLUTION.width;\n    height = playlist.attributes.RESOLUTION && playlist.attributes.RESOLUTION.height;\n    bandwidth = playlist.attributes.BANDWIDTH;\n\n    bandwidth = bandwidth || window.Number.MAX_VALUE;\n\n    return {\n      bandwidth: bandwidth,\n      width: width,\n      height: height,\n      playlist: playlist\n    };\n  });\n\n  stableSort(sortedPlaylistReps, function (left, right) {\n    return left.bandwidth - right.bandwidth;\n  });\n\n  // filter out any playlists that have been excluded due to\n  // incompatible configurations\n  sortedPlaylistReps = sortedPlaylistReps.filter(function (rep) {\n    return !_playlist2['default'].isIncompatible(rep.playlist);\n  });\n\n  // filter out any playlists that have been disabled manually through the representations\n  // api or blacklisted temporarily due to playback errors.\n  var enabledPlaylistReps = sortedPlaylistReps.filter(function (rep) {\n    return _playlist2['default'].isEnabled(rep.playlist);\n  });\n\n  if (!enabledPlaylistReps.length) {\n    // if there are no enabled playlists, then they have all been blacklisted or disabled\n    // by the user through the representations api. In this case, ignore blacklisting and\n    // fallback to what the user wants by using playlists the user has not disabled.\n    enabledPlaylistReps = sortedPlaylistReps.filter(function (rep) {\n      return !_playlist2['default'].isDisabled(rep.playlist);\n    });\n  }\n\n  // filter out any variant that has greater effective bitrate\n  // than the current estimated bandwidth\n  var bandwidthPlaylistReps = enabledPlaylistReps.filter(function (rep) {\n    return rep.bandwidth * _config2['default'].BANDWIDTH_VARIANCE < playerBandwidth;\n  });\n\n  var highestRemainingBandwidthRep = bandwidthPlaylistReps[bandwidthPlaylistReps.length - 1];\n\n  // get all of the renditions with the same (highest) bandwidth\n  // and then taking the very first element\n  var bandwidthBestRep = bandwidthPlaylistReps.filter(function (rep) {\n    return rep.bandwidth === highestRemainingBandwidthRep.bandwidth;\n  })[0];\n\n  // filter out playlists without resolution information\n  var haveResolution = bandwidthPlaylistReps.filter(function (rep) {\n    return rep.width && rep.height;\n  });\n\n  // sort variants by resolution\n  stableSort(haveResolution, function (left, right) {\n    return left.width - right.width;\n  });\n\n  // if we have the exact resolution as the player use it\n  var resolutionBestRepList = haveResolution.filter(function (rep) {\n    return rep.width === playerWidth && rep.height === playerHeight;\n  });\n\n  highestRemainingBandwidthRep = resolutionBestRepList[resolutionBestRepList.length - 1];\n  // ensure that we pick the highest bandwidth variant that have exact resolution\n  var resolutionBestRep = resolutionBestRepList.filter(function (rep) {\n    return rep.bandwidth === highestRemainingBandwidthRep.bandwidth;\n  })[0];\n\n  var resolutionPlusOneList = undefined;\n  var resolutionPlusOneSmallest = undefined;\n  var resolutionPlusOneRep = undefined;\n\n  // find the smallest variant that is larger than the player\n  // if there is no match of exact resolution\n  if (!resolutionBestRep) {\n    resolutionPlusOneList = haveResolution.filter(function (rep) {\n      return rep.width > playerWidth || rep.height > playerHeight;\n    });\n\n    // find all the variants have the same smallest resolution\n    resolutionPlusOneSmallest = resolutionPlusOneList.filter(function (rep) {\n      return rep.width === resolutionPlusOneList[0].width && rep.height === resolutionPlusOneList[0].height;\n    });\n\n    // ensure that we also pick the highest bandwidth variant that\n    // is just-larger-than the video player\n    highestRemainingBandwidthRep = resolutionPlusOneSmallest[resolutionPlusOneSmallest.length - 1];\n    resolutionPlusOneRep = resolutionPlusOneSmallest.filter(function (rep) {\n      return rep.bandwidth === highestRemainingBandwidthRep.bandwidth;\n    })[0];\n  }\n\n  // fallback chain of variants\n  var chosenRep = resolutionPlusOneRep || resolutionBestRep || bandwidthBestRep || enabledPlaylistReps[0] || sortedPlaylistReps[0];\n\n  return chosenRep ? chosenRep.playlist : null;\n};\n\nexports.simpleSelector = simpleSelector;\n// Playlist Selectors\n\n/**\n * Chooses the appropriate media playlist based on the most recent\n * bandwidth estimate and the player size.\n *\n * Expects to be called within the context of an instance of HlsHandler\n *\n * @return {Playlist} the highest bitrate playlist less than the\n * currently detected bandwidth, accounting for some amount of\n * bandwidth variance\n */\nvar lastBandwidthSelector = function lastBandwidthSelector() {\n  return simpleSelector(this.playlists.master, this.systemBandwidth, parseInt(safeGetComputedStyle(this.tech_.el(), 'width'), 10), parseInt(safeGetComputedStyle(this.tech_.el(), 'height'), 10));\n};\n\nexports.lastBandwidthSelector = lastBandwidthSelector;\n/**\n * Chooses the appropriate media playlist based on an\n * exponential-weighted moving average of the bandwidth after\n * filtering for player size.\n *\n * Expects to be called within the context of an instance of HlsHandler\n *\n * @param {Number} decay - a number between 0 and 1. Higher values of\n * this parameter will cause previous bandwidth estimates to lose\n * significance more quickly.\n * @return {Function} a function which can be invoked to create a new\n * playlist selector function.\n * @see https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n */\nvar movingAverageBandwidthSelector = function movingAverageBandwidthSelector(decay) {\n  var average = -1;\n\n  if (decay < 0 || decay > 1) {\n    throw new Error('Moving average bandwidth decay must be between 0 and 1.');\n  }\n\n  return function () {\n    if (average < 0) {\n      average = this.systemBandwidth;\n    }\n\n    average = decay * this.systemBandwidth + (1 - decay) * average;\n    return simpleSelector(this.playlists.master, average, parseInt(safeGetComputedStyle(this.tech_.el(), 'width'), 10), parseInt(safeGetComputedStyle(this.tech_.el(), 'height'), 10));\n  };\n};\n\nexports.movingAverageBandwidthSelector = movingAverageBandwidthSelector;\n/**\n * Chooses the appropriate media playlist based on the potential to rebuffer\n *\n * @param {Object} settings\n *        Object of information required to use this selector\n * @param {Object} settings.master\n *        Object representation of the master manifest\n * @param {Number} settings.currentTime\n *        The current time of the player\n * @param {Number} settings.bandwidth\n *        Current measured bandwidth\n * @param {Number} settings.duration\n *        Duration of the media\n * @param {Number} settings.segmentDuration\n *        Segment duration to be used in round trip time calculations\n * @param {Number} settings.timeUntilRebuffer\n *        Time left in seconds until the player has to rebuffer\n * @param {Number} settings.currentTimeline\n *        The current timeline segments are being loaded from\n * @param {SyncController} settings.syncController\n *        SyncController for determining if we have a sync point for a given playlist\n * @return {Object|null}\n *         {Object} return.playlist\n *         The highest bandwidth playlist with the least amount of rebuffering\n *         {Number} return.rebufferingImpact\n *         The amount of time in seconds switching to this playlist will rebuffer. A\n *         negative value means that switching will cause zero rebuffering.\n */\nvar minRebufferMaxBandwidthSelector = function minRebufferMaxBandwidthSelector(settings) {\n  var master = settings.master;\n  var currentTime = settings.currentTime;\n  var bandwidth = settings.bandwidth;\n  var duration = settings.duration;\n  var segmentDuration = settings.segmentDuration;\n  var timeUntilRebuffer = settings.timeUntilRebuffer;\n  var currentTimeline = settings.currentTimeline;\n  var syncController = settings.syncController;\n\n  // filter out any playlists that have been excluded due to\n  // incompatible configurations\n  var compatiblePlaylists = master.playlists.filter(function (playlist) {\n    return !_playlist2['default'].isIncompatible(playlist);\n  });\n\n  // filter out any playlists that have been disabled manually through the representations\n  // api or blacklisted temporarily due to playback errors.\n  var enabledPlaylists = compatiblePlaylists.filter(_playlist2['default'].isEnabled);\n\n  if (!enabledPlaylists.length) {\n    // if there are no enabled playlists, then they have all been blacklisted or disabled\n    // by the user through the representations api. In this case, ignore blacklisting and\n    // fallback to what the user wants by using playlists the user has not disabled.\n    enabledPlaylists = compatiblePlaylists.filter(function (playlist) {\n      return !_playlist2['default'].isDisabled(playlist);\n    });\n  }\n\n  var bandwidthPlaylists = enabledPlaylists.filter(_playlist2['default'].hasAttribute.bind(null, 'BANDWIDTH'));\n\n  var rebufferingEstimates = bandwidthPlaylists.map(function (playlist) {\n    var syncPoint = syncController.getSyncPoint(playlist, duration, currentTimeline, currentTime);\n    // If there is no sync point for this playlist, switching to it will require a\n    // sync request first. This will double the request time\n    var numRequests = syncPoint ? 1 : 2;\n    var requestTimeEstimate = _playlist2['default'].estimateSegmentRequestTime(segmentDuration, bandwidth, playlist);\n    var rebufferingImpact = requestTimeEstimate * numRequests - timeUntilRebuffer;\n\n    return {\n      playlist: playlist,\n      rebufferingImpact: rebufferingImpact\n    };\n  });\n\n  var noRebufferingPlaylists = rebufferingEstimates.filter(function (estimate) {\n    return estimate.rebufferingImpact <= 0;\n  });\n\n  // Sort by bandwidth DESC\n  stableSort(noRebufferingPlaylists, function (a, b) {\n    return comparePlaylistBandwidth(b.playlist, a.playlist);\n  });\n\n  if (noRebufferingPlaylists.length) {\n    return noRebufferingPlaylists[0];\n  }\n\n  stableSort(rebufferingEstimates, function (a, b) {\n    return a.rebufferingImpact - b.rebufferingImpact;\n  });\n\n  return rebufferingEstimates[0] || null;\n};\n\nexports.minRebufferMaxBandwidthSelector = minRebufferMaxBandwidthSelector;\n/**\n * Chooses the appropriate media playlist, which in this case is the lowest bitrate\n * one with video.  If no renditions with video exist, return the lowest audio rendition.\n *\n * Expects to be called within the context of an instance of HlsHandler\n *\n * @return {Object|null}\n *         {Object} return.playlist\n *         The lowest bitrate playlist that contains a video codec.  If no such rendition\n *         exists pick the lowest audio rendition.\n */\nvar lowestBitrateCompatibleVariantSelector = function lowestBitrateCompatibleVariantSelector() {\n  // filter out any playlists that have been excluded due to\n  // incompatible configurations or playback errors\n  var playlists = this.playlists.master.playlists.filter(_playlist2['default'].isEnabled);\n\n  // Sort ascending by bitrate\n  stableSort(playlists, function (a, b) {\n    return comparePlaylistBandwidth(a, b);\n  });\n\n  // Parse and assume that playlists with no video codec have no video\n  // (this is not necessarily true, although it is generally true).\n  //\n  // If an entire manifest has no valid videos everything will get filtered\n  // out.\n  var playlistsWithVideo = playlists.filter(function (playlist) {\n    return (0, _utilCodecsJs.parseCodecs)(playlist.attributes.CODECS).videoCodec;\n  });\n\n  return playlistsWithVideo[0] || null;\n};\nexports.lowestBitrateCompatibleVariantSelector = lowestBitrateCompatibleVariantSelector;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/playlist-selectors.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/playlist.js":
/*!**********************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/playlist.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file playlist.js\n *\n * Playlist related utilities.\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\n/**\n * walk backward until we find a duration we can use\n * or return a failure\n *\n * @param {Playlist} playlist the playlist to walk through\n * @param {Number} endSequence the mediaSequence to stop walking on\n */\n\nvar backwardDuration = function backwardDuration(playlist, endSequence) {\n  var result = 0;\n  var i = endSequence - playlist.mediaSequence;\n  // if a start time is available for segment immediately following\n  // the interval, use it\n  var segment = playlist.segments[i];\n\n  // Walk backward until we find the latest segment with timeline\n  // information that is earlier than endSequence\n  if (segment) {\n    if (typeof segment.start !== 'undefined') {\n      return { result: segment.start, precise: true };\n    }\n    if (typeof segment.end !== 'undefined') {\n      return {\n        result: segment.end - segment.duration,\n        precise: true\n      };\n    }\n  }\n  while (i--) {\n    segment = playlist.segments[i];\n    if (typeof segment.end !== 'undefined') {\n      return { result: result + segment.end, precise: true };\n    }\n\n    result += segment.duration;\n\n    if (typeof segment.start !== 'undefined') {\n      return { result: result + segment.start, precise: true };\n    }\n  }\n  return { result: result, precise: false };\n};\n\n/**\n * walk forward until we find a duration we can use\n * or return a failure\n *\n * @param {Playlist} playlist the playlist to walk through\n * @param {Number} endSequence the mediaSequence to stop walking on\n */\nvar forwardDuration = function forwardDuration(playlist, endSequence) {\n  var result = 0;\n  var segment = undefined;\n  var i = endSequence - playlist.mediaSequence;\n  // Walk forward until we find the earliest segment with timeline\n  // information\n\n  for (; i < playlist.segments.length; i++) {\n    segment = playlist.segments[i];\n    if (typeof segment.start !== 'undefined') {\n      return {\n        result: segment.start - result,\n        precise: true\n      };\n    }\n\n    result += segment.duration;\n\n    if (typeof segment.end !== 'undefined') {\n      return {\n        result: segment.end - result,\n        precise: true\n      };\n    }\n  }\n  // indicate we didn't find a useful duration estimate\n  return { result: -1, precise: false };\n};\n\n/**\n  * Calculate the media duration from the segments associated with a\n  * playlist. The duration of a subinterval of the available segments\n  * may be calculated by specifying an end index.\n  *\n  * @param {Object} playlist a media playlist object\n  * @param {Number=} endSequence an exclusive upper boundary\n  * for the playlist.  Defaults to playlist length.\n  * @param {Number} expired the amount of time that has dropped\n  * off the front of the playlist in a live scenario\n  * @return {Number} the duration between the first available segment\n  * and end index.\n  */\nvar intervalDuration = function intervalDuration(playlist, endSequence, expired) {\n  var backward = undefined;\n  var forward = undefined;\n\n  if (typeof endSequence === 'undefined') {\n    endSequence = playlist.mediaSequence + playlist.segments.length;\n  }\n\n  if (endSequence < playlist.mediaSequence) {\n    return 0;\n  }\n\n  // do a backward walk to estimate the duration\n  backward = backwardDuration(playlist, endSequence);\n  if (backward.precise) {\n    // if we were able to base our duration estimate on timing\n    // information provided directly from the Media Source, return\n    // it\n    return backward.result;\n  }\n\n  // walk forward to see if a precise duration estimate can be made\n  // that way\n  forward = forwardDuration(playlist, endSequence);\n  if (forward.precise) {\n    // we found a segment that has been buffered and so it's\n    // position is known precisely\n    return forward.result;\n  }\n\n  // return the less-precise, playlist-based duration estimate\n  return backward.result + expired;\n};\n\n/**\n  * Calculates the duration of a playlist. If a start and end index\n  * are specified, the duration will be for the subset of the media\n  * timeline between those two indices. The total duration for live\n  * playlists is always Infinity.\n  *\n  * @param {Object} playlist a media playlist object\n  * @param {Number=} endSequence an exclusive upper\n  * boundary for the playlist. Defaults to the playlist media\n  * sequence number plus its length.\n  * @param {Number=} expired the amount of time that has\n  * dropped off the front of the playlist in a live scenario\n  * @return {Number} the duration between the start index and end\n  * index.\n  */\nvar duration = function duration(playlist, endSequence, expired) {\n  if (!playlist) {\n    return 0;\n  }\n\n  if (typeof expired !== 'number') {\n    expired = 0;\n  }\n\n  // if a slice of the total duration is not requested, use\n  // playlist-level duration indicators when they're present\n  if (typeof endSequence === 'undefined') {\n    // if present, use the duration specified in the playlist\n    if (playlist.totalDuration) {\n      return playlist.totalDuration;\n    }\n\n    // duration should be Infinity for live playlists\n    if (!playlist.endList) {\n      return _globalWindow2['default'].Infinity;\n    }\n  }\n\n  // calculate the total duration based on the segment durations\n  return intervalDuration(playlist, endSequence, expired);\n};\n\nexports.duration = duration;\n/**\n  * Calculate the time between two indexes in the current playlist\n  * neight the start- nor the end-index need to be within the current\n  * playlist in which case, the targetDuration of the playlist is used\n  * to approximate the durations of the segments\n  *\n  * @param {Object} playlist a media playlist object\n  * @param {Number} startIndex\n  * @param {Number} endIndex\n  * @return {Number} the number of seconds between startIndex and endIndex\n  */\nvar sumDurations = function sumDurations(playlist, startIndex, endIndex) {\n  var durations = 0;\n\n  if (startIndex > endIndex) {\n    var _ref = [endIndex, startIndex];\n    startIndex = _ref[0];\n    endIndex = _ref[1];\n  }\n\n  if (startIndex < 0) {\n    for (var i = startIndex; i < Math.min(0, endIndex); i++) {\n      durations += playlist.targetDuration;\n    }\n    startIndex = 0;\n  }\n\n  for (var i = startIndex; i < endIndex; i++) {\n    durations += playlist.segments[i].duration;\n  }\n\n  return durations;\n};\n\nexports.sumDurations = sumDurations;\n/**\n * Determines the media index of the segment corresponding to the safe edge of the live\n * window which is the duration of the last segment plus 2 target durations from the end\n * of the playlist.\n *\n * @param {Object} playlist\n *        a media playlist object\n * @return {Number}\n *         The media index of the segment at the safe live point. 0 if there is no \"safe\"\n *         point.\n * @function safeLiveIndex\n */\nvar safeLiveIndex = function safeLiveIndex(playlist) {\n  if (!playlist.segments.length) {\n    return 0;\n  }\n\n  var i = playlist.segments.length - 1;\n  var distanceFromEnd = playlist.segments[i].duration || playlist.targetDuration;\n  var safeDistance = distanceFromEnd + playlist.targetDuration * 2;\n\n  while (i--) {\n    distanceFromEnd += playlist.segments[i].duration;\n\n    if (distanceFromEnd >= safeDistance) {\n      break;\n    }\n  }\n\n  return Math.max(0, i);\n};\n\nexports.safeLiveIndex = safeLiveIndex;\n/**\n * Calculates the playlist end time\n *\n * @param {Object} playlist a media playlist object\n * @param {Number=} expired the amount of time that has\n *                  dropped off the front of the playlist in a live scenario\n * @param {Boolean|false} useSafeLiveEnd a boolean value indicating whether or not the\n *                        playlist end calculation should consider the safe live end\n *                        (truncate the playlist end by three segments). This is normally\n *                        used for calculating the end of the playlist's seekable range.\n * @returns {Number} the end time of playlist\n * @function playlistEnd\n */\nvar playlistEnd = function playlistEnd(playlist, expired, useSafeLiveEnd) {\n  if (!playlist || !playlist.segments) {\n    return null;\n  }\n  if (playlist.endList) {\n    return duration(playlist);\n  }\n\n  if (expired === null) {\n    return null;\n  }\n\n  expired = expired || 0;\n\n  var endSequence = useSafeLiveEnd ? safeLiveIndex(playlist) : playlist.segments.length;\n\n  return intervalDuration(playlist, playlist.mediaSequence + endSequence, expired);\n};\n\nexports.playlistEnd = playlistEnd;\n/**\n  * Calculates the interval of time that is currently seekable in a\n  * playlist. The returned time ranges are relative to the earliest\n  * moment in the specified playlist that is still available. A full\n  * seekable implementation for live streams would need to offset\n  * these values by the duration of content that has expired from the\n  * stream.\n  *\n  * @param {Object} playlist a media playlist object\n  * dropped off the front of the playlist in a live scenario\n  * @param {Number=} expired the amount of time that has\n  * dropped off the front of the playlist in a live scenario\n  * @return {TimeRanges} the periods of time that are valid targets\n  * for seeking\n  */\nvar seekable = function seekable(playlist, expired) {\n  var useSafeLiveEnd = true;\n  var seekableStart = expired || 0;\n  var seekableEnd = playlistEnd(playlist, expired, useSafeLiveEnd);\n\n  if (seekableEnd === null) {\n    return (0, _videoJs.createTimeRange)();\n  }\n  return (0, _videoJs.createTimeRange)(seekableStart, seekableEnd);\n};\n\nexports.seekable = seekable;\nvar isWholeNumber = function isWholeNumber(num) {\n  return num - Math.floor(num) === 0;\n};\n\nvar roundSignificantDigit = function roundSignificantDigit(increment, num) {\n  // If we have a whole number, just add 1 to it\n  if (isWholeNumber(num)) {\n    return num + increment * 0.1;\n  }\n\n  var numDecimalDigits = num.toString().split('.')[1].length;\n\n  for (var i = 1; i <= numDecimalDigits; i++) {\n    var scale = Math.pow(10, i);\n    var temp = num * scale;\n\n    if (isWholeNumber(temp) || i === numDecimalDigits) {\n      return (temp + increment) / scale;\n    }\n  }\n};\n\nvar ceilLeastSignificantDigit = roundSignificantDigit.bind(null, 1);\nvar floorLeastSignificantDigit = roundSignificantDigit.bind(null, -1);\n\n/**\n * Determine the index and estimated starting time of the segment that\n * contains a specified playback position in a media playlist.\n *\n * @param {Object} playlist the media playlist to query\n * @param {Number} currentTime The number of seconds since the earliest\n * possible position to determine the containing segment for\n * @param {Number} startIndex\n * @param {Number} startTime\n * @return {Object}\n */\nvar getMediaInfoForTime = function getMediaInfoForTime(playlist, currentTime, startIndex, startTime) {\n  var i = undefined;\n  var segment = undefined;\n  var numSegments = playlist.segments.length;\n\n  var time = currentTime - startTime;\n\n  if (time < 0) {\n    // Walk backward from startIndex in the playlist, adding durations\n    // until we find a segment that contains `time` and return it\n    if (startIndex > 0) {\n      for (i = startIndex - 1; i >= 0; i--) {\n        segment = playlist.segments[i];\n        time += floorLeastSignificantDigit(segment.duration);\n        if (time > 0) {\n          return {\n            mediaIndex: i,\n            startTime: startTime - sumDurations(playlist, startIndex, i)\n          };\n        }\n      }\n    }\n    // We were unable to find a good segment within the playlist\n    // so select the first segment\n    return {\n      mediaIndex: 0,\n      startTime: currentTime\n    };\n  }\n\n  // When startIndex is negative, we first walk forward to first segment\n  // adding target durations. If we \"run out of time\" before getting to\n  // the first segment, return the first segment\n  if (startIndex < 0) {\n    for (i = startIndex; i < 0; i++) {\n      time -= playlist.targetDuration;\n      if (time < 0) {\n        return {\n          mediaIndex: 0,\n          startTime: currentTime\n        };\n      }\n    }\n    startIndex = 0;\n  }\n\n  // Walk forward from startIndex in the playlist, subtracting durations\n  // until we find a segment that contains `time` and return it\n  for (i = startIndex; i < numSegments; i++) {\n    segment = playlist.segments[i];\n    time -= ceilLeastSignificantDigit(segment.duration);\n    if (time < 0) {\n      return {\n        mediaIndex: i,\n        startTime: startTime + sumDurations(playlist, startIndex, i)\n      };\n    }\n  }\n\n  // We are out of possible candidates so load the last one...\n  return {\n    mediaIndex: numSegments - 1,\n    startTime: currentTime\n  };\n};\n\nexports.getMediaInfoForTime = getMediaInfoForTime;\n/**\n * Check whether the playlist is blacklisted or not.\n *\n * @param {Object} playlist the media playlist object\n * @return {boolean} whether the playlist is blacklisted or not\n * @function isBlacklisted\n */\nvar isBlacklisted = function isBlacklisted(playlist) {\n  return playlist.excludeUntil && playlist.excludeUntil > Date.now();\n};\n\nexports.isBlacklisted = isBlacklisted;\n/**\n * Check whether the playlist is compatible with current playback configuration or has\n * been blacklisted permanently for being incompatible.\n *\n * @param {Object} playlist the media playlist object\n * @return {boolean} whether the playlist is incompatible or not\n * @function isIncompatible\n */\nvar isIncompatible = function isIncompatible(playlist) {\n  return playlist.excludeUntil && playlist.excludeUntil === Infinity;\n};\n\nexports.isIncompatible = isIncompatible;\n/**\n * Check whether the playlist is enabled or not.\n *\n * @param {Object} playlist the media playlist object\n * @return {boolean} whether the playlist is enabled or not\n * @function isEnabled\n */\nvar isEnabled = function isEnabled(playlist) {\n  var blacklisted = isBlacklisted(playlist);\n\n  return !playlist.disabled && !blacklisted;\n};\n\nexports.isEnabled = isEnabled;\n/**\n * Check whether the playlist has been manually disabled through the representations api.\n *\n * @param {Object} playlist the media playlist object\n * @return {boolean} whether the playlist is disabled manually or not\n * @function isDisabled\n */\nvar isDisabled = function isDisabled(playlist) {\n  return playlist.disabled;\n};\n\nexports.isDisabled = isDisabled;\n/**\n * Returns whether the current playlist is an AES encrypted HLS stream\n *\n * @return {Boolean} true if it's an AES encrypted HLS stream\n */\nvar isAes = function isAes(media) {\n  for (var i = 0; i < media.segments.length; i++) {\n    if (media.segments[i].key) {\n      return true;\n    }\n  }\n  return false;\n};\n\nexports.isAes = isAes;\n/**\n * Returns whether the current playlist contains fMP4\n *\n * @return {Boolean} true if the playlist contains fMP4\n */\nvar isFmp4 = function isFmp4(media) {\n  for (var i = 0; i < media.segments.length; i++) {\n    if (media.segments[i].map) {\n      return true;\n    }\n  }\n  return false;\n};\n\nexports.isFmp4 = isFmp4;\n/**\n * Checks if the playlist has a value for the specified attribute\n *\n * @param {String} attr\n *        Attribute to check for\n * @param {Object} playlist\n *        The media playlist object\n * @return {Boolean}\n *         Whether the playlist contains a value for the attribute or not\n * @function hasAttribute\n */\nvar hasAttribute = function hasAttribute(attr, playlist) {\n  return playlist.attributes && playlist.attributes[attr];\n};\n\nexports.hasAttribute = hasAttribute;\n/**\n * Estimates the time required to complete a segment download from the specified playlist\n *\n * @param {Number} segmentDuration\n *        Duration of requested segment\n * @param {Number} bandwidth\n *        Current measured bandwidth of the player\n * @param {Object} playlist\n *        The media playlist object\n * @param {Number=} bytesReceived\n *        Number of bytes already received for the request. Defaults to 0\n * @return {Number|NaN}\n *         The estimated time to request the segment. NaN if bandwidth information for\n *         the given playlist is unavailable\n * @function estimateSegmentRequestTime\n */\nvar estimateSegmentRequestTime = function estimateSegmentRequestTime(segmentDuration, bandwidth, playlist) {\n  var bytesReceived = arguments.length <= 3 || arguments[3] === undefined ? 0 : arguments[3];\n\n  if (!hasAttribute('BANDWIDTH', playlist)) {\n    return NaN;\n  }\n\n  var size = segmentDuration * playlist.attributes.BANDWIDTH;\n\n  return (size - bytesReceived * 8) / bandwidth;\n};\n\nexports.estimateSegmentRequestTime = estimateSegmentRequestTime;\n/*\n * Returns whether the current playlist is the lowest rendition\n *\n * @return {Boolean} true if on lowest rendition\n */\nvar isLowestEnabledRendition = function isLowestEnabledRendition(master, media) {\n  if (master.playlists.length === 1) {\n    return true;\n  }\n\n  var currentBandwidth = media.attributes.BANDWIDTH || Number.MAX_VALUE;\n\n  return master.playlists.filter(function (playlist) {\n    if (!isEnabled(playlist)) {\n      return false;\n    }\n\n    return (playlist.attributes.BANDWIDTH || 0) < currentBandwidth;\n  }).length === 0;\n};\n\nexports.isLowestEnabledRendition = isLowestEnabledRendition;\n// exports\nexports['default'] = {\n  duration: duration,\n  seekable: seekable,\n  safeLiveIndex: safeLiveIndex,\n  getMediaInfoForTime: getMediaInfoForTime,\n  isEnabled: isEnabled,\n  isDisabled: isDisabled,\n  isBlacklisted: isBlacklisted,\n  isIncompatible: isIncompatible,\n  playlistEnd: playlistEnd,\n  isAes: isAes,\n  isFmp4: isFmp4,\n  hasAttribute: hasAttribute,\n  estimateSegmentRequestTime: estimateSegmentRequestTime,\n  isLowestEnabledRendition: isLowestEnabledRendition\n};\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/playlist.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/ranges.js":
/*!********************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/ranges.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * ranges\n *\n * Utilities for working with TimeRanges.\n *\n */\n\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _slicedToArray = (function () { function sliceIterator(arr, i) { var _arr = []; var _n = true; var _d = false; var _e = undefined; try { for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i['return']) _i['return'](); } finally { if (_d) throw _e; } } return _arr; } return function (arr, i) { if (Array.isArray(arr)) { return arr; } else if (Symbol.iterator in Object(arr)) { return sliceIterator(arr, i); } else { throw new TypeError('Invalid attempt to destructure non-iterable instance'); } }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\n// Fudge factor to account for TimeRanges rounding\nvar TIME_FUDGE_FACTOR = 1 / 30;\n// Comparisons between time values such as current time and the end of the buffered range\n// can be misleading because of precision differences or when the current media has poorly\n// aligned audio and video, which can cause values to be slightly off from what you would\n// expect. This value is what we consider to be safe to use in such comparisons to account\n// for these scenarios.\nvar SAFE_TIME_DELTA = TIME_FUDGE_FACTOR * 3;\n\n/**\n * Clamps a value to within a range\n * @param {Number} num - the value to clamp\n * @param {Number} start - the start of the range to clamp within, inclusive\n * @param {Number} end - the end of the range to clamp within, inclusive\n * @return {Number}\n */\nvar clamp = function clamp(num, _ref) {\n  var _ref2 = _slicedToArray(_ref, 2);\n\n  var start = _ref2[0];\n  var end = _ref2[1];\n\n  return Math.min(Math.max(start, num), end);\n};\nvar filterRanges = function filterRanges(timeRanges, predicate) {\n  var results = [];\n  var i = undefined;\n\n  if (timeRanges && timeRanges.length) {\n    // Search for ranges that match the predicate\n    for (i = 0; i < timeRanges.length; i++) {\n      if (predicate(timeRanges.start(i), timeRanges.end(i))) {\n        results.push([timeRanges.start(i), timeRanges.end(i)]);\n      }\n    }\n  }\n\n  return _videoJs2['default'].createTimeRanges(results);\n};\n\n/**\n * Attempts to find the buffered TimeRange that contains the specified\n * time.\n * @param {TimeRanges} buffered - the TimeRanges object to query\n * @param {number} time  - the time to filter on.\n * @returns {TimeRanges} a new TimeRanges object\n */\nvar findRange = function findRange(buffered, time) {\n  return filterRanges(buffered, function (start, end) {\n    return start - TIME_FUDGE_FACTOR <= time && end + TIME_FUDGE_FACTOR >= time;\n  });\n};\n\n/**\n * Returns the TimeRanges that begin later than the specified time.\n * @param {TimeRanges} timeRanges - the TimeRanges object to query\n * @param {number} time - the time to filter on.\n * @returns {TimeRanges} a new TimeRanges object.\n */\nvar findNextRange = function findNextRange(timeRanges, time) {\n  return filterRanges(timeRanges, function (start) {\n    return start - TIME_FUDGE_FACTOR >= time;\n  });\n};\n\n/**\n * Returns gaps within a list of TimeRanges\n * @param {TimeRanges} buffered - the TimeRanges object\n * @return {TimeRanges} a TimeRanges object of gaps\n */\nvar findGaps = function findGaps(buffered) {\n  if (buffered.length < 2) {\n    return _videoJs2['default'].createTimeRanges();\n  }\n\n  var ranges = [];\n\n  for (var i = 1; i < buffered.length; i++) {\n    var start = buffered.end(i - 1);\n    var end = buffered.start(i);\n\n    ranges.push([start, end]);\n  }\n\n  return _videoJs2['default'].createTimeRanges(ranges);\n};\n\n/**\n * Search for a likely end time for the segment that was just appened\n * based on the state of the `buffered` property before and after the\n * append. If we fin only one such uncommon end-point return it.\n * @param {TimeRanges} original - the buffered time ranges before the update\n * @param {TimeRanges} update - the buffered time ranges after the update\n * @returns {Number|null} the end time added between `original` and `update`,\n * or null if one cannot be unambiguously determined.\n */\nvar findSoleUncommonTimeRangesEnd = function findSoleUncommonTimeRangesEnd(original, update) {\n  var i = undefined;\n  var start = undefined;\n  var end = undefined;\n  var result = [];\n  var edges = [];\n\n  // In order to qualify as a possible candidate, the end point must:\n  //  1) Not have already existed in the `original` ranges\n  //  2) Not result from the shrinking of a range that already existed\n  //     in the `original` ranges\n  //  3) Not be contained inside of a range that existed in `original`\n  var overlapsCurrentEnd = function overlapsCurrentEnd(span) {\n    return span[0] <= end && span[1] >= end;\n  };\n\n  if (original) {\n    // Save all the edges in the `original` TimeRanges object\n    for (i = 0; i < original.length; i++) {\n      start = original.start(i);\n      end = original.end(i);\n\n      edges.push([start, end]);\n    }\n  }\n\n  if (update) {\n    // Save any end-points in `update` that are not in the `original`\n    // TimeRanges object\n    for (i = 0; i < update.length; i++) {\n      start = update.start(i);\n      end = update.end(i);\n\n      if (edges.some(overlapsCurrentEnd)) {\n        continue;\n      }\n\n      // at this point it must be a unique non-shrinking end edge\n      result.push(end);\n    }\n  }\n\n  // we err on the side of caution and return null if didn't find\n  // exactly *one* differing end edge in the search above\n  if (result.length !== 1) {\n    return null;\n  }\n\n  return result[0];\n};\n\n/**\n * Calculate the intersection of two TimeRanges\n * @param {TimeRanges} bufferA\n * @param {TimeRanges} bufferB\n * @returns {TimeRanges} The interesection of `bufferA` with `bufferB`\n */\nvar bufferIntersection = function bufferIntersection(bufferA, bufferB) {\n  var start = null;\n  var end = null;\n  var arity = 0;\n  var extents = [];\n  var ranges = [];\n\n  if (!bufferA || !bufferA.length || !bufferB || !bufferB.length) {\n    return _videoJs2['default'].createTimeRange();\n  }\n\n  // Handle the case where we have both buffers and create an\n  // intersection of the two\n  var count = bufferA.length;\n\n  // A) Gather up all start and end times\n  while (count--) {\n    extents.push({ time: bufferA.start(count), type: 'start' });\n    extents.push({ time: bufferA.end(count), type: 'end' });\n  }\n  count = bufferB.length;\n  while (count--) {\n    extents.push({ time: bufferB.start(count), type: 'start' });\n    extents.push({ time: bufferB.end(count), type: 'end' });\n  }\n  // B) Sort them by time\n  extents.sort(function (a, b) {\n    return a.time - b.time;\n  });\n\n  // C) Go along one by one incrementing arity for start and decrementing\n  //    arity for ends\n  for (count = 0; count < extents.length; count++) {\n    if (extents[count].type === 'start') {\n      arity++;\n\n      // D) If arity is ever incremented to 2 we are entering an\n      //    overlapping range\n      if (arity === 2) {\n        start = extents[count].time;\n      }\n    } else if (extents[count].type === 'end') {\n      arity--;\n\n      // E) If arity is ever decremented to 1 we leaving an\n      //    overlapping range\n      if (arity === 1) {\n        end = extents[count].time;\n      }\n    }\n\n    // F) Record overlapping ranges\n    if (start !== null && end !== null) {\n      ranges.push([start, end]);\n      start = null;\n      end = null;\n    }\n  }\n\n  return _videoJs2['default'].createTimeRanges(ranges);\n};\n\n/**\n * Calculates the percentage of `segmentRange` that overlaps the\n * `buffered` time ranges.\n * @param {TimeRanges} segmentRange - the time range that the segment\n * covers adjusted according to currentTime\n * @param {TimeRanges} referenceRange - the original time range that the\n * segment covers\n * @param {Number} currentTime - time in seconds where the current playback\n * is at\n * @param {TimeRanges} buffered - the currently buffered time ranges\n * @returns {Number} percent of the segment currently buffered\n */\nvar calculateBufferedPercent = function calculateBufferedPercent(adjustedRange, referenceRange, currentTime, buffered) {\n  var referenceDuration = referenceRange.end(0) - referenceRange.start(0);\n  var adjustedDuration = adjustedRange.end(0) - adjustedRange.start(0);\n  var bufferMissingFromAdjusted = referenceDuration - adjustedDuration;\n  var adjustedIntersection = bufferIntersection(adjustedRange, buffered);\n  var referenceIntersection = bufferIntersection(referenceRange, buffered);\n  var adjustedOverlap = 0;\n  var referenceOverlap = 0;\n\n  var count = adjustedIntersection.length;\n\n  while (count--) {\n    adjustedOverlap += adjustedIntersection.end(count) - adjustedIntersection.start(count);\n\n    // If the current overlap segment starts at currentTime, then increase the\n    // overlap duration so that it actually starts at the beginning of referenceRange\n    // by including the difference between the two Range's durations\n    // This is a work around for the way Flash has no buffer before currentTime\n    if (adjustedIntersection.start(count) === currentTime) {\n      adjustedOverlap += bufferMissingFromAdjusted;\n    }\n  }\n\n  count = referenceIntersection.length;\n\n  while (count--) {\n    referenceOverlap += referenceIntersection.end(count) - referenceIntersection.start(count);\n  }\n\n  // Use whichever value is larger for the percentage-buffered since that value\n  // is likely more accurate because the only way\n  return Math.max(adjustedOverlap, referenceOverlap) / referenceDuration * 100;\n};\n\n/**\n * Return the amount of a range specified by the startOfSegment and segmentDuration\n * overlaps the current buffered content.\n *\n * @param {Number} startOfSegment - the time where the segment begins\n * @param {Number} segmentDuration - the duration of the segment in seconds\n * @param {Number} currentTime - time in seconds where the current playback\n * is at\n * @param {TimeRanges} buffered - the state of the buffer\n * @returns {Number} percentage of the segment's time range that is\n * already in `buffered`\n */\nvar getSegmentBufferedPercent = function getSegmentBufferedPercent(startOfSegment, segmentDuration, currentTime, buffered) {\n  var endOfSegment = startOfSegment + segmentDuration;\n\n  // The entire time range of the segment\n  var originalSegmentRange = _videoJs2['default'].createTimeRanges([[startOfSegment, endOfSegment]]);\n\n  // The adjusted segment time range that is setup such that it starts\n  // no earlier than currentTime\n  // Flash has no notion of a back-buffer so adjustedSegmentRange adjusts\n  // for that and the function will still return 100% if a only half of a\n  // segment is actually in the buffer as long as the currentTime is also\n  // half-way through the segment\n  var adjustedSegmentRange = _videoJs2['default'].createTimeRanges([[clamp(startOfSegment, [currentTime, endOfSegment]), endOfSegment]]);\n\n  // This condition happens when the currentTime is beyond the segment's\n  // end time\n  if (adjustedSegmentRange.start(0) === adjustedSegmentRange.end(0)) {\n    return 0;\n  }\n\n  var percent = calculateBufferedPercent(adjustedSegmentRange, originalSegmentRange, currentTime, buffered);\n\n  // If the segment is reported as having a zero duration, return 0%\n  // since it is likely that we will need to fetch the segment\n  if (isNaN(percent) || percent === Infinity || percent === -Infinity) {\n    return 0;\n  }\n\n  return percent;\n};\n\n/**\n * Gets a human readable string for a TimeRange\n *\n * @param {TimeRange} range\n * @returns {String} a human readable string\n */\nvar printableRange = function printableRange(range) {\n  var strArr = [];\n\n  if (!range || !range.length) {\n    return '';\n  }\n\n  for (var i = 0; i < range.length; i++) {\n    strArr.push(range.start(i) + ' => ' + range.end(i));\n  }\n\n  return strArr.join(', ');\n};\n\n/**\n * Calculates the amount of time left in seconds until the player hits the end of the\n * buffer and causes a rebuffer\n *\n * @param {TimeRange} buffered\n *        The state of the buffer\n * @param {Numnber} currentTime\n *        The current time of the player\n * @param {Number} playbackRate\n *        The current playback rate of the player. Defaults to 1.\n * @return {Number}\n *         Time until the player has to start rebuffering in seconds.\n * @function timeUntilRebuffer\n */\nvar timeUntilRebuffer = function timeUntilRebuffer(buffered, currentTime) {\n  var playbackRate = arguments.length <= 2 || arguments[2] === undefined ? 1 : arguments[2];\n\n  var bufferedEnd = buffered.length ? buffered.end(buffered.length - 1) : 0;\n\n  return (bufferedEnd - currentTime) / playbackRate;\n};\n\nexports['default'] = {\n  findRange: findRange,\n  findNextRange: findNextRange,\n  findGaps: findGaps,\n  findSoleUncommonTimeRangesEnd: findSoleUncommonTimeRangesEnd,\n  getSegmentBufferedPercent: getSegmentBufferedPercent,\n  TIME_FUDGE_FACTOR: TIME_FUDGE_FACTOR,\n  SAFE_TIME_DELTA: SAFE_TIME_DELTA,\n  printableRange: printableRange,\n  timeUntilRebuffer: timeUntilRebuffer\n};\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/ranges.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/reload-source-on-error.js":
/*!************************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/reload-source-on-error.js ***!
  \************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar defaultOptions = {\n  errorInterval: 30,\n  getSource: function getSource(next) {\n    var tech = this.tech({ IWillNotUseThisInPlugins: true });\n    var sourceObj = tech.currentSource_;\n\n    return next(sourceObj);\n  }\n};\n\n/**\n * Main entry point for the plugin\n *\n * @param {Player} player a reference to a videojs Player instance\n * @param {Object} [options] an object with plugin options\n * @private\n */\nvar initPlugin = function initPlugin(player, options) {\n  var lastCalled = 0;\n  var seekTo = 0;\n  var localOptions = _videoJs2['default'].mergeOptions(defaultOptions, options);\n\n  player.ready(function () {\n    player.trigger({ type: 'usage', name: 'hls-error-reload-initialized' });\n  });\n\n  /**\n   * Player modifications to perform that must wait until `loadedmetadata`\n   * has been triggered\n   *\n   * @private\n   */\n  var loadedMetadataHandler = function loadedMetadataHandler() {\n    if (seekTo) {\n      player.currentTime(seekTo);\n    }\n  };\n\n  /**\n   * Set the source on the player element, play, and seek if necessary\n   *\n   * @param {Object} sourceObj An object specifying the source url and mime-type to play\n   * @private\n   */\n  var setSource = function setSource(sourceObj) {\n    if (sourceObj === null || sourceObj === undefined) {\n      return;\n    }\n    seekTo = player.duration() !== Infinity && player.currentTime() || 0;\n\n    player.one('loadedmetadata', loadedMetadataHandler);\n\n    player.src(sourceObj);\n    player.trigger({ type: 'usage', name: 'hls-error-reload' });\n    player.play();\n  };\n\n  /**\n   * Attempt to get a source from either the built-in getSource function\n   * or a custom function provided via the options\n   *\n   * @private\n   */\n  var errorHandler = function errorHandler() {\n    // Do not attempt to reload the source if a source-reload occurred before\n    // 'errorInterval' time has elapsed since the last source-reload\n    if (Date.now() - lastCalled < localOptions.errorInterval * 1000) {\n      player.trigger({ type: 'usage', name: 'hls-error-reload-canceled' });\n      return;\n    }\n\n    if (!localOptions.getSource || typeof localOptions.getSource !== 'function') {\n      _videoJs2['default'].log.error('ERROR: reloadSourceOnError - The option getSource must be a function!');\n      return;\n    }\n    lastCalled = Date.now();\n\n    return localOptions.getSource.call(player, setSource);\n  };\n\n  /**\n   * Unbind any event handlers that were bound by the plugin\n   *\n   * @private\n   */\n  var cleanupEvents = function cleanupEvents() {\n    player.off('loadedmetadata', loadedMetadataHandler);\n    player.off('error', errorHandler);\n    player.off('dispose', cleanupEvents);\n  };\n\n  /**\n   * Cleanup before re-initializing the plugin\n   *\n   * @param {Object} [newOptions] an object with plugin options\n   * @private\n   */\n  var reinitPlugin = function reinitPlugin(newOptions) {\n    cleanupEvents();\n    initPlugin(player, newOptions);\n  };\n\n  player.on('error', errorHandler);\n  player.on('dispose', cleanupEvents);\n\n  // Overwrite the plugin function so that we can correctly cleanup before\n  // initializing the plugin\n  player.reloadSourceOnError = reinitPlugin;\n};\n\n/**\n * Reload the source when an error is detected as long as there\n * wasn't an error previously within the last 30 seconds\n *\n * @param {Object} [options] an object with plugin options\n */\nvar reloadSourceOnError = function reloadSourceOnError(options) {\n  initPlugin(this, options);\n};\n\nexports['default'] = reloadSourceOnError;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/reload-source-on-error.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/rendition-mixin.js":
/*!*****************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/rendition-mixin.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar _playlistJs = __webpack_require__(/*! ./playlist.js */ \"./node_modules/videojs-contrib-hls/es5/playlist.js\");\n\n/**\n * Returns a function that acts as the Enable/disable playlist function.\n *\n * @param {PlaylistLoader} loader - The master playlist loader\n * @param {String} playlistUri - uri of the playlist\n * @param {Function} changePlaylistFn - A function to be called after a\n * playlist's enabled-state has been changed. Will NOT be called if a\n * playlist's enabled-state is unchanged\n * @param {Boolean=} enable - Value to set the playlist enabled-state to\n * or if undefined returns the current enabled-state for the playlist\n * @return {Function} Function for setting/getting enabled\n */\nvar enableFunction = function enableFunction(loader, playlistUri, changePlaylistFn) {\n  return function (enable) {\n    var playlist = loader.master.playlists[playlistUri];\n    var incompatible = (0, _playlistJs.isIncompatible)(playlist);\n    var currentlyEnabled = (0, _playlistJs.isEnabled)(playlist);\n\n    if (typeof enable === 'undefined') {\n      return currentlyEnabled;\n    }\n\n    if (enable) {\n      delete playlist.disabled;\n    } else {\n      playlist.disabled = true;\n    }\n\n    if (enable !== currentlyEnabled && !incompatible) {\n      // Ensure the outside world knows about our changes\n      changePlaylistFn();\n      if (enable) {\n        loader.trigger('renditionenabled');\n      } else {\n        loader.trigger('renditiondisabled');\n      }\n    }\n    return enable;\n  };\n};\n\n/**\n * The representation object encapsulates the publicly visible information\n * in a media playlist along with a setter/getter-type function (enabled)\n * for changing the enabled-state of a particular playlist entry\n *\n * @class Representation\n */\n\nvar Representation = function Representation(hlsHandler, playlist, id) {\n  _classCallCheck(this, Representation);\n\n  // Get a reference to a bound version of fastQualityChange_\n  var fastChangeFunction = hlsHandler.masterPlaylistController_.fastQualityChange_.bind(hlsHandler.masterPlaylistController_);\n\n  // some playlist attributes are optional\n  if (playlist.attributes.RESOLUTION) {\n    var resolution = playlist.attributes.RESOLUTION;\n\n    this.width = resolution.width;\n    this.height = resolution.height;\n  }\n\n  this.bandwidth = playlist.attributes.BANDWIDTH;\n\n  // The id is simply the ordinality of the media playlist\n  // within the master playlist\n  this.id = id;\n\n  // Partially-apply the enableFunction to create a playlist-\n  // specific variant\n  this.enabled = enableFunction(hlsHandler.playlists, playlist.uri, fastChangeFunction);\n}\n\n/**\n * A mixin function that adds the `representations` api to an instance\n * of the HlsHandler class\n * @param {HlsHandler} hlsHandler - An instance of HlsHandler to add the\n * representation API into\n */\n;\n\nvar renditionSelectionMixin = function renditionSelectionMixin(hlsHandler) {\n  var playlists = hlsHandler.playlists;\n\n  // Add a single API-specific function to the HlsHandler instance\n  hlsHandler.representations = function () {\n    return playlists.master.playlists.filter(function (media) {\n      return !(0, _playlistJs.isIncompatible)(media);\n    }).map(function (e, i) {\n      return new Representation(hlsHandler, e, e.uri);\n    });\n  };\n};\n\nexports['default'] = renditionSelectionMixin;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/rendition-mixin.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/resolve-url.js":
/*!*************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/resolve-url.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file resolve-url.js\n */\n\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _urlToolkit = __webpack_require__(/*! url-toolkit */ \"./node_modules/url-toolkit/src/url-toolkit.js\");\n\nvar _urlToolkit2 = _interopRequireDefault(_urlToolkit);\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar resolveUrl = function resolveUrl(baseURL, relativeURL) {\n  // return early if we don't need to resolve\n  if (/^[a-z]+:/i.test(relativeURL)) {\n    return relativeURL;\n  }\n\n  // if the base URL is relative then combine with the current location\n  if (!/\\/\\//i.test(baseURL)) {\n    baseURL = _urlToolkit2['default'].buildAbsoluteURL(_globalWindow2['default'].location.href, baseURL);\n  }\n\n  return _urlToolkit2['default'].buildAbsoluteURL(baseURL, relativeURL);\n};\n\nexports['default'] = resolveUrl;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/resolve-url.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/segment-loader.js":
/*!****************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/segment-loader.js ***!
  \****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file segment-loader.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x4, _x5, _x6) { var _again = true; _function: while (_again) { var object = _x4, property = _x5, receiver = _x6; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x4 = parent; _x5 = property; _x6 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _playlist = __webpack_require__(/*! ./playlist */ \"./node_modules/videojs-contrib-hls/es5/playlist.js\");\n\nvar _playlist2 = _interopRequireDefault(_playlist);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _sourceUpdater = __webpack_require__(/*! ./source-updater */ \"./node_modules/videojs-contrib-hls/es5/source-updater.js\");\n\nvar _sourceUpdater2 = _interopRequireDefault(_sourceUpdater);\n\nvar _config = __webpack_require__(/*! ./config */ \"./node_modules/videojs-contrib-hls/es5/config.js\");\n\nvar _config2 = _interopRequireDefault(_config);\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs = __webpack_require__(/*! videojs-contrib-media-sources/es5/remove-cues-from-track.js */ \"./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js\");\n\nvar _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs2 = _interopRequireDefault(_videojsContribMediaSourcesEs5RemoveCuesFromTrackJs);\n\nvar _binUtils = __webpack_require__(/*! ./bin-utils */ \"./node_modules/videojs-contrib-hls/es5/bin-utils.js\");\n\nvar _mediaSegmentRequest = __webpack_require__(/*! ./media-segment-request */ \"./node_modules/videojs-contrib-hls/es5/media-segment-request.js\");\n\nvar _ranges = __webpack_require__(/*! ./ranges */ \"./node_modules/videojs-contrib-hls/es5/ranges.js\");\n\nvar _playlistSelectors = __webpack_require__(/*! ./playlist-selectors */ \"./node_modules/videojs-contrib-hls/es5/playlist-selectors.js\");\n\n// in ms\nvar CHECK_BUFFER_DELAY = 500;\n\n/**\n * Determines if we should call endOfStream on the media source based\n * on the state of the buffer or if appened segment was the final\n * segment in the playlist.\n *\n * @param {Object} playlist a media playlist object\n * @param {Object} mediaSource the MediaSource object\n * @param {Number} segmentIndex the index of segment we last appended\n * @returns {Boolean} do we need to call endOfStream on the MediaSource\n */\nvar detectEndOfStream = function detectEndOfStream(playlist, mediaSource, segmentIndex) {\n  if (!playlist || !mediaSource) {\n    return false;\n  }\n\n  var segments = playlist.segments;\n\n  // determine a few boolean values to help make the branch below easier\n  // to read\n  var appendedLastSegment = segmentIndex === segments.length;\n\n  // if we've buffered to the end of the video, we need to call endOfStream\n  // so that MediaSources can trigger the `ended` event when it runs out of\n  // buffered data instead of waiting for me\n  return playlist.endList && mediaSource.readyState === 'open' && appendedLastSegment;\n};\n\nvar finite = function finite(num) {\n  return typeof num === 'number' && isFinite(num);\n};\n\nvar illegalMediaSwitch = function illegalMediaSwitch(loaderType, startingMedia, newSegmentMedia) {\n  // Although these checks should most likely cover non 'main' types, for now it narrows\n  // the scope of our checks.\n  if (loaderType !== 'main' || !startingMedia || !newSegmentMedia) {\n    return null;\n  }\n\n  if (!newSegmentMedia.containsAudio && !newSegmentMedia.containsVideo) {\n    return 'Neither audio nor video found in segment.';\n  }\n\n  if (startingMedia.containsVideo && !newSegmentMedia.containsVideo) {\n    return 'Only audio found in segment when we expected video.' + ' We can\\'t switch to audio only from a stream that had video.' + ' To get rid of this message, please add codec information to the manifest.';\n  }\n\n  if (!startingMedia.containsVideo && newSegmentMedia.containsVideo) {\n    return 'Video found in segment when we expected only audio.' + ' We can\\'t switch to a stream with video from an audio only stream.' + ' To get rid of this message, please add codec information to the manifest.';\n  }\n\n  return null;\n};\n\nexports.illegalMediaSwitch = illegalMediaSwitch;\n/**\n * Calculates a time value that is safe to remove from the back buffer without interupting\n * playback.\n *\n * @param {TimeRange} seekable\n *        The current seekable range\n * @param {Number} currentTime\n *        The current time of the player\n * @param {Number} targetDuration\n *        The target duration of the current playlist\n * @return {Number}\n *         Time that is safe to remove from the back buffer without interupting playback\n */\nvar safeBackBufferTrimTime = function safeBackBufferTrimTime(seekable, currentTime, targetDuration) {\n  var removeToTime = undefined;\n\n  if (seekable.length && seekable.start(0) > 0 && seekable.start(0) < currentTime) {\n    // If we have a seekable range use that as the limit for what can be removed safely\n    removeToTime = seekable.start(0);\n  } else {\n    // otherwise remove anything older than 30 seconds before the current play head\n    removeToTime = currentTime - 30;\n  }\n\n  // Don't allow removing from the buffer within target duration of current time\n  // to avoid the possibility of removing the GOP currently being played which could\n  // cause playback stalls.\n  return Math.min(removeToTime, currentTime - targetDuration);\n};\n\nexports.safeBackBufferTrimTime = safeBackBufferTrimTime;\n/**\n * An object that manages segment loading and appending.\n *\n * @class SegmentLoader\n * @param {Object} options required and optional options\n * @extends videojs.EventTarget\n */\n\nvar SegmentLoader = (function (_videojs$EventTarget) {\n  _inherits(SegmentLoader, _videojs$EventTarget);\n\n  function SegmentLoader(settings) {\n    var _this = this;\n\n    var options = arguments.length <= 1 || arguments[1] === undefined ? {} : arguments[1];\n\n    _classCallCheck(this, SegmentLoader);\n\n    _get(Object.getPrototypeOf(SegmentLoader.prototype), 'constructor', this).call(this);\n    // check pre-conditions\n    if (!settings) {\n      throw new TypeError('Initialization settings are required');\n    }\n    if (typeof settings.currentTime !== 'function') {\n      throw new TypeError('No currentTime getter specified');\n    }\n    if (!settings.mediaSource) {\n      throw new TypeError('No MediaSource specified');\n    }\n    // public properties\n    this.state = 'INIT';\n    this.bandwidth = settings.bandwidth;\n    this.throughput = { rate: 0, count: 0 };\n    this.roundTrip = NaN;\n    this.resetStats_();\n    this.mediaIndex = null;\n\n    // private settings\n    this.hasPlayed_ = settings.hasPlayed;\n    this.currentTime_ = settings.currentTime;\n    this.seekable_ = settings.seekable;\n    this.seeking_ = settings.seeking;\n    this.duration_ = settings.duration;\n    this.mediaSource_ = settings.mediaSource;\n    this.hls_ = settings.hls;\n    this.loaderType_ = settings.loaderType;\n    this.startingMedia_ = void 0;\n    this.segmentMetadataTrack_ = settings.segmentMetadataTrack;\n    this.goalBufferLength_ = settings.goalBufferLength;\n\n    // private instance variables\n    this.checkBufferTimeout_ = null;\n    this.error_ = void 0;\n    this.currentTimeline_ = -1;\n    this.pendingSegment_ = null;\n    this.mimeType_ = null;\n    this.sourceUpdater_ = null;\n    this.xhrOptions_ = null;\n\n    // Fragmented mp4 playback\n    this.activeInitSegmentId_ = null;\n    this.initSegments_ = {};\n\n    this.decrypter_ = settings.decrypter;\n\n    // Manages the tracking and generation of sync-points, mappings\n    // between a time in the display time and a segment index within\n    // a playlist\n    this.syncController_ = settings.syncController;\n    this.syncPoint_ = {\n      segmentIndex: 0,\n      time: 0\n    };\n\n    this.syncController_.on('syncinfoupdate', function () {\n      return _this.trigger('syncinfoupdate');\n    });\n\n    this.mediaSource_.addEventListener('sourceopen', function () {\n      return _this.ended_ = false;\n    });\n\n    // ...for determining the fetch location\n    this.fetchAtBuffer_ = false;\n\n    if (options.debug) {\n      this.logger_ = _videoJs2['default'].log.bind(_videoJs2['default'], 'segment-loader', this.loaderType_, '->');\n    }\n  }\n\n  /**\n   * reset all of our media stats\n   *\n   * @private\n   */\n\n  _createClass(SegmentLoader, [{\n    key: 'resetStats_',\n    value: function resetStats_() {\n      this.mediaBytesTransferred = 0;\n      this.mediaRequests = 0;\n      this.mediaRequestsAborted = 0;\n      this.mediaRequestsTimedout = 0;\n      this.mediaRequestsErrored = 0;\n      this.mediaTransferDuration = 0;\n      this.mediaSecondsLoaded = 0;\n    }\n\n    /**\n     * dispose of the SegmentLoader and reset to the default state\n     */\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      this.state = 'DISPOSED';\n      this.pause();\n      this.abort_();\n      if (this.sourceUpdater_) {\n        this.sourceUpdater_.dispose();\n      }\n      this.resetStats_();\n    }\n\n    /**\n     * abort anything that is currently doing on with the SegmentLoader\n     * and reset to a default state\n     */\n  }, {\n    key: 'abort',\n    value: function abort() {\n      if (this.state !== 'WAITING') {\n        if (this.pendingSegment_) {\n          this.pendingSegment_ = null;\n        }\n        return;\n      }\n\n      this.abort_();\n\n      // We aborted the requests we were waiting on, so reset the loader's state to READY\n      // since we are no longer \"waiting\" on any requests. XHR callback is not always run\n      // when the request is aborted. This will prevent the loader from being stuck in the\n      // WAITING state indefinitely.\n      this.state = 'READY';\n\n      // don't wait for buffer check timeouts to begin fetching the\n      // next segment\n      if (!this.paused()) {\n        this.monitorBuffer_();\n      }\n    }\n\n    /**\n     * abort all pending xhr requests and null any pending segements\n     *\n     * @private\n     */\n  }, {\n    key: 'abort_',\n    value: function abort_() {\n      if (this.pendingSegment_) {\n        this.pendingSegment_.abortRequests();\n      }\n\n      // clear out the segment being processed\n      this.pendingSegment_ = null;\n    }\n\n    /**\n     * set an error on the segment loader and null out any pending segements\n     *\n     * @param {Error} error the error to set on the SegmentLoader\n     * @return {Error} the error that was set or that is currently set\n     */\n  }, {\n    key: 'error',\n    value: function error(_error) {\n      if (typeof _error !== 'undefined') {\n        this.error_ = _error;\n      }\n\n      this.pendingSegment_ = null;\n      return this.error_;\n    }\n  }, {\n    key: 'endOfStream',\n    value: function endOfStream() {\n      this.ended_ = true;\n      this.pause();\n      this.trigger('ended');\n    }\n\n    /**\n     * Indicates which time ranges are buffered\n     *\n     * @return {TimeRange}\n     *         TimeRange object representing the current buffered ranges\n     */\n  }, {\n    key: 'buffered_',\n    value: function buffered_() {\n      if (!this.sourceUpdater_) {\n        return _videoJs2['default'].createTimeRanges();\n      }\n\n      return this.sourceUpdater_.buffered();\n    }\n\n    /**\n     * Gets and sets init segment for the provided map\n     *\n     * @param {Object} map\n     *        The map object representing the init segment to get or set\n     * @param {Boolean=} set\n     *        If true, the init segment for the provided map should be saved\n     * @return {Object}\n     *         map object for desired init segment\n     */\n  }, {\n    key: 'initSegment',\n    value: function initSegment(map) {\n      var set = arguments.length <= 1 || arguments[1] === undefined ? false : arguments[1];\n\n      if (!map) {\n        return null;\n      }\n\n      var id = (0, _binUtils.initSegmentId)(map);\n      var storedMap = this.initSegments_[id];\n\n      if (set && !storedMap && map.bytes) {\n        this.initSegments_[id] = storedMap = {\n          resolvedUri: map.resolvedUri,\n          byterange: map.byterange,\n          bytes: map.bytes\n        };\n      }\n\n      return storedMap || map;\n    }\n\n    /**\n     * Returns true if all configuration required for loading is present, otherwise false.\n     *\n     * @return {Boolean} True if the all configuration is ready for loading\n     * @private\n     */\n  }, {\n    key: 'couldBeginLoading_',\n    value: function couldBeginLoading_() {\n      return this.playlist_ && (\n      // the source updater is created when init_ is called, so either having a\n      // source updater or being in the INIT state with a mimeType is enough\n      // to say we have all the needed configuration to start loading.\n      this.sourceUpdater_ || this.mimeType_ && this.state === 'INIT') && !this.paused();\n    }\n\n    /**\n     * load a playlist and start to fill the buffer\n     */\n  }, {\n    key: 'load',\n    value: function load() {\n      // un-pause\n      this.monitorBuffer_();\n\n      // if we don't have a playlist yet, keep waiting for one to be\n      // specified\n      if (!this.playlist_) {\n        return;\n      }\n\n      // not sure if this is the best place for this\n      this.syncController_.setDateTimeMapping(this.playlist_);\n\n      // if all the configuration is ready, initialize and begin loading\n      if (this.state === 'INIT' && this.couldBeginLoading_()) {\n        return this.init_();\n      }\n\n      // if we're in the middle of processing a segment already, don't\n      // kick off an additional segment request\n      if (!this.couldBeginLoading_() || this.state !== 'READY' && this.state !== 'INIT') {\n        return;\n      }\n\n      this.state = 'READY';\n    }\n\n    /**\n     * Once all the starting parameters have been specified, begin\n     * operation. This method should only be invoked from the INIT\n     * state.\n     *\n     * @private\n     */\n  }, {\n    key: 'init_',\n    value: function init_() {\n      this.state = 'READY';\n      this.sourceUpdater_ = new _sourceUpdater2['default'](this.mediaSource_, this.mimeType_);\n      this.resetEverything();\n      return this.monitorBuffer_();\n    }\n\n    /**\n     * set a playlist on the segment loader\n     *\n     * @param {PlaylistLoader} media the playlist to set on the segment loader\n     */\n  }, {\n    key: 'playlist',\n    value: function playlist(newPlaylist) {\n      var options = arguments.length <= 1 || arguments[1] === undefined ? {} : arguments[1];\n\n      if (!newPlaylist) {\n        return;\n      }\n\n      var oldPlaylist = this.playlist_;\n      var segmentInfo = this.pendingSegment_;\n\n      this.playlist_ = newPlaylist;\n      this.xhrOptions_ = options;\n\n      // when we haven't started playing yet, the start of a live playlist\n      // is always our zero-time so force a sync update each time the playlist\n      // is refreshed from the server\n      if (!this.hasPlayed_()) {\n        newPlaylist.syncInfo = {\n          mediaSequence: newPlaylist.mediaSequence,\n          time: 0\n        };\n      }\n\n      // in VOD, this is always a rendition switch (or we updated our syncInfo above)\n      // in LIVE, we always want to update with new playlists (including refreshes)\n      this.trigger('syncinfoupdate');\n\n      // if we were unpaused but waiting for a playlist, start\n      // buffering now\n      if (this.state === 'INIT' && this.couldBeginLoading_()) {\n        return this.init_();\n      }\n\n      if (!oldPlaylist || oldPlaylist.uri !== newPlaylist.uri) {\n        if (this.mediaIndex !== null) {\n          // we must \"resync\" the segment loader when we switch renditions and\n          // the segment loader is already synced to the previous rendition\n          this.resyncLoader();\n        }\n\n        // the rest of this function depends on `oldPlaylist` being defined\n        return;\n      }\n\n      // we reloaded the same playlist so we are in a live scenario\n      // and we will likely need to adjust the mediaIndex\n      var mediaSequenceDiff = newPlaylist.mediaSequence - oldPlaylist.mediaSequence;\n\n      this.logger_('mediaSequenceDiff', mediaSequenceDiff);\n\n      // update the mediaIndex on the SegmentLoader\n      // this is important because we can abort a request and this value must be\n      // equal to the last appended mediaIndex\n      if (this.mediaIndex !== null) {\n        this.mediaIndex -= mediaSequenceDiff;\n      }\n\n      // update the mediaIndex on the SegmentInfo object\n      // this is important because we will update this.mediaIndex with this value\n      // in `handleUpdateEnd_` after the segment has been successfully appended\n      if (segmentInfo) {\n        segmentInfo.mediaIndex -= mediaSequenceDiff;\n\n        // we need to update the referenced segment so that timing information is\n        // saved for the new playlist's segment, however, if the segment fell off the\n        // playlist, we can leave the old reference and just lose the timing info\n        if (segmentInfo.mediaIndex >= 0) {\n          segmentInfo.segment = newPlaylist.segments[segmentInfo.mediaIndex];\n        }\n      }\n\n      this.syncController_.saveExpiredSegmentInfo(oldPlaylist, newPlaylist);\n    }\n\n    /**\n     * Prevent the loader from fetching additional segments. If there\n     * is a segment request outstanding, it will finish processing\n     * before the loader halts. A segment loader can be unpaused by\n     * calling load().\n     */\n  }, {\n    key: 'pause',\n    value: function pause() {\n      if (this.checkBufferTimeout_) {\n        _globalWindow2['default'].clearTimeout(this.checkBufferTimeout_);\n\n        this.checkBufferTimeout_ = null;\n      }\n    }\n\n    /**\n     * Returns whether the segment loader is fetching additional\n     * segments when given the opportunity. This property can be\n     * modified through calls to pause() and load().\n     */\n  }, {\n    key: 'paused',\n    value: function paused() {\n      return this.checkBufferTimeout_ === null;\n    }\n\n    /**\n     * create/set the following mimetype on the SourceBuffer through a\n     * SourceUpdater\n     *\n     * @param {String} mimeType the mime type string to use\n     */\n  }, {\n    key: 'mimeType',\n    value: function mimeType(_mimeType) {\n      if (this.mimeType_) {\n        return;\n      }\n\n      this.mimeType_ = _mimeType;\n      // if we were unpaused but waiting for a sourceUpdater, start\n      // buffering now\n      if (this.state === 'INIT' && this.couldBeginLoading_()) {\n        this.init_();\n      }\n    }\n\n    /**\n     * Delete all the buffered data and reset the SegmentLoader\n     */\n  }, {\n    key: 'resetEverything',\n    value: function resetEverything() {\n      this.ended_ = false;\n      this.resetLoader();\n      this.remove(0, this.duration_());\n      this.trigger('reseteverything');\n    }\n\n    /**\n     * Force the SegmentLoader to resync and start loading around the currentTime instead\n     * of starting at the end of the buffer\n     *\n     * Useful for fast quality changes\n     */\n  }, {\n    key: 'resetLoader',\n    value: function resetLoader() {\n      this.fetchAtBuffer_ = false;\n      this.resyncLoader();\n    }\n\n    /**\n     * Force the SegmentLoader to restart synchronization and make a conservative guess\n     * before returning to the simple walk-forward method\n     */\n  }, {\n    key: 'resyncLoader',\n    value: function resyncLoader() {\n      this.mediaIndex = null;\n      this.syncPoint_ = null;\n      this.abort();\n    }\n\n    /**\n     * Remove any data in the source buffer between start and end times\n     * @param {Number} start - the start time of the region to remove from the buffer\n     * @param {Number} end - the end time of the region to remove from the buffer\n     */\n  }, {\n    key: 'remove',\n    value: function remove(start, end) {\n      if (this.sourceUpdater_) {\n        this.sourceUpdater_.remove(start, end);\n      }\n      (0, _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs2['default'])(start, end, this.segmentMetadataTrack_);\n    }\n\n    /**\n     * (re-)schedule monitorBufferTick_ to run as soon as possible\n     *\n     * @private\n     */\n  }, {\n    key: 'monitorBuffer_',\n    value: function monitorBuffer_() {\n      if (this.checkBufferTimeout_) {\n        _globalWindow2['default'].clearTimeout(this.checkBufferTimeout_);\n      }\n\n      this.checkBufferTimeout_ = _globalWindow2['default'].setTimeout(this.monitorBufferTick_.bind(this), 1);\n    }\n\n    /**\n     * As long as the SegmentLoader is in the READY state, periodically\n     * invoke fillBuffer_().\n     *\n     * @private\n     */\n  }, {\n    key: 'monitorBufferTick_',\n    value: function monitorBufferTick_() {\n      if (this.state === 'READY') {\n        this.fillBuffer_();\n      }\n\n      if (this.checkBufferTimeout_) {\n        _globalWindow2['default'].clearTimeout(this.checkBufferTimeout_);\n      }\n\n      this.checkBufferTimeout_ = _globalWindow2['default'].setTimeout(this.monitorBufferTick_.bind(this), CHECK_BUFFER_DELAY);\n    }\n\n    /**\n     * fill the buffer with segements unless the sourceBuffers are\n     * currently updating\n     *\n     * Note: this function should only ever be called by monitorBuffer_\n     * and never directly\n     *\n     * @private\n     */\n  }, {\n    key: 'fillBuffer_',\n    value: function fillBuffer_() {\n      if (this.sourceUpdater_.updating()) {\n        return;\n      }\n\n      if (!this.syncPoint_) {\n        this.syncPoint_ = this.syncController_.getSyncPoint(this.playlist_, this.duration_(), this.currentTimeline_, this.currentTime_());\n      }\n\n      // see if we need to begin loading immediately\n      var segmentInfo = this.checkBuffer_(this.buffered_(), this.playlist_, this.mediaIndex, this.hasPlayed_(), this.currentTime_(), this.syncPoint_);\n\n      if (!segmentInfo) {\n        return;\n      }\n\n      var isEndOfStream = detectEndOfStream(this.playlist_, this.mediaSource_, segmentInfo.mediaIndex);\n\n      if (isEndOfStream) {\n        this.endOfStream();\n        return;\n      }\n\n      if (segmentInfo.mediaIndex === this.playlist_.segments.length - 1 && this.mediaSource_.readyState === 'ended' && !this.seeking_()) {\n        return;\n      }\n\n      // We will need to change timestampOffset of the sourceBuffer if either of\n      // the following conditions are true:\n      // - The segment.timeline !== this.currentTimeline\n      //   (we are crossing a discontinuity somehow)\n      // - The \"timestampOffset\" for the start of this segment is less than\n      //   the currently set timestampOffset\n      if (segmentInfo.timeline !== this.currentTimeline_ || segmentInfo.startOfSegment !== null && segmentInfo.startOfSegment < this.sourceUpdater_.timestampOffset()) {\n        this.syncController_.reset();\n        segmentInfo.timestampOffset = segmentInfo.startOfSegment;\n      }\n\n      this.loadSegment_(segmentInfo);\n    }\n\n    /**\n     * Determines what segment request should be made, given current playback\n     * state.\n     *\n     * @param {TimeRanges} buffered - the state of the buffer\n     * @param {Object} playlist - the playlist object to fetch segments from\n     * @param {Number} mediaIndex - the previous mediaIndex fetched or null\n     * @param {Boolean} hasPlayed - a flag indicating whether we have played or not\n     * @param {Number} currentTime - the playback position in seconds\n     * @param {Object} syncPoint - a segment info object that describes the\n     * @returns {Object} a segment request object that describes the segment to load\n     */\n  }, {\n    key: 'checkBuffer_',\n    value: function checkBuffer_(buffered, playlist, mediaIndex, hasPlayed, currentTime, syncPoint) {\n      var lastBufferedEnd = 0;\n      var startOfSegment = undefined;\n\n      if (buffered.length) {\n        lastBufferedEnd = buffered.end(buffered.length - 1);\n      }\n\n      var bufferedTime = Math.max(0, lastBufferedEnd - currentTime);\n\n      if (!playlist.segments.length) {\n        return null;\n      }\n\n      // if there is plenty of content buffered, and the video has\n      // been played before relax for awhile\n      if (bufferedTime >= this.goalBufferLength_()) {\n        return null;\n      }\n\n      // if the video has not yet played once, and we already have\n      // one segment downloaded do nothing\n      if (!hasPlayed && bufferedTime >= 1) {\n        return null;\n      }\n\n      this.logger_('checkBuffer_', 'mediaIndex:', mediaIndex, 'hasPlayed:', hasPlayed, 'currentTime:', currentTime, 'syncPoint:', syncPoint, 'fetchAtBuffer:', this.fetchAtBuffer_, 'bufferedTime:', bufferedTime);\n\n      // When the syncPoint is null, there is no way of determining a good\n      // conservative segment index to fetch from\n      // The best thing to do here is to get the kind of sync-point data by\n      // making a request\n      if (syncPoint === null) {\n        mediaIndex = this.getSyncSegmentCandidate_(playlist);\n        this.logger_('getSync', 'mediaIndex:', mediaIndex);\n        return this.generateSegmentInfo_(playlist, mediaIndex, null, true);\n      }\n\n      // Under normal playback conditions fetching is a simple walk forward\n      if (mediaIndex !== null) {\n        this.logger_('walkForward', 'mediaIndex:', mediaIndex + 1);\n        var segment = playlist.segments[mediaIndex];\n\n        if (segment && segment.end) {\n          startOfSegment = segment.end;\n        } else {\n          startOfSegment = lastBufferedEnd;\n        }\n        return this.generateSegmentInfo_(playlist, mediaIndex + 1, startOfSegment, false);\n      }\n\n      // There is a sync-point but the lack of a mediaIndex indicates that\n      // we need to make a good conservative guess about which segment to\n      // fetch\n      if (this.fetchAtBuffer_) {\n        // Find the segment containing the end of the buffer\n        var mediaSourceInfo = _playlist2['default'].getMediaInfoForTime(playlist, lastBufferedEnd, syncPoint.segmentIndex, syncPoint.time);\n\n        mediaIndex = mediaSourceInfo.mediaIndex;\n        startOfSegment = mediaSourceInfo.startTime;\n      } else {\n        // Find the segment containing currentTime\n        var mediaSourceInfo = _playlist2['default'].getMediaInfoForTime(playlist, currentTime, syncPoint.segmentIndex, syncPoint.time);\n\n        mediaIndex = mediaSourceInfo.mediaIndex;\n        startOfSegment = mediaSourceInfo.startTime;\n      }\n      this.logger_('getMediaIndexForTime', 'mediaIndex:', mediaIndex, 'startOfSegment:', startOfSegment);\n\n      return this.generateSegmentInfo_(playlist, mediaIndex, startOfSegment, false);\n    }\n\n    /**\n     * The segment loader has no recourse except to fetch a segment in the\n     * current playlist and use the internal timestamps in that segment to\n     * generate a syncPoint. This function returns a good candidate index\n     * for that process.\n     *\n     * @param {Object} playlist - the playlist object to look for a\n     * @returns {Number} An index of a segment from the playlist to load\n     */\n  }, {\n    key: 'getSyncSegmentCandidate_',\n    value: function getSyncSegmentCandidate_(playlist) {\n      var _this2 = this;\n\n      if (this.currentTimeline_ === -1) {\n        return 0;\n      }\n\n      var segmentIndexArray = playlist.segments.map(function (s, i) {\n        return {\n          timeline: s.timeline,\n          segmentIndex: i\n        };\n      }).filter(function (s) {\n        return s.timeline === _this2.currentTimeline_;\n      });\n\n      if (segmentIndexArray.length) {\n        return segmentIndexArray[Math.min(segmentIndexArray.length - 1, 1)].segmentIndex;\n      }\n\n      return Math.max(playlist.segments.length - 1, 0);\n    }\n  }, {\n    key: 'generateSegmentInfo_',\n    value: function generateSegmentInfo_(playlist, mediaIndex, startOfSegment, isSyncRequest) {\n      if (mediaIndex < 0 || mediaIndex >= playlist.segments.length) {\n        return null;\n      }\n\n      var segment = playlist.segments[mediaIndex];\n\n      return {\n        requestId: 'segment-loader-' + Math.random(),\n        // resolve the segment URL relative to the playlist\n        uri: segment.resolvedUri,\n        // the segment's mediaIndex at the time it was requested\n        mediaIndex: mediaIndex,\n        // whether or not to update the SegmentLoader's state with this\n        // segment's mediaIndex\n        isSyncRequest: isSyncRequest,\n        startOfSegment: startOfSegment,\n        // the segment's playlist\n        playlist: playlist,\n        // unencrypted bytes of the segment\n        bytes: null,\n        // when a key is defined for this segment, the encrypted bytes\n        encryptedBytes: null,\n        // The target timestampOffset for this segment when we append it\n        // to the source buffer\n        timestampOffset: null,\n        // The timeline that the segment is in\n        timeline: segment.timeline,\n        // The expected duration of the segment in seconds\n        duration: segment.duration,\n        // retain the segment in case the playlist updates while doing an async process\n        segment: segment\n      };\n    }\n\n    /**\n     * Determines if the network has enough bandwidth to complete the current segment\n     * request in a timely manner. If not, the request will be aborted early and bandwidth\n     * updated to trigger a playlist switch.\n     *\n     * @param {Object} stats\n     *        Object containing stats about the request timing and size\n     * @return {Boolean} True if the request was aborted, false otherwise\n     * @private\n     */\n  }, {\n    key: 'abortRequestEarly_',\n    value: function abortRequestEarly_(stats) {\n      if (this.hls_.tech_.paused() ||\n      // Don't abort if the current playlist is on the lowestEnabledRendition\n      // TODO: Replace using timeout with a boolean indicating whether this playlist is\n      //       the lowestEnabledRendition.\n      !this.xhrOptions_.timeout ||\n      // Don't abort if we have no bandwidth information to estimate segment sizes\n      !this.playlist_.attributes.BANDWIDTH) {\n        return false;\n      }\n\n      // Wait at least 1 second since the first byte of data has been received before\n      // using the calculated bandwidth from the progress event to allow the bitrate\n      // to stabilize\n      if (Date.now() - (stats.firstBytesReceivedAt || Date.now()) < 1000) {\n        return false;\n      }\n\n      var currentTime = this.currentTime_();\n      var measuredBandwidth = stats.bandwidth;\n      var segmentDuration = this.pendingSegment_.duration;\n\n      var requestTimeRemaining = _playlist2['default'].estimateSegmentRequestTime(segmentDuration, measuredBandwidth, this.playlist_, stats.bytesReceived);\n\n      // Subtract 1 from the timeUntilRebuffer so we still consider an early abort\n      // if we are only left with less than 1 second when the request completes.\n      // A negative timeUntilRebuffering indicates we are already rebuffering\n      var timeUntilRebuffer = (0, _ranges.timeUntilRebuffer)(this.buffered_(), currentTime, this.hls_.tech_.playbackRate()) - 1;\n\n      // Only consider aborting early if the estimated time to finish the download\n      // is larger than the estimated time until the player runs out of forward buffer\n      if (requestTimeRemaining <= timeUntilRebuffer) {\n        return false;\n      }\n\n      var switchCandidate = (0, _playlistSelectors.minRebufferMaxBandwidthSelector)({\n        master: this.hls_.playlists.master,\n        currentTime: currentTime,\n        bandwidth: measuredBandwidth,\n        duration: this.duration_(),\n        segmentDuration: segmentDuration,\n        timeUntilRebuffer: timeUntilRebuffer,\n        currentTimeline: this.currentTimeline_,\n        syncController: this.syncController_\n      });\n\n      if (!switchCandidate) {\n        return;\n      }\n\n      var rebufferingImpact = requestTimeRemaining - timeUntilRebuffer;\n\n      var timeSavedBySwitching = rebufferingImpact - switchCandidate.rebufferingImpact;\n\n      var minimumTimeSaving = 0.5;\n\n      // If we are already rebuffering, increase the amount of variance we add to the\n      // potential round trip time of the new request so that we are not too aggressive\n      // with switching to a playlist that might save us a fraction of a second.\n      if (timeUntilRebuffer <= _ranges.TIME_FUDGE_FACTOR) {\n        minimumTimeSaving = 1;\n      }\n\n      if (!switchCandidate.playlist || switchCandidate.playlist.uri === this.playlist_.uri || timeSavedBySwitching < minimumTimeSaving) {\n        return false;\n      }\n\n      // set the bandwidth to that of the desired playlist being sure to scale by\n      // BANDWIDTH_VARIANCE and add one so the playlist selector does not exclude it\n      // don't trigger a bandwidthupdate as the bandwidth is artifial\n      this.bandwidth = switchCandidate.playlist.attributes.BANDWIDTH * _config2['default'].BANDWIDTH_VARIANCE + 1;\n      this.abort();\n      this.trigger('earlyabort');\n      return true;\n    }\n\n    /**\n     * XHR `progress` event handler\n     *\n     * @param {Event}\n     *        The XHR `progress` event\n     * @param {Object} simpleSegment\n     *        A simplified segment object copy\n     * @private\n     */\n  }, {\n    key: 'handleProgress_',\n    value: function handleProgress_(event, simpleSegment) {\n      if (!this.pendingSegment_ || simpleSegment.requestId !== this.pendingSegment_.requestId || this.abortRequestEarly_(simpleSegment.stats)) {\n        return;\n      }\n\n      this.trigger('progress');\n    }\n\n    /**\n     * load a specific segment from a request into the buffer\n     *\n     * @private\n     */\n  }, {\n    key: 'loadSegment_',\n    value: function loadSegment_(segmentInfo) {\n      this.state = 'WAITING';\n      this.pendingSegment_ = segmentInfo;\n      this.trimBackBuffer_(segmentInfo);\n\n      segmentInfo.abortRequests = (0, _mediaSegmentRequest.mediaSegmentRequest)(this.hls_.xhr, this.xhrOptions_, this.decrypter_, this.createSimplifiedSegmentObj_(segmentInfo),\n      // progress callback\n      this.handleProgress_.bind(this), this.segmentRequestFinished_.bind(this));\n    }\n\n    /**\n     * trim the back buffer so that we don't have too much data\n     * in the source buffer\n     *\n     * @private\n     *\n     * @param {Object} segmentInfo - the current segment\n     */\n  }, {\n    key: 'trimBackBuffer_',\n    value: function trimBackBuffer_(segmentInfo) {\n      var removeToTime = safeBackBufferTrimTime(this.seekable_(), this.currentTime_(), this.playlist_.targetDuration || 10);\n\n      // Chrome has a hard limit of 150MB of\n      // buffer and a very conservative \"garbage collector\"\n      // We manually clear out the old buffer to ensure\n      // we don't trigger the QuotaExceeded error\n      // on the source buffer during subsequent appends\n\n      if (removeToTime > 0) {\n        this.remove(0, removeToTime);\n      }\n    }\n\n    /**\n     * created a simplified copy of the segment object with just the\n     * information necessary to perform the XHR and decryption\n     *\n     * @private\n     *\n     * @param {Object} segmentInfo - the current segment\n     * @returns {Object} a simplified segment object copy\n     */\n  }, {\n    key: 'createSimplifiedSegmentObj_',\n    value: function createSimplifiedSegmentObj_(segmentInfo) {\n      var segment = segmentInfo.segment;\n      var simpleSegment = {\n        resolvedUri: segment.resolvedUri,\n        byterange: segment.byterange,\n        requestId: segmentInfo.requestId\n      };\n\n      if (segment.key) {\n        // if the media sequence is greater than 2^32, the IV will be incorrect\n        // assuming 10s segments, that would be about 1300 years\n        var iv = segment.key.iv || new Uint32Array([0, 0, 0, segmentInfo.mediaIndex + segmentInfo.playlist.mediaSequence]);\n\n        simpleSegment.key = {\n          resolvedUri: segment.key.resolvedUri,\n          iv: iv\n        };\n      }\n\n      if (segment.map) {\n        simpleSegment.map = this.initSegment(segment.map);\n      }\n\n      return simpleSegment;\n    }\n\n    /**\n     * Handle the callback from the segmentRequest function and set the\n     * associated SegmentLoader state and errors if necessary\n     *\n     * @private\n     */\n  }, {\n    key: 'segmentRequestFinished_',\n    value: function segmentRequestFinished_(error, simpleSegment) {\n      // every request counts as a media request even if it has been aborted\n      // or canceled due to a timeout\n      this.mediaRequests += 1;\n\n      if (simpleSegment.stats) {\n        this.mediaBytesTransferred += simpleSegment.stats.bytesReceived;\n        this.mediaTransferDuration += simpleSegment.stats.roundTripTime;\n      }\n\n      // The request was aborted and the SegmentLoader has already been reset\n      if (!this.pendingSegment_) {\n        this.mediaRequestsAborted += 1;\n        return;\n      }\n\n      // the request was aborted and the SegmentLoader has already started\n      // another request. this can happen when the timeout for an aborted\n      // request triggers due to a limitation in the XHR library\n      // do not count this as any sort of request or we risk double-counting\n      if (simpleSegment.requestId !== this.pendingSegment_.requestId) {\n        return;\n      }\n\n      // an error occurred from the active pendingSegment_ so reset everything\n      if (error) {\n        this.pendingSegment_ = null;\n        this.state = 'READY';\n\n        // the requests were aborted just record the aborted stat and exit\n        // this is not a true error condition and nothing corrective needs\n        // to be done\n        if (error.code === _mediaSegmentRequest.REQUEST_ERRORS.ABORTED) {\n          this.mediaRequestsAborted += 1;\n          return;\n        }\n\n        this.pause();\n\n        // the error is really just that at least one of the requests timed-out\n        // set the bandwidth to a very low value and trigger an ABR switch to\n        // take emergency action\n        if (error.code === _mediaSegmentRequest.REQUEST_ERRORS.TIMEOUT) {\n          this.mediaRequestsTimedout += 1;\n          this.bandwidth = 1;\n          this.roundTrip = NaN;\n          this.trigger('bandwidthupdate');\n          return;\n        }\n\n        // if control-flow has arrived here, then the error is real\n        // emit an error event to blacklist the current playlist\n        this.mediaRequestsErrored += 1;\n        this.error(error);\n        this.trigger('error');\n        return;\n      }\n\n      // the response was a success so set any bandwidth stats the request\n      // generated for ABR purposes\n      this.bandwidth = simpleSegment.stats.bandwidth;\n      this.roundTrip = simpleSegment.stats.roundTripTime;\n\n      // if this request included an initialization segment, save that data\n      // to the initSegment cache\n      if (simpleSegment.map) {\n        simpleSegment.map = this.initSegment(simpleSegment.map, true);\n      }\n\n      this.processSegmentResponse_(simpleSegment);\n    }\n\n    /**\n     * Move any important data from the simplified segment object\n     * back to the real segment object for future phases\n     *\n     * @private\n     */\n  }, {\n    key: 'processSegmentResponse_',\n    value: function processSegmentResponse_(simpleSegment) {\n      var segmentInfo = this.pendingSegment_;\n\n      segmentInfo.bytes = simpleSegment.bytes;\n      if (simpleSegment.map) {\n        segmentInfo.segment.map.bytes = simpleSegment.map.bytes;\n      }\n\n      segmentInfo.endOfAllRequests = simpleSegment.endOfAllRequests;\n      this.handleSegment_();\n    }\n\n    /**\n     * append a decrypted segement to the SourceBuffer through a SourceUpdater\n     *\n     * @private\n     */\n  }, {\n    key: 'handleSegment_',\n    value: function handleSegment_() {\n      var _this3 = this;\n\n      if (!this.pendingSegment_) {\n        this.state = 'READY';\n        return;\n      }\n\n      var segmentInfo = this.pendingSegment_;\n      var segment = segmentInfo.segment;\n      var timingInfo = this.syncController_.probeSegmentInfo(segmentInfo);\n\n      // When we have our first timing info, determine what media types this loader is\n      // dealing with. Although we're maintaining extra state, it helps to preserve the\n      // separation of segment loader from the actual source buffers.\n      if (typeof this.startingMedia_ === 'undefined' && timingInfo && (\n      // Guard against cases where we're not getting timing info at all until we are\n      // certain that all streams will provide it.\n      timingInfo.containsAudio || timingInfo.containsVideo)) {\n        this.startingMedia_ = {\n          containsAudio: timingInfo.containsAudio,\n          containsVideo: timingInfo.containsVideo\n        };\n      }\n\n      var illegalMediaSwitchError = illegalMediaSwitch(this.loaderType_, this.startingMedia_, timingInfo);\n\n      if (illegalMediaSwitchError) {\n        this.error({\n          message: illegalMediaSwitchError,\n          blacklistDuration: Infinity\n        });\n        this.trigger('error');\n        return;\n      }\n\n      if (segmentInfo.isSyncRequest) {\n        this.trigger('syncinfoupdate');\n        this.pendingSegment_ = null;\n        this.state = 'READY';\n        return;\n      }\n\n      if (segmentInfo.timestampOffset !== null && segmentInfo.timestampOffset !== this.sourceUpdater_.timestampOffset()) {\n        this.sourceUpdater_.timestampOffset(segmentInfo.timestampOffset);\n        // fired when a timestamp offset is set in HLS (can also identify discontinuities)\n        this.trigger('timestampoffset');\n      }\n\n      var timelineMapping = this.syncController_.mappingForTimeline(segmentInfo.timeline);\n\n      if (timelineMapping !== null) {\n        this.trigger({\n          type: 'segmenttimemapping',\n          mapping: timelineMapping\n        });\n      }\n\n      this.state = 'APPENDING';\n\n      // if the media initialization segment is changing, append it\n      // before the content segment\n      if (segment.map) {\n        (function () {\n          var initId = (0, _binUtils.initSegmentId)(segment.map);\n\n          if (!_this3.activeInitSegmentId_ || _this3.activeInitSegmentId_ !== initId) {\n            var initSegment = _this3.initSegment(segment.map);\n\n            _this3.sourceUpdater_.appendBuffer(initSegment.bytes, function () {\n              _this3.activeInitSegmentId_ = initId;\n            });\n          }\n        })();\n      }\n\n      segmentInfo.byteLength = segmentInfo.bytes.byteLength;\n      if (typeof segment.start === 'number' && typeof segment.end === 'number') {\n        this.mediaSecondsLoaded += segment.end - segment.start;\n      } else {\n        this.mediaSecondsLoaded += segment.duration;\n      }\n\n      this.sourceUpdater_.appendBuffer(segmentInfo.bytes, this.handleUpdateEnd_.bind(this));\n    }\n\n    /**\n     * callback to run when appendBuffer is finished. detects if we are\n     * in a good state to do things with the data we got, or if we need\n     * to wait for more\n     *\n     * @private\n     */\n  }, {\n    key: 'handleUpdateEnd_',\n    value: function handleUpdateEnd_() {\n      this.logger_('handleUpdateEnd_', 'segmentInfo:', this.pendingSegment_);\n\n      if (!this.pendingSegment_) {\n        this.state = 'READY';\n        if (!this.paused()) {\n          this.monitorBuffer_();\n        }\n        return;\n      }\n\n      var segmentInfo = this.pendingSegment_;\n      var segment = segmentInfo.segment;\n      var isWalkingForward = this.mediaIndex !== null;\n\n      this.pendingSegment_ = null;\n      this.recordThroughput_(segmentInfo);\n      this.addSegmentMetadataCue_(segmentInfo);\n\n      this.state = 'READY';\n\n      this.mediaIndex = segmentInfo.mediaIndex;\n      this.fetchAtBuffer_ = true;\n      this.currentTimeline_ = segmentInfo.timeline;\n\n      // We must update the syncinfo to recalculate the seekable range before\n      // the following conditional otherwise it may consider this a bad \"guess\"\n      // and attempt to resync when the post-update seekable window and live\n      // point would mean that this was the perfect segment to fetch\n      this.trigger('syncinfoupdate');\n\n      // If we previously appended a segment that ends more than 3 targetDurations before\n      // the currentTime_ that means that our conservative guess was too conservative.\n      // In that case, reset the loader state so that we try to use any information gained\n      // from the previous request to create a new, more accurate, sync-point.\n      if (segment.end && this.currentTime_() - segment.end > segmentInfo.playlist.targetDuration * 3) {\n        this.resetEverything();\n        return;\n      }\n\n      // Don't do a rendition switch unless we have enough time to get a sync segment\n      // and conservatively guess\n      if (isWalkingForward) {\n        this.trigger('bandwidthupdate');\n      }\n      this.trigger('progress');\n\n      // any time an update finishes and the last segment is in the\n      // buffer, end the stream. this ensures the \"ended\" event will\n      // fire if playback reaches that point.\n      var isEndOfStream = detectEndOfStream(segmentInfo.playlist, this.mediaSource_, segmentInfo.mediaIndex + 1);\n\n      if (isEndOfStream) {\n        this.endOfStream();\n      }\n\n      if (!this.paused()) {\n        this.monitorBuffer_();\n      }\n    }\n\n    /**\n     * Records the current throughput of the decrypt, transmux, and append\n     * portion of the semgment pipeline. `throughput.rate` is a the cumulative\n     * moving average of the throughput. `throughput.count` is the number of\n     * data points in the average.\n     *\n     * @private\n     * @param {Object} segmentInfo the object returned by loadSegment\n     */\n  }, {\n    key: 'recordThroughput_',\n    value: function recordThroughput_(segmentInfo) {\n      var rate = this.throughput.rate;\n      // Add one to the time to ensure that we don't accidentally attempt to divide\n      // by zero in the case where the throughput is ridiculously high\n      var segmentProcessingTime = Date.now() - segmentInfo.endOfAllRequests + 1;\n      // Multiply by 8000 to convert from bytes/millisecond to bits/second\n      var segmentProcessingThroughput = Math.floor(segmentInfo.byteLength / segmentProcessingTime * 8 * 1000);\n\n      // This is just a cumulative moving average calculation:\n      //   newAvg = oldAvg + (sample - oldAvg) / (sampleCount + 1)\n      this.throughput.rate += (segmentProcessingThroughput - rate) / ++this.throughput.count;\n    }\n\n    /**\n     * A debugging logger noop that is set to console.log only if debugging\n     * is enabled globally\n     *\n     * @private\n     */\n  }, {\n    key: 'logger_',\n    value: function logger_() {}\n\n    /**\n     * Adds a cue to the segment-metadata track with some metadata information about the\n     * segment\n     *\n     * @private\n     * @param {Object} segmentInfo\n     *        the object returned by loadSegment\n     * @method addSegmentMetadataCue_\n     */\n  }, {\n    key: 'addSegmentMetadataCue_',\n    value: function addSegmentMetadataCue_(segmentInfo) {\n      if (!this.segmentMetadataTrack_) {\n        return;\n      }\n\n      var segment = segmentInfo.segment;\n      var start = segment.start;\n      var end = segment.end;\n\n      // Do not try adding the cue if the start and end times are invalid.\n      if (!finite(start) || !finite(end)) {\n        return;\n      }\n\n      (0, _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs2['default'])(start, end, this.segmentMetadataTrack_);\n\n      var Cue = _globalWindow2['default'].WebKitDataCue || _globalWindow2['default'].VTTCue;\n      var value = {\n        bandwidth: segmentInfo.playlist.attributes.BANDWIDTH,\n        resolution: segmentInfo.playlist.attributes.RESOLUTION,\n        codecs: segmentInfo.playlist.attributes.CODECS,\n        byteLength: segmentInfo.byteLength,\n        uri: segmentInfo.uri,\n        timeline: segmentInfo.timeline,\n        playlist: segmentInfo.playlist.uri,\n        start: start,\n        end: end\n      };\n      var data = JSON.stringify(value);\n      var cue = new Cue(start, end, data);\n\n      // Attach the metadata to the value property of the cue to keep consistency between\n      // the differences of WebKitDataCue in safari and VTTCue in other browsers\n      cue.value = value;\n\n      this.segmentMetadataTrack_.addCue(cue);\n    }\n  }]);\n\n  return SegmentLoader;\n})(_videoJs2['default'].EventTarget);\n\nexports['default'] = SegmentLoader;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/segment-loader.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/source-updater.js":
/*!****************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/source-updater.js ***!
  \****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file source-updater.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar noop = function noop() {};\n\n/**\n * A queue of callbacks to be serialized and applied when a\n * MediaSource and its associated SourceBuffers are not in the\n * updating state. It is used by the segment loader to update the\n * underlying SourceBuffers when new data is loaded, for instance.\n *\n * @class SourceUpdater\n * @param {MediaSource} mediaSource the MediaSource to create the\n * SourceBuffer from\n * @param {String} mimeType the desired MIME type of the underlying\n * SourceBuffer\n */\n\nvar SourceUpdater = (function () {\n  function SourceUpdater(mediaSource, mimeType) {\n    var _this = this;\n\n    _classCallCheck(this, SourceUpdater);\n\n    var createSourceBuffer = function createSourceBuffer() {\n      _this.sourceBuffer_ = mediaSource.addSourceBuffer(mimeType);\n\n      // run completion handlers and process callbacks as updateend\n      // events fire\n      _this.onUpdateendCallback_ = function () {\n        var pendingCallback = _this.pendingCallback_;\n\n        _this.pendingCallback_ = null;\n\n        if (pendingCallback) {\n          pendingCallback();\n        }\n\n        _this.runCallback_();\n      };\n\n      _this.sourceBuffer_.addEventListener('updateend', _this.onUpdateendCallback_);\n\n      _this.runCallback_();\n    };\n\n    this.callbacks_ = [];\n    this.pendingCallback_ = null;\n    this.timestampOffset_ = 0;\n    this.mediaSource = mediaSource;\n    this.processedAppend_ = false;\n\n    if (mediaSource.readyState === 'closed') {\n      mediaSource.addEventListener('sourceopen', createSourceBuffer);\n    } else {\n      createSourceBuffer();\n    }\n  }\n\n  /**\n   * Aborts the current segment and resets the segment parser.\n   *\n   * @param {Function} done function to call when done\n   * @see http://w3c.github.io/media-source/#widl-SourceBuffer-abort-void\n   */\n\n  _createClass(SourceUpdater, [{\n    key: 'abort',\n    value: function abort(done) {\n      var _this2 = this;\n\n      if (this.processedAppend_) {\n        this.queueCallback_(function () {\n          _this2.sourceBuffer_.abort();\n        }, done);\n      }\n    }\n\n    /**\n     * Queue an update to append an ArrayBuffer.\n     *\n     * @param {ArrayBuffer} bytes\n     * @param {Function} done the function to call when done\n     * @see http://www.w3.org/TR/media-source/#widl-SourceBuffer-appendBuffer-void-ArrayBuffer-data\n     */\n  }, {\n    key: 'appendBuffer',\n    value: function appendBuffer(bytes, done) {\n      var _this3 = this;\n\n      this.processedAppend_ = true;\n      this.queueCallback_(function () {\n        _this3.sourceBuffer_.appendBuffer(bytes);\n      }, done);\n    }\n\n    /**\n     * Indicates what TimeRanges are buffered in the managed SourceBuffer.\n     *\n     * @see http://www.w3.org/TR/media-source/#widl-SourceBuffer-buffered\n     */\n  }, {\n    key: 'buffered',\n    value: function buffered() {\n      if (!this.sourceBuffer_) {\n        return _videoJs2['default'].createTimeRanges();\n      }\n      return this.sourceBuffer_.buffered;\n    }\n\n    /**\n     * Queue an update to remove a time range from the buffer.\n     *\n     * @param {Number} start where to start the removal\n     * @param {Number} end where to end the removal\n     * @see http://www.w3.org/TR/media-source/#widl-SourceBuffer-remove-void-double-start-unrestricted-double-end\n     */\n  }, {\n    key: 'remove',\n    value: function remove(start, end) {\n      var _this4 = this;\n\n      if (this.processedAppend_) {\n        this.queueCallback_(function () {\n          _this4.sourceBuffer_.remove(start, end);\n        }, noop);\n      }\n    }\n\n    /**\n     * Whether the underlying sourceBuffer is updating or not\n     *\n     * @return {Boolean} the updating status of the SourceBuffer\n     */\n  }, {\n    key: 'updating',\n    value: function updating() {\n      return !this.sourceBuffer_ || this.sourceBuffer_.updating || this.pendingCallback_;\n    }\n\n    /**\n     * Set/get the timestampoffset on the SourceBuffer\n     *\n     * @return {Number} the timestamp offset\n     */\n  }, {\n    key: 'timestampOffset',\n    value: function timestampOffset(offset) {\n      var _this5 = this;\n\n      if (typeof offset !== 'undefined') {\n        this.queueCallback_(function () {\n          _this5.sourceBuffer_.timestampOffset = offset;\n        });\n        this.timestampOffset_ = offset;\n      }\n      return this.timestampOffset_;\n    }\n\n    /**\n     * Queue a callback to run\n     */\n  }, {\n    key: 'queueCallback_',\n    value: function queueCallback_(callback, done) {\n      this.callbacks_.push([callback.bind(this), done]);\n      this.runCallback_();\n    }\n\n    /**\n     * Run a queued callback\n     */\n  }, {\n    key: 'runCallback_',\n    value: function runCallback_() {\n      var callbacks = undefined;\n\n      if (!this.updating() && this.callbacks_.length) {\n        callbacks = this.callbacks_.shift();\n        this.pendingCallback_ = callbacks[1];\n        callbacks[0]();\n      }\n    }\n\n    /**\n     * dispose of the source updater and the underlying sourceBuffer\n     */\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      this.sourceBuffer_.removeEventListener('updateend', this.onUpdateendCallback_);\n      if (this.sourceBuffer_ && this.mediaSource.readyState === 'open') {\n        this.sourceBuffer_.abort();\n      }\n    }\n  }]);\n\n  return SourceUpdater;\n})();\n\nexports['default'] = SourceUpdater;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/source-updater.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/sync-controller.js":
/*!*****************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/sync-controller.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file sync-controller.js\n */\n\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x2, _x3, _x4) { var _again = true; _function: while (_again) { var object = _x2, property = _x3, receiver = _x4; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x2 = parent; _x3 = property; _x4 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _muxJsLibMp4Probe = __webpack_require__(/*! mux.js/lib/mp4/probe */ \"./node_modules/mux.js/lib/mp4/probe.js\");\n\nvar _muxJsLibMp4Probe2 = _interopRequireDefault(_muxJsLibMp4Probe);\n\nvar _muxJsLibToolsTsInspectorJs = __webpack_require__(/*! mux.js/lib/tools/ts-inspector.js */ \"./node_modules/mux.js/lib/tools/ts-inspector.js\");\n\nvar _playlist = __webpack_require__(/*! ./playlist */ \"./node_modules/videojs-contrib-hls/es5/playlist.js\");\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar syncPointStrategies = [\n// Stategy \"VOD\": Handle the VOD-case where the sync-point is *always*\n//                the equivalence display-time 0 === segment-index 0\n{\n  name: 'VOD',\n  run: function run(syncController, playlist, duration, currentTimeline, currentTime) {\n    if (duration !== Infinity) {\n      var syncPoint = {\n        time: 0,\n        segmentIndex: 0\n      };\n\n      return syncPoint;\n    }\n    return null;\n  }\n},\n// Stategy \"ProgramDateTime\": We have a program-date-time tag in this playlist\n{\n  name: 'ProgramDateTime',\n  run: function run(syncController, playlist, duration, currentTimeline, currentTime) {\n    if (syncController.datetimeToDisplayTime && playlist.dateTimeObject) {\n      var playlistTime = playlist.dateTimeObject.getTime() / 1000;\n      var playlistStart = playlistTime + syncController.datetimeToDisplayTime;\n      var syncPoint = {\n        time: playlistStart,\n        segmentIndex: 0\n      };\n\n      return syncPoint;\n    }\n    return null;\n  }\n},\n// Stategy \"Segment\": We have a known time mapping for a timeline and a\n//                    segment in the current timeline with timing data\n{\n  name: 'Segment',\n  run: function run(syncController, playlist, duration, currentTimeline, currentTime) {\n    var segments = playlist.segments || [];\n    var syncPoint = null;\n    var lastDistance = null;\n\n    currentTime = currentTime || 0;\n\n    for (var i = 0; i < segments.length; i++) {\n      var segment = segments[i];\n\n      if (segment.timeline === currentTimeline && typeof segment.start !== 'undefined') {\n        var distance = Math.abs(currentTime - segment.start);\n\n        // Once the distance begins to increase, we have passed\n        // currentTime and can stop looking for better candidates\n        if (lastDistance !== null && lastDistance < distance) {\n          break;\n        }\n\n        if (!syncPoint || lastDistance === null || lastDistance >= distance) {\n          lastDistance = distance;\n          syncPoint = {\n            time: segment.start,\n            segmentIndex: i\n          };\n        }\n      }\n    }\n    return syncPoint;\n  }\n},\n// Stategy \"Discontinuity\": We have a discontinuity with a known\n//                          display-time\n{\n  name: 'Discontinuity',\n  run: function run(syncController, playlist, duration, currentTimeline, currentTime) {\n    var syncPoint = null;\n\n    currentTime = currentTime || 0;\n\n    if (playlist.discontinuityStarts && playlist.discontinuityStarts.length) {\n      var lastDistance = null;\n\n      for (var i = 0; i < playlist.discontinuityStarts.length; i++) {\n        var segmentIndex = playlist.discontinuityStarts[i];\n        var discontinuity = playlist.discontinuitySequence + i + 1;\n        var discontinuitySync = syncController.discontinuities[discontinuity];\n\n        if (discontinuitySync) {\n          var distance = Math.abs(currentTime - discontinuitySync.time);\n\n          // Once the distance begins to increase, we have passed\n          // currentTime and can stop looking for better candidates\n          if (lastDistance !== null && lastDistance < distance) {\n            break;\n          }\n\n          if (!syncPoint || lastDistance === null || lastDistance >= distance) {\n            lastDistance = distance;\n            syncPoint = {\n              time: discontinuitySync.time,\n              segmentIndex: segmentIndex\n            };\n          }\n        }\n      }\n    }\n    return syncPoint;\n  }\n},\n// Stategy \"Playlist\": We have a playlist with a known mapping of\n//                     segment index to display time\n{\n  name: 'Playlist',\n  run: function run(syncController, playlist, duration, currentTimeline, currentTime) {\n    if (playlist.syncInfo) {\n      var syncPoint = {\n        time: playlist.syncInfo.time,\n        segmentIndex: playlist.syncInfo.mediaSequence - playlist.mediaSequence\n      };\n\n      return syncPoint;\n    }\n    return null;\n  }\n}];\n\nexports.syncPointStrategies = syncPointStrategies;\n\nvar SyncController = (function (_videojs$EventTarget) {\n  _inherits(SyncController, _videojs$EventTarget);\n\n  function SyncController() {\n    var options = arguments.length <= 0 || arguments[0] === undefined ? {} : arguments[0];\n\n    _classCallCheck(this, SyncController);\n\n    _get(Object.getPrototypeOf(SyncController.prototype), 'constructor', this).call(this);\n    // Segment Loader state variables...\n    // ...for synching across variants\n    this.inspectCache_ = undefined;\n\n    // ...for synching across variants\n    this.timelines = [];\n    this.discontinuities = [];\n    this.datetimeToDisplayTime = null;\n\n    if (options.debug) {\n      this.logger_ = _videoJs2['default'].log.bind(_videoJs2['default'], 'sync-controller ->');\n    }\n  }\n\n  /**\n   * Find a sync-point for the playlist specified\n   *\n   * A sync-point is defined as a known mapping from display-time to\n   * a segment-index in the current playlist.\n   *\n   * @param {Playlist} playlist\n   *        The playlist that needs a sync-point\n   * @param {Number} duration\n   *        Duration of the MediaSource (Infinite if playing a live source)\n   * @param {Number} currentTimeline\n   *        The last timeline from which a segment was loaded\n   * @returns {Object}\n   *          A sync-point object\n   */\n\n  _createClass(SyncController, [{\n    key: 'getSyncPoint',\n    value: function getSyncPoint(playlist, duration, currentTimeline, currentTime) {\n      var syncPoints = this.runStrategies_(playlist, duration, currentTimeline, currentTime);\n\n      if (!syncPoints.length) {\n        // Signal that we need to attempt to get a sync-point manually\n        // by fetching a segment in the playlist and constructing\n        // a sync-point from that information\n        return null;\n      }\n\n      // Now find the sync-point that is closest to the currentTime because\n      // that should result in the most accurate guess about which segment\n      // to fetch\n      return this.selectSyncPoint_(syncPoints, { key: 'time', value: currentTime });\n    }\n\n    /**\n     * Calculate the amount of time that has expired off the playlist during playback\n     *\n     * @param {Playlist} playlist\n     *        Playlist object to calculate expired from\n     * @param {Number} duration\n     *        Duration of the MediaSource (Infinity if playling a live source)\n     * @returns {Number|null}\n     *          The amount of time that has expired off the playlist during playback. Null\n     *          if no sync-points for the playlist can be found.\n     */\n  }, {\n    key: 'getExpiredTime',\n    value: function getExpiredTime(playlist, duration) {\n      if (!playlist || !playlist.segments) {\n        return null;\n      }\n\n      var syncPoints = this.runStrategies_(playlist, duration, playlist.discontinuitySequence, 0);\n\n      // Without sync-points, there is not enough information to determine the expired time\n      if (!syncPoints.length) {\n        return null;\n      }\n\n      var syncPoint = this.selectSyncPoint_(syncPoints, {\n        key: 'segmentIndex',\n        value: 0\n      });\n\n      // If the sync-point is beyond the start of the playlist, we want to subtract the\n      // duration from index 0 to syncPoint.segmentIndex instead of adding.\n      if (syncPoint.segmentIndex > 0) {\n        syncPoint.time *= -1;\n      }\n\n      return Math.abs(syncPoint.time + (0, _playlist.sumDurations)(playlist, syncPoint.segmentIndex, 0));\n    }\n\n    /**\n     * Runs each sync-point strategy and returns a list of sync-points returned by the\n     * strategies\n     *\n     * @private\n     * @param {Playlist} playlist\n     *        The playlist that needs a sync-point\n     * @param {Number} duration\n     *        Duration of the MediaSource (Infinity if playing a live source)\n     * @param {Number} currentTimeline\n     *        The last timeline from which a segment was loaded\n     * @returns {Array}\n     *          A list of sync-point objects\n     */\n  }, {\n    key: 'runStrategies_',\n    value: function runStrategies_(playlist, duration, currentTimeline, currentTime) {\n      var syncPoints = [];\n\n      // Try to find a sync-point in by utilizing various strategies...\n      for (var i = 0; i < syncPointStrategies.length; i++) {\n        var strategy = syncPointStrategies[i];\n        var syncPoint = strategy.run(this, playlist, duration, currentTimeline, currentTime);\n\n        if (syncPoint) {\n          syncPoint.strategy = strategy.name;\n          syncPoints.push({\n            strategy: strategy.name,\n            syncPoint: syncPoint\n          });\n          this.logger_('syncPoint found via <' + strategy.name + '>:', syncPoint);\n        }\n      }\n\n      return syncPoints;\n    }\n\n    /**\n     * Selects the sync-point nearest the specified target\n     *\n     * @private\n     * @param {Array} syncPoints\n     *        List of sync-points to select from\n     * @param {Object} target\n     *        Object specifying the property and value we are targeting\n     * @param {String} target.key\n     *        Specifies the property to target. Must be either 'time' or 'segmentIndex'\n     * @param {Number} target.value\n     *        The value to target for the specified key.\n     * @returns {Object}\n     *          The sync-point nearest the target\n     */\n  }, {\n    key: 'selectSyncPoint_',\n    value: function selectSyncPoint_(syncPoints, target) {\n      var bestSyncPoint = syncPoints[0].syncPoint;\n      var bestDistance = Math.abs(syncPoints[0].syncPoint[target.key] - target.value);\n      var bestStrategy = syncPoints[0].strategy;\n\n      for (var i = 1; i < syncPoints.length; i++) {\n        var newDistance = Math.abs(syncPoints[i].syncPoint[target.key] - target.value);\n\n        if (newDistance < bestDistance) {\n          bestDistance = newDistance;\n          bestSyncPoint = syncPoints[i].syncPoint;\n          bestStrategy = syncPoints[i].strategy;\n        }\n      }\n\n      this.logger_('syncPoint with strategy <' + bestStrategy + '> chosen: ', bestSyncPoint);\n      return bestSyncPoint;\n    }\n\n    /**\n     * Save any meta-data present on the segments when segments leave\n     * the live window to the playlist to allow for synchronization at the\n     * playlist level later.\n     *\n     * @param {Playlist} oldPlaylist - The previous active playlist\n     * @param {Playlist} newPlaylist - The updated and most current playlist\n     */\n  }, {\n    key: 'saveExpiredSegmentInfo',\n    value: function saveExpiredSegmentInfo(oldPlaylist, newPlaylist) {\n      var mediaSequenceDiff = newPlaylist.mediaSequence - oldPlaylist.mediaSequence;\n\n      // When a segment expires from the playlist and it has a start time\n      // save that information as a possible sync-point reference in future\n      for (var i = mediaSequenceDiff - 1; i >= 0; i--) {\n        var lastRemovedSegment = oldPlaylist.segments[i];\n\n        if (lastRemovedSegment && typeof lastRemovedSegment.start !== 'undefined') {\n          newPlaylist.syncInfo = {\n            mediaSequence: oldPlaylist.mediaSequence + i,\n            time: lastRemovedSegment.start\n          };\n          this.logger_('playlist sync:', newPlaylist.syncInfo);\n          this.trigger('syncinfoupdate');\n          break;\n        }\n      }\n    }\n\n    /**\n     * Save the mapping from playlist's ProgramDateTime to display. This should\n     * only ever happen once at the start of playback.\n     *\n     * @param {Playlist} playlist - The currently active playlist\n     */\n  }, {\n    key: 'setDateTimeMapping',\n    value: function setDateTimeMapping(playlist) {\n      if (!this.datetimeToDisplayTime && playlist.dateTimeObject) {\n        var playlistTimestamp = playlist.dateTimeObject.getTime() / 1000;\n\n        this.datetimeToDisplayTime = -playlistTimestamp;\n      }\n    }\n\n    /**\n     * Reset the state of the inspection cache when we do a rendition\n     * switch\n     */\n  }, {\n    key: 'reset',\n    value: function reset() {\n      this.inspectCache_ = undefined;\n    }\n\n    /**\n     * Probe or inspect a fmp4 or an mpeg2-ts segment to determine the start\n     * and end of the segment in it's internal \"media time\". Used to generate\n     * mappings from that internal \"media time\" to the display time that is\n     * shown on the player.\n     *\n     * @param {SegmentInfo} segmentInfo - The current active request information\n     */\n  }, {\n    key: 'probeSegmentInfo',\n    value: function probeSegmentInfo(segmentInfo) {\n      var segment = segmentInfo.segment;\n      var playlist = segmentInfo.playlist;\n      var timingInfo = undefined;\n\n      if (segment.map) {\n        timingInfo = this.probeMp4Segment_(segmentInfo);\n      } else {\n        timingInfo = this.probeTsSegment_(segmentInfo);\n      }\n\n      if (timingInfo) {\n        if (this.calculateSegmentTimeMapping_(segmentInfo, timingInfo)) {\n          this.saveDiscontinuitySyncInfo_(segmentInfo);\n\n          // If the playlist does not have sync information yet, record that information\n          // now with segment timing information\n          if (!playlist.syncInfo) {\n            playlist.syncInfo = {\n              mediaSequence: playlist.mediaSequence + segmentInfo.mediaIndex,\n              time: segment.start\n            };\n          }\n        }\n      }\n\n      return timingInfo;\n    }\n\n    /**\n     * Probe an fmp4 or an mpeg2-ts segment to determine the start of the segment\n     * in it's internal \"media time\".\n     *\n     * @private\n     * @param {SegmentInfo} segmentInfo - The current active request information\n     * @return {object} The start and end time of the current segment in \"media time\"\n     */\n  }, {\n    key: 'probeMp4Segment_',\n    value: function probeMp4Segment_(segmentInfo) {\n      var segment = segmentInfo.segment;\n      var timescales = _muxJsLibMp4Probe2['default'].timescale(segment.map.bytes);\n      var startTime = _muxJsLibMp4Probe2['default'].startTime(timescales, segmentInfo.bytes);\n\n      if (segmentInfo.timestampOffset !== null) {\n        segmentInfo.timestampOffset -= startTime;\n      }\n\n      return {\n        start: startTime,\n        end: startTime + segment.duration\n      };\n    }\n\n    /**\n     * Probe an mpeg2-ts segment to determine the start and end of the segment\n     * in it's internal \"media time\".\n     *\n     * @private\n     * @param {SegmentInfo} segmentInfo - The current active request information\n     * @return {object} The start and end time of the current segment in \"media time\"\n     */\n  }, {\n    key: 'probeTsSegment_',\n    value: function probeTsSegment_(segmentInfo) {\n      var timeInfo = (0, _muxJsLibToolsTsInspectorJs.inspect)(segmentInfo.bytes, this.inspectCache_);\n      var segmentStartTime = undefined;\n      var segmentEndTime = undefined;\n\n      if (!timeInfo) {\n        return null;\n      }\n\n      if (timeInfo.video && timeInfo.video.length === 2) {\n        this.inspectCache_ = timeInfo.video[1].dts;\n        segmentStartTime = timeInfo.video[0].dtsTime;\n        segmentEndTime = timeInfo.video[1].dtsTime;\n      } else if (timeInfo.audio && timeInfo.audio.length === 2) {\n        this.inspectCache_ = timeInfo.audio[1].dts;\n        segmentStartTime = timeInfo.audio[0].dtsTime;\n        segmentEndTime = timeInfo.audio[1].dtsTime;\n      }\n\n      return {\n        start: segmentStartTime,\n        end: segmentEndTime,\n        containsVideo: timeInfo.video && timeInfo.video.length === 2,\n        containsAudio: timeInfo.audio && timeInfo.audio.length === 2\n      };\n    }\n  }, {\n    key: 'timestampOffsetForTimeline',\n    value: function timestampOffsetForTimeline(timeline) {\n      if (typeof this.timelines[timeline] === 'undefined') {\n        return null;\n      }\n      return this.timelines[timeline].time;\n    }\n  }, {\n    key: 'mappingForTimeline',\n    value: function mappingForTimeline(timeline) {\n      if (typeof this.timelines[timeline] === 'undefined') {\n        return null;\n      }\n      return this.timelines[timeline].mapping;\n    }\n\n    /**\n     * Use the \"media time\" for a segment to generate a mapping to \"display time\" and\n     * save that display time to the segment.\n     *\n     * @private\n     * @param {SegmentInfo} segmentInfo\n     *        The current active request information\n     * @param {object} timingInfo\n     *        The start and end time of the current segment in \"media time\"\n     * @returns {Boolean}\n     *          Returns false if segment time mapping could not be calculated\n     */\n  }, {\n    key: 'calculateSegmentTimeMapping_',\n    value: function calculateSegmentTimeMapping_(segmentInfo, timingInfo) {\n      var segment = segmentInfo.segment;\n      var mappingObj = this.timelines[segmentInfo.timeline];\n\n      if (segmentInfo.timestampOffset !== null) {\n        this.logger_('tsO:', segmentInfo.timestampOffset);\n\n        mappingObj = {\n          time: segmentInfo.startOfSegment,\n          mapping: segmentInfo.startOfSegment - timingInfo.start\n        };\n        this.timelines[segmentInfo.timeline] = mappingObj;\n        this.trigger('timestampoffset');\n\n        segment.start = segmentInfo.startOfSegment;\n        segment.end = timingInfo.end + mappingObj.mapping;\n      } else if (mappingObj) {\n        segment.start = timingInfo.start + mappingObj.mapping;\n        segment.end = timingInfo.end + mappingObj.mapping;\n      } else {\n        return false;\n      }\n\n      return true;\n    }\n\n    /**\n     * Each time we have discontinuity in the playlist, attempt to calculate the location\n     * in display of the start of the discontinuity and save that. We also save an accuracy\n     * value so that we save values with the most accuracy (closest to 0.)\n     *\n     * @private\n     * @param {SegmentInfo} segmentInfo - The current active request information\n     */\n  }, {\n    key: 'saveDiscontinuitySyncInfo_',\n    value: function saveDiscontinuitySyncInfo_(segmentInfo) {\n      var playlist = segmentInfo.playlist;\n      var segment = segmentInfo.segment;\n\n      // If the current segment is a discontinuity then we know exactly where\n      // the start of the range and it's accuracy is 0 (greater accuracy values\n      // mean more approximation)\n      if (segment.discontinuity) {\n        this.discontinuities[segment.timeline] = {\n          time: segment.start,\n          accuracy: 0\n        };\n      } else if (playlist.discontinuityStarts.length) {\n        // Search for future discontinuities that we can provide better timing\n        // information for and save that information for sync purposes\n        for (var i = 0; i < playlist.discontinuityStarts.length; i++) {\n          var segmentIndex = playlist.discontinuityStarts[i];\n          var discontinuity = playlist.discontinuitySequence + i + 1;\n          var mediaIndexDiff = segmentIndex - segmentInfo.mediaIndex;\n          var accuracy = Math.abs(mediaIndexDiff);\n\n          if (!this.discontinuities[discontinuity] || this.discontinuities[discontinuity].accuracy > accuracy) {\n            var time = undefined;\n\n            if (mediaIndexDiff < 0) {\n              time = segment.start - (0, _playlist.sumDurations)(playlist, segmentInfo.mediaIndex, segmentIndex);\n            } else {\n              time = segment.end + (0, _playlist.sumDurations)(playlist, segmentInfo.mediaIndex + 1, segmentIndex);\n            }\n\n            this.discontinuities[discontinuity] = {\n              time: time,\n              accuracy: accuracy\n            };\n          }\n        }\n      }\n    }\n\n    /**\n     * A debugging logger noop that is set to console.log only if debugging\n     * is enabled globally\n     *\n     * @private\n     */\n  }, {\n    key: 'logger_',\n    value: function logger_() {}\n  }]);\n\n  return SyncController;\n})(_videoJs2['default'].EventTarget);\n\nexports['default'] = SyncController;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/sync-controller.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/util/codecs.js":
/*!*************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/util/codecs.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n/**\n * @file - codecs.js - Handles tasks regarding codec strings such as translating them to\n * codec strings, or translating codec strings into objects that can be examined.\n */\n\n/**\n * Parses a codec string to retrieve the number of codecs specified,\n * the video codec and object type indicator, and the audio profile.\n */\n\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\nvar parseCodecs = function parseCodecs() {\n  var codecs = arguments.length <= 0 || arguments[0] === undefined ? '' : arguments[0];\n\n  var result = {\n    codecCount: 0\n  };\n  var parsed = undefined;\n\n  result.codecCount = codecs.split(',').length;\n  result.codecCount = result.codecCount || 2;\n\n  // parse the video codec\n  parsed = /(^|\\s|,)+(avc1)([^ ,]*)/i.exec(codecs);\n  if (parsed) {\n    result.videoCodec = parsed[2];\n    result.videoObjectTypeIndicator = parsed[3];\n  }\n\n  // parse the last field of the audio codec\n  result.audioProfile = /(^|\\s|,)+mp4a.[0-9A-Fa-f]+\\.([0-9A-Fa-f]+)/i.exec(codecs);\n  result.audioProfile = result.audioProfile && result.audioProfile[2];\n\n  return result;\n};\nexports.parseCodecs = parseCodecs;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/util/codecs.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/videojs-contrib-hls.js":
/*!*********************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/videojs-contrib-hls.js ***!
  \*********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file videojs-contrib-hls.js\n *\n * The main file for the HLS project.\n * License: https://github.com/videojs/videojs-contrib-hls/blob/master/LICENSE\n */\n\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x4, _x5, _x6) { var _again = true; _function: while (_again) { var object = _x4, property = _x5, receiver = _x6; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x4 = parent; _x5 = property; _x6 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _globalDocument = __webpack_require__(/*! global/document */ \"./node_modules/global/document.js\");\n\nvar _globalDocument2 = _interopRequireDefault(_globalDocument);\n\nvar _playlistLoader = __webpack_require__(/*! ./playlist-loader */ \"./node_modules/videojs-contrib-hls/es5/playlist-loader.js\");\n\nvar _playlistLoader2 = _interopRequireDefault(_playlistLoader);\n\nvar _playlist = __webpack_require__(/*! ./playlist */ \"./node_modules/videojs-contrib-hls/es5/playlist.js\");\n\nvar _playlist2 = _interopRequireDefault(_playlist);\n\nvar _xhr = __webpack_require__(/*! ./xhr */ \"./node_modules/videojs-contrib-hls/es5/xhr.js\");\n\nvar _xhr2 = _interopRequireDefault(_xhr);\n\nvar _aesDecrypter = __webpack_require__(/*! aes-decrypter */ \"./node_modules/aes-decrypter/es5/index.js\");\n\nvar _binUtils = __webpack_require__(/*! ./bin-utils */ \"./node_modules/videojs-contrib-hls/es5/bin-utils.js\");\n\nvar _binUtils2 = _interopRequireDefault(_binUtils);\n\nvar _videojsContribMediaSources = __webpack_require__(/*! videojs-contrib-media-sources */ \"./node_modules/videojs-contrib-media-sources/es5/videojs-contrib-media-sources.js\");\n\nvar _m3u8Parser = __webpack_require__(/*! m3u8-parser */ \"./node_modules/m3u8-parser/es5/index.js\");\n\nvar _m3u8Parser2 = _interopRequireDefault(_m3u8Parser);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _masterPlaylistController = __webpack_require__(/*! ./master-playlist-controller */ \"./node_modules/videojs-contrib-hls/es5/master-playlist-controller.js\");\n\nvar _config = __webpack_require__(/*! ./config */ \"./node_modules/videojs-contrib-hls/es5/config.js\");\n\nvar _config2 = _interopRequireDefault(_config);\n\nvar _renditionMixin = __webpack_require__(/*! ./rendition-mixin */ \"./node_modules/videojs-contrib-hls/es5/rendition-mixin.js\");\n\nvar _renditionMixin2 = _interopRequireDefault(_renditionMixin);\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _playbackWatcher = __webpack_require__(/*! ./playback-watcher */ \"./node_modules/videojs-contrib-hls/es5/playback-watcher.js\");\n\nvar _playbackWatcher2 = _interopRequireDefault(_playbackWatcher);\n\nvar _reloadSourceOnError = __webpack_require__(/*! ./reload-source-on-error */ \"./node_modules/videojs-contrib-hls/es5/reload-source-on-error.js\");\n\nvar _reloadSourceOnError2 = _interopRequireDefault(_reloadSourceOnError);\n\nvar _playlistSelectorsJs = __webpack_require__(/*! ./playlist-selectors.js */ \"./node_modules/videojs-contrib-hls/es5/playlist-selectors.js\");\n\nvar Hls = {\n  PlaylistLoader: _playlistLoader2['default'],\n  Playlist: _playlist2['default'],\n  Decrypter: _aesDecrypter.Decrypter,\n  AsyncStream: _aesDecrypter.AsyncStream,\n  decrypt: _aesDecrypter.decrypt,\n  utils: _binUtils2['default'],\n\n  STANDARD_PLAYLIST_SELECTOR: _playlistSelectorsJs.lastBandwidthSelector,\n  INITIAL_PLAYLIST_SELECTOR: _playlistSelectorsJs.lowestBitrateCompatibleVariantSelector,\n  comparePlaylistBandwidth: _playlistSelectorsJs.comparePlaylistBandwidth,\n  comparePlaylistResolution: _playlistSelectorsJs.comparePlaylistResolution,\n\n  xhr: (0, _xhr2['default'])()\n};\n\n// 0.5 MB/s\nvar INITIAL_BANDWIDTH = 4194304;\n\n// Define getter/setters for config properites\n['GOAL_BUFFER_LENGTH', 'MAX_GOAL_BUFFER_LENGTH', 'GOAL_BUFFER_LENGTH_RATE', 'BUFFER_LOW_WATER_LINE', 'MAX_BUFFER_LOW_WATER_LINE', 'BUFFER_LOW_WATER_LINE_RATE', 'BANDWIDTH_VARIANCE'].forEach(function (prop) {\n  Object.defineProperty(Hls, prop, {\n    get: function get() {\n      _videoJs2['default'].log.warn('using Hls.' + prop + ' is UNSAFE be sure you know what you are doing');\n      return _config2['default'][prop];\n    },\n    set: function set(value) {\n      _videoJs2['default'].log.warn('using Hls.' + prop + ' is UNSAFE be sure you know what you are doing');\n\n      if (typeof value !== 'number' || value < 0) {\n        _videoJs2['default'].log.warn('value of Hls.' + prop + ' must be greater than or equal to 0');\n        return;\n      }\n\n      _config2['default'][prop] = value;\n    }\n  });\n});\n\n/**\n * Updates the selectedIndex of the QualityLevelList when a mediachange happens in hls.\n *\n * @param {QualityLevelList} qualityLevels The QualityLevelList to update.\n * @param {PlaylistLoader} playlistLoader PlaylistLoader containing the new media info.\n * @function handleHlsMediaChange\n */\nvar handleHlsMediaChange = function handleHlsMediaChange(qualityLevels, playlistLoader) {\n  var newPlaylist = playlistLoader.media();\n  var selectedIndex = -1;\n\n  for (var i = 0; i < qualityLevels.length; i++) {\n    if (qualityLevels[i].id === newPlaylist.uri) {\n      selectedIndex = i;\n      break;\n    }\n  }\n\n  qualityLevels.selectedIndex_ = selectedIndex;\n  qualityLevels.trigger({\n    selectedIndex: selectedIndex,\n    type: 'change'\n  });\n};\n\n/**\n * Adds quality levels to list once playlist metadata is available\n *\n * @param {QualityLevelList} qualityLevels The QualityLevelList to attach events to.\n * @param {Object} hls Hls object to listen to for media events.\n * @function handleHlsLoadedMetadata\n */\nvar handleHlsLoadedMetadata = function handleHlsLoadedMetadata(qualityLevels, hls) {\n  hls.representations().forEach(function (rep) {\n    qualityLevels.addQualityLevel(rep);\n  });\n  handleHlsMediaChange(qualityLevels, hls.playlists);\n};\n\n// HLS is a source handler, not a tech. Make sure attempts to use it\n// as one do not cause exceptions.\nHls.canPlaySource = function () {\n  return _videoJs2['default'].log.warn('HLS is no longer a tech. Please remove it from ' + 'your player\\'s techOrder.');\n};\n\n/**\n * Whether the browser has built-in HLS support.\n */\nHls.supportsNativeHls = (function () {\n  var video = _globalDocument2['default'].createElement('video');\n\n  // native HLS is definitely not supported if HTML5 video isn't\n  if (!_videoJs2['default'].getTech('Html5').isSupported()) {\n    return false;\n  }\n\n  // HLS manifests can go by many mime-types\n  var canPlay = [\n  // Apple santioned\n  'application/vnd.apple.mpegurl',\n  // Apple sanctioned for backwards compatibility\n  'audio/mpegurl',\n  // Very common\n  'audio/x-mpegurl',\n  // Very common\n  'application/x-mpegurl',\n  // Included for completeness\n  'video/x-mpegurl', 'video/mpegurl', 'application/mpegurl'];\n\n  return canPlay.some(function (canItPlay) {\n    return (/maybe|probably/i.test(video.canPlayType(canItPlay))\n    );\n  });\n})();\n\n/**\n * HLS is a source handler, not a tech. Make sure attempts to use it\n * as one do not cause exceptions.\n */\nHls.isSupported = function () {\n  return _videoJs2['default'].log.warn('HLS is no longer a tech. Please remove it from ' + 'your player\\'s techOrder.');\n};\n\nvar Component = _videoJs2['default'].getComponent('Component');\n\n/**\n * The Hls Handler object, where we orchestrate all of the parts\n * of HLS to interact with video.js\n *\n * @class HlsHandler\n * @extends videojs.Component\n * @param {Object} source the soruce object\n * @param {Tech} tech the parent tech object\n * @param {Object} options optional and required options\n */\n\nvar HlsHandler = (function (_Component) {\n  _inherits(HlsHandler, _Component);\n\n  function HlsHandler(source, tech, options) {\n    var _this = this;\n\n    _classCallCheck(this, HlsHandler);\n\n    _get(Object.getPrototypeOf(HlsHandler.prototype), 'constructor', this).call(this, tech, options.hls);\n\n    // tech.player() is deprecated but setup a reference to HLS for\n    // backwards-compatibility\n    if (tech.options_ && tech.options_.playerId) {\n      var _player = (0, _videoJs2['default'])(tech.options_.playerId);\n\n      if (!_player.hasOwnProperty('hls')) {\n        Object.defineProperty(_player, 'hls', {\n          get: function get() {\n            _videoJs2['default'].log.warn('player.hls is deprecated. Use player.tech_.hls instead.');\n            tech.trigger({ type: 'usage', name: 'hls-player-access' });\n            return _this;\n          }\n        });\n      }\n    }\n\n    this.tech_ = tech;\n    this.source_ = source;\n    this.stats = {};\n    this.ignoreNextSeekingEvent_ = false;\n    this.setOptions_();\n\n    // overriding native HLS only works if audio tracks have been emulated\n    // error early if we're misconfigured:\n    if (this.options_.overrideNative && (tech.featuresNativeVideoTracks || tech.featuresNativeAudioTracks)) {\n      throw new Error('Overriding native HLS requires emulated tracks. ' + 'See https://git.io/vMpjB');\n    }\n\n    // listen for fullscreenchange events for this player so that we\n    // can adjust our quality selection quickly\n    this.on(_globalDocument2['default'], ['fullscreenchange', 'webkitfullscreenchange', 'mozfullscreenchange', 'MSFullscreenChange'], function (event) {\n      var fullscreenElement = _globalDocument2['default'].fullscreenElement || _globalDocument2['default'].webkitFullscreenElement || _globalDocument2['default'].mozFullScreenElement || _globalDocument2['default'].msFullscreenElement;\n\n      if (fullscreenElement && fullscreenElement.contains(_this.tech_.el())) {\n        _this.masterPlaylistController_.fastQualityChange_();\n      }\n    });\n\n    this.on(this.tech_, 'seeking', function () {\n      if (this.ignoreNextSeekingEvent_) {\n        this.ignoreNextSeekingEvent_ = false;\n        return;\n      }\n\n      this.setCurrentTime(this.tech_.currentTime());\n    });\n    this.on(this.tech_, 'error', function () {\n      if (this.masterPlaylistController_) {\n        this.masterPlaylistController_.pauseLoading();\n      }\n    });\n\n    this.on(this.tech_, 'play', this.play);\n  }\n\n  /**\n   * The Source Handler object, which informs video.js what additional\n   * MIME types are supported and sets up playback. It is registered\n   * automatically to the appropriate tech based on the capabilities of\n   * the browser it is running in. It is not necessary to use or modify\n   * this object in normal usage.\n   */\n\n  _createClass(HlsHandler, [{\n    key: 'setOptions_',\n    value: function setOptions_() {\n      var _this2 = this;\n\n      // defaults\n      this.options_.withCredentials = this.options_.withCredentials || false;\n\n      if (typeof this.options_.blacklistDuration !== 'number') {\n        this.options_.blacklistDuration = 5 * 60;\n      }\n\n      // start playlist selection at a reasonable bandwidth for\n      // broadband internet (0.5 MB/s) or mobile (0.0625 MB/s)\n      if (typeof this.options_.bandwidth !== 'number') {\n        this.options_.bandwidth = INITIAL_BANDWIDTH;\n      }\n\n      // If the bandwidth number is unchanged from the initial setting\n      // then this takes precedence over the enableLowInitialPlaylist option\n      this.options_.enableLowInitialPlaylist = this.options_.enableLowInitialPlaylist && this.options_.bandwidth === INITIAL_BANDWIDTH;\n\n      // grab options passed to player.src\n      ['withCredentials', 'bandwidth', 'handleManifestRedirects'].forEach(function (option) {\n        if (typeof _this2.source_[option] !== 'undefined') {\n          _this2.options_[option] = _this2.source_[option];\n        }\n      });\n\n      this.bandwidth = this.options_.bandwidth;\n    }\n\n    /**\n     * called when player.src gets called, handle a new source\n     *\n     * @param {Object} src the source object to handle\n     */\n  }, {\n    key: 'src',\n    value: function src(_src) {\n      var _this3 = this;\n\n      // do nothing if the src is falsey\n      if (!_src) {\n        return;\n      }\n      this.setOptions_();\n      // add master playlist controller options\n      this.options_.url = this.source_.src;\n      this.options_.tech = this.tech_;\n      this.options_.externHls = Hls;\n\n      this.masterPlaylistController_ = new _masterPlaylistController.MasterPlaylistController(this.options_);\n      this.playbackWatcher_ = new _playbackWatcher2['default'](_videoJs2['default'].mergeOptions(this.options_, {\n        seekable: function seekable() {\n          return _this3.seekable();\n        }\n      }));\n\n      this.masterPlaylistController_.on('error', function () {\n        var player = _videoJs2['default'].players[_this3.tech_.options_.playerId];\n\n        player.error(_this3.masterPlaylistController_.error);\n      });\n\n      // `this` in selectPlaylist should be the HlsHandler for backwards\n      // compatibility with < v2\n      this.masterPlaylistController_.selectPlaylist = this.selectPlaylist ? this.selectPlaylist.bind(this) : Hls.STANDARD_PLAYLIST_SELECTOR.bind(this);\n\n      this.masterPlaylistController_.selectInitialPlaylist = Hls.INITIAL_PLAYLIST_SELECTOR.bind(this);\n\n      // re-expose some internal objects for backwards compatibility with < v2\n      this.playlists = this.masterPlaylistController_.masterPlaylistLoader_;\n      this.mediaSource = this.masterPlaylistController_.mediaSource;\n\n      // Proxy assignment of some properties to the master playlist\n      // controller. Using a custom property for backwards compatibility\n      // with < v2\n      Object.defineProperties(this, {\n        selectPlaylist: {\n          get: function get() {\n            return this.masterPlaylistController_.selectPlaylist;\n          },\n          set: function set(selectPlaylist) {\n            this.masterPlaylistController_.selectPlaylist = selectPlaylist.bind(this);\n          }\n        },\n        throughput: {\n          get: function get() {\n            return this.masterPlaylistController_.mainSegmentLoader_.throughput.rate;\n          },\n          set: function set(throughput) {\n            this.masterPlaylistController_.mainSegmentLoader_.throughput.rate = throughput;\n            // By setting `count` to 1 the throughput value becomes the starting value\n            // for the cumulative average\n            this.masterPlaylistController_.mainSegmentLoader_.throughput.count = 1;\n          }\n        },\n        bandwidth: {\n          get: function get() {\n            return this.masterPlaylistController_.mainSegmentLoader_.bandwidth;\n          },\n          set: function set(bandwidth) {\n            this.masterPlaylistController_.mainSegmentLoader_.bandwidth = bandwidth;\n            // setting the bandwidth manually resets the throughput counter\n            // `count` is set to zero that current value of `rate` isn't included\n            // in the cumulative average\n            this.masterPlaylistController_.mainSegmentLoader_.throughput = {\n              rate: 0,\n              count: 0\n            };\n          }\n        },\n        /**\n         * `systemBandwidth` is a combination of two serial processes bit-rates. The first\n         * is the network bitrate provided by `bandwidth` and the second is the bitrate of\n         * the entire process after that - decryption, transmuxing, and appending - provided\n         * by `throughput`.\n         *\n         * Since the two process are serial, the overall system bandwidth is given by:\n         *   sysBandwidth = 1 / (1 / bandwidth + 1 / throughput)\n         */\n        systemBandwidth: {\n          get: function get() {\n            var invBandwidth = 1 / (this.bandwidth || 1);\n            var invThroughput = undefined;\n\n            if (this.throughput > 0) {\n              invThroughput = 1 / this.throughput;\n            } else {\n              invThroughput = 0;\n            }\n\n            var systemBitrate = Math.floor(1 / (invBandwidth + invThroughput));\n\n            return systemBitrate;\n          },\n          set: function set() {\n            _videoJs2['default'].log.error('The \"systemBandwidth\" property is read-only');\n          }\n        }\n      });\n\n      Object.defineProperties(this.stats, {\n        bandwidth: {\n          get: function get() {\n            return _this3.bandwidth || 0;\n          },\n          enumerable: true\n        },\n        mediaRequests: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaRequests_() || 0;\n          },\n          enumerable: true\n        },\n        mediaRequestsAborted: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaRequestsAborted_() || 0;\n          },\n          enumerable: true\n        },\n        mediaRequestsTimedout: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaRequestsTimedout_() || 0;\n          },\n          enumerable: true\n        },\n        mediaRequestsErrored: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaRequestsErrored_() || 0;\n          },\n          enumerable: true\n        },\n        mediaTransferDuration: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaTransferDuration_() || 0;\n          },\n          enumerable: true\n        },\n        mediaBytesTransferred: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaBytesTransferred_() || 0;\n          },\n          enumerable: true\n        },\n        mediaSecondsLoaded: {\n          get: function get() {\n            return _this3.masterPlaylistController_.mediaSecondsLoaded_() || 0;\n          },\n          enumerable: true\n        }\n      });\n\n      this.tech_.one('canplay', this.masterPlaylistController_.setupFirstPlay.bind(this.masterPlaylistController_));\n\n      this.masterPlaylistController_.on('selectedinitialmedia', function () {\n        // Add the manual rendition mix-in to HlsHandler\n        (0, _renditionMixin2['default'])(_this3);\n      });\n\n      // the bandwidth of the primary segment loader is our best\n      // estimate of overall bandwidth\n      this.on(this.masterPlaylistController_, 'progress', function () {\n        this.tech_.trigger('progress');\n      });\n\n      // In the live case, we need to ignore the very first `seeking` event since\n      // that will be the result of the seek-to-live behavior\n      this.on(this.masterPlaylistController_, 'firstplay', function () {\n        this.ignoreNextSeekingEvent_ = true;\n      });\n\n      this.tech_.ready(function () {\n        return _this3.setupQualityLevels_();\n      });\n\n      // do nothing if the tech has been disposed already\n      // this can occur if someone sets the src in player.ready(), for instance\n      if (!this.tech_.el()) {\n        return;\n      }\n\n      this.tech_.src(_videoJs2['default'].URL.createObjectURL(this.masterPlaylistController_.mediaSource));\n    }\n\n    /**\n     * Initializes the quality levels and sets listeners to update them.\n     *\n     * @method setupQualityLevels_\n     * @private\n     */\n  }, {\n    key: 'setupQualityLevels_',\n    value: function setupQualityLevels_() {\n      var _this4 = this;\n\n      var player = _videoJs2['default'].players[this.tech_.options_.playerId];\n\n      if (player && player.qualityLevels) {\n        this.qualityLevels_ = player.qualityLevels();\n\n        this.masterPlaylistController_.on('selectedinitialmedia', function () {\n          handleHlsLoadedMetadata(_this4.qualityLevels_, _this4);\n        });\n\n        this.playlists.on('mediachange', function () {\n          handleHlsMediaChange(_this4.qualityLevels_, _this4.playlists);\n        });\n      }\n    }\n\n    /**\n     * Begin playing the video.\n     */\n  }, {\n    key: 'play',\n    value: function play() {\n      this.masterPlaylistController_.play();\n    }\n\n    /**\n     * a wrapper around the function in MasterPlaylistController\n     */\n  }, {\n    key: 'setCurrentTime',\n    value: function setCurrentTime(currentTime) {\n      this.masterPlaylistController_.setCurrentTime(currentTime);\n    }\n\n    /**\n     * a wrapper around the function in MasterPlaylistController\n     */\n  }, {\n    key: 'duration',\n    value: function duration() {\n      return this.masterPlaylistController_.duration();\n    }\n\n    /**\n     * a wrapper around the function in MasterPlaylistController\n     */\n  }, {\n    key: 'seekable',\n    value: function seekable() {\n      return this.masterPlaylistController_.seekable();\n    }\n\n    /**\n    * Abort all outstanding work and cleanup.\n    */\n  }, {\n    key: 'dispose',\n    value: function dispose() {\n      if (this.playbackWatcher_) {\n        this.playbackWatcher_.dispose();\n      }\n      if (this.masterPlaylistController_) {\n        this.masterPlaylistController_.dispose();\n      }\n      if (this.qualityLevels_) {\n        this.qualityLevels_.dispose();\n      }\n      _get(Object.getPrototypeOf(HlsHandler.prototype), 'dispose', this).call(this);\n    }\n  }]);\n\n  return HlsHandler;\n})(Component);\n\nvar HlsSourceHandler = function HlsSourceHandler(mode) {\n  return {\n    canHandleSource: function canHandleSource(srcObj) {\n      var options = arguments.length <= 1 || arguments[1] === undefined ? {} : arguments[1];\n\n      var localOptions = _videoJs2['default'].mergeOptions(_videoJs2['default'].options, options);\n\n      // this forces video.js to skip this tech/mode if its not the one we have been\n      // overriden to use, by returing that we cannot handle the source.\n      if (localOptions.hls && localOptions.hls.mode && localOptions.hls.mode !== mode) {\n        return false;\n      }\n      return HlsSourceHandler.canPlayType(srcObj.type, localOptions);\n    },\n    handleSource: function handleSource(source, tech) {\n      var options = arguments.length <= 2 || arguments[2] === undefined ? {} : arguments[2];\n\n      var localOptions = _videoJs2['default'].mergeOptions(_videoJs2['default'].options, options, { hls: { mode: mode } });\n\n      if (mode === 'flash') {\n        // We need to trigger this asynchronously to give others the chance\n        // to bind to the event when a source is set at player creation\n        tech.setTimeout(function () {\n          tech.trigger('loadstart');\n        }, 1);\n      }\n\n      tech.hls = new HlsHandler(source, tech, localOptions);\n      tech.hls.xhr = (0, _xhr2['default'])();\n\n      tech.hls.src(source.src);\n      return tech.hls;\n    },\n    canPlayType: function canPlayType(type) {\n      var options = arguments.length <= 1 || arguments[1] === undefined ? {} : arguments[1];\n\n      var localOptions = _videoJs2['default'].mergeOptions(_videoJs2['default'].options, options);\n\n      if (HlsSourceHandler.canPlayType(type, localOptions)) {\n        return 'maybe';\n      }\n      return '';\n    }\n  };\n};\n\nHlsSourceHandler.canPlayType = function (type, options) {\n  // No support for IE 10 or below\n  if (_videoJs2['default'].browser.IE_VERSION && _videoJs2['default'].browser.IE_VERSION <= 10) {\n    return false;\n  }\n\n  var mpegurlRE = /^(audio|video|application)\\/(x-|vnd\\.apple\\.)?mpegurl/i;\n\n  // favor native HLS support if it's available\n  if (!options.hls.overrideNative && Hls.supportsNativeHls) {\n    return false;\n  }\n  return mpegurlRE.test(type);\n};\n\nif (typeof _videoJs2['default'].MediaSource === 'undefined' || typeof _videoJs2['default'].URL === 'undefined') {\n  _videoJs2['default'].MediaSource = _videojsContribMediaSources.MediaSource;\n  _videoJs2['default'].URL = _videojsContribMediaSources.URL;\n}\n\nvar flashTech = _videoJs2['default'].getTech('Flash');\n\n// register source handlers with the appropriate techs\nif (_videojsContribMediaSources.MediaSource.supportsNativeMediaSources()) {\n  _videoJs2['default'].getTech('Html5').registerSourceHandler(HlsSourceHandler('html5'), 0);\n}\nif (_globalWindow2['default'].Uint8Array && flashTech) {\n  flashTech.registerSourceHandler(HlsSourceHandler('flash'));\n}\n\n_videoJs2['default'].HlsHandler = HlsHandler;\n_videoJs2['default'].HlsSourceHandler = HlsSourceHandler;\n_videoJs2['default'].Hls = Hls;\nif (!_videoJs2['default'].use) {\n  _videoJs2['default'].registerComponent('Hls', Hls);\n}\n_videoJs2['default'].m3u8 = _m3u8Parser2['default'];\n_videoJs2['default'].options.hls = _videoJs2['default'].options.hls || {};\n\nif (_videoJs2['default'].registerPlugin) {\n  _videoJs2['default'].registerPlugin('reloadSourceOnError', _reloadSourceOnError2['default']);\n} else {\n  _videoJs2['default'].plugin('reloadSourceOnError', _reloadSourceOnError2['default']);\n}\n\nmodule.exports = {\n  Hls: Hls,\n  HlsHandler: HlsHandler,\n  HlsSourceHandler: HlsSourceHandler\n};\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/videojs-contrib-hls.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/vtt-segment-loader.js":
/*!********************************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/vtt-segment-loader.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file vtt-segment-loader.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x3, _x4, _x5) { var _again = true; _function: while (_again) { var object = _x3, property = _x4, receiver = _x5; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x3 = parent; _x4 = property; _x5 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _segmentLoader = __webpack_require__(/*! ./segment-loader */ \"./node_modules/videojs-contrib-hls/es5/segment-loader.js\");\n\nvar _segmentLoader2 = _interopRequireDefault(_segmentLoader);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs = __webpack_require__(/*! videojs-contrib-media-sources/es5/remove-cues-from-track.js */ \"./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js\");\n\nvar _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs2 = _interopRequireDefault(_videojsContribMediaSourcesEs5RemoveCuesFromTrackJs);\n\nvar _binUtils = __webpack_require__(/*! ./bin-utils */ \"./node_modules/videojs-contrib-hls/es5/bin-utils.js\");\n\nvar VTT_LINE_TERMINATORS = new Uint8Array('\\n\\n'.split('').map(function (char) {\n  return char.charCodeAt(0);\n}));\n\nvar uintToString = function uintToString(uintArray) {\n  return String.fromCharCode.apply(null, uintArray);\n};\n\n/**\n * An object that manages segment loading and appending.\n *\n * @class VTTSegmentLoader\n * @param {Object} options required and optional options\n * @extends videojs.EventTarget\n */\n\nvar VTTSegmentLoader = (function (_SegmentLoader) {\n  _inherits(VTTSegmentLoader, _SegmentLoader);\n\n  function VTTSegmentLoader(settings) {\n    var options = arguments.length <= 1 || arguments[1] === undefined ? {} : arguments[1];\n\n    _classCallCheck(this, VTTSegmentLoader);\n\n    _get(Object.getPrototypeOf(VTTSegmentLoader.prototype), 'constructor', this).call(this, settings, options);\n\n    // SegmentLoader requires a MediaSource be specified or it will throw an error;\n    // however, VTTSegmentLoader has no need of a media source, so delete the reference\n    this.mediaSource_ = null;\n\n    this.subtitlesTrack_ = null;\n  }\n\n  /**\n   * Indicates which time ranges are buffered\n   *\n   * @return {TimeRange}\n   *         TimeRange object representing the current buffered ranges\n   */\n\n  _createClass(VTTSegmentLoader, [{\n    key: 'buffered_',\n    value: function buffered_() {\n      if (!this.subtitlesTrack_ || !this.subtitlesTrack_.cues.length) {\n        return _videoJs2['default'].createTimeRanges();\n      }\n\n      var cues = this.subtitlesTrack_.cues;\n      var start = cues[0].startTime;\n      var end = cues[cues.length - 1].startTime;\n\n      return _videoJs2['default'].createTimeRanges([[start, end]]);\n    }\n\n    /**\n     * Gets and sets init segment for the provided map\n     *\n     * @param {Object} map\n     *        The map object representing the init segment to get or set\n     * @param {Boolean=} set\n     *        If true, the init segment for the provided map should be saved\n     * @return {Object}\n     *         map object for desired init segment\n     */\n  }, {\n    key: 'initSegment',\n    value: function initSegment(map) {\n      var set = arguments.length <= 1 || arguments[1] === undefined ? false : arguments[1];\n\n      if (!map) {\n        return null;\n      }\n\n      var id = (0, _binUtils.initSegmentId)(map);\n      var storedMap = this.initSegments_[id];\n\n      if (set && !storedMap && map.bytes) {\n        // append WebVTT line terminators to the media initialization segment if it exists\n        // to follow the WebVTT spec (https://w3c.github.io/webvtt/#file-structure) that\n        // requires two or more WebVTT line terminators between the WebVTT header and the\n        // rest of the file\n        var combinedByteLength = VTT_LINE_TERMINATORS.byteLength + map.bytes.byteLength;\n        var combinedSegment = new Uint8Array(combinedByteLength);\n\n        combinedSegment.set(map.bytes);\n        combinedSegment.set(VTT_LINE_TERMINATORS, map.bytes.byteLength);\n\n        this.initSegments_[id] = storedMap = {\n          resolvedUri: map.resolvedUri,\n          byterange: map.byterange,\n          bytes: combinedSegment\n        };\n      }\n\n      return storedMap || map;\n    }\n\n    /**\n     * Returns true if all configuration required for loading is present, otherwise false.\n     *\n     * @return {Boolean} True if the all configuration is ready for loading\n     * @private\n     */\n  }, {\n    key: 'couldBeginLoading_',\n    value: function couldBeginLoading_() {\n      return this.playlist_ && this.subtitlesTrack_ && !this.paused();\n    }\n\n    /**\n     * Once all the starting parameters have been specified, begin\n     * operation. This method should only be invoked from the INIT\n     * state.\n     *\n     * @private\n     */\n  }, {\n    key: 'init_',\n    value: function init_() {\n      this.state = 'READY';\n      this.resetEverything();\n      return this.monitorBuffer_();\n    }\n\n    /**\n     * Set a subtitle track on the segment loader to add subtitles to\n     *\n     * @param {TextTrack=} track\n     *        The text track to add loaded subtitles to\n     * @return {TextTrack}\n     *        Returns the subtitles track\n     */\n  }, {\n    key: 'track',\n    value: function track(_track) {\n      if (typeof _track === 'undefined') {\n        return this.subtitlesTrack_;\n      }\n\n      this.subtitlesTrack_ = _track;\n\n      // if we were unpaused but waiting for a sourceUpdater, start\n      // buffering now\n      if (this.state === 'INIT' && this.couldBeginLoading_()) {\n        this.init_();\n      }\n\n      return this.subtitlesTrack_;\n    }\n\n    /**\n     * Remove any data in the source buffer between start and end times\n     * @param {Number} start - the start time of the region to remove from the buffer\n     * @param {Number} end - the end time of the region to remove from the buffer\n     */\n  }, {\n    key: 'remove',\n    value: function remove(start, end) {\n      (0, _videojsContribMediaSourcesEs5RemoveCuesFromTrackJs2['default'])(start, end, this.subtitlesTrack_);\n    }\n\n    /**\n     * fill the buffer with segements unless the sourceBuffers are\n     * currently updating\n     *\n     * Note: this function should only ever be called by monitorBuffer_\n     * and never directly\n     *\n     * @private\n     */\n  }, {\n    key: 'fillBuffer_',\n    value: function fillBuffer_() {\n      var _this = this;\n\n      if (!this.syncPoint_) {\n        this.syncPoint_ = this.syncController_.getSyncPoint(this.playlist_, this.duration_(), this.currentTimeline_, this.currentTime_());\n      }\n\n      // see if we need to begin loading immediately\n      var segmentInfo = this.checkBuffer_(this.buffered_(), this.playlist_, this.mediaIndex, this.hasPlayed_(), this.currentTime_(), this.syncPoint_);\n\n      segmentInfo = this.skipEmptySegments_(segmentInfo);\n\n      if (!segmentInfo) {\n        return;\n      }\n\n      if (this.syncController_.timestampOffsetForTimeline(segmentInfo.timeline) === null) {\n        // We don't have the timestamp offset that we need to sync subtitles.\n        // Rerun on a timestamp offset or user interaction.\n        var checkTimestampOffset = function checkTimestampOffset() {\n          _this.state = 'READY';\n          if (!_this.paused()) {\n            // if not paused, queue a buffer check as soon as possible\n            _this.monitorBuffer_();\n          }\n        };\n\n        this.syncController_.one('timestampoffset', checkTimestampOffset);\n        this.state = 'WAITING_ON_TIMELINE';\n        return;\n      }\n\n      this.loadSegment_(segmentInfo);\n    }\n\n    /**\n     * Prevents the segment loader from requesting segments we know contain no subtitles\n     * by walking forward until we find the next segment that we don't know whether it is\n     * empty or not.\n     *\n     * @param {Object} segmentInfo\n     *        a segment info object that describes the current segment\n     * @return {Object}\n     *         a segment info object that describes the current segment\n     */\n  }, {\n    key: 'skipEmptySegments_',\n    value: function skipEmptySegments_(segmentInfo) {\n      while (segmentInfo && segmentInfo.segment.empty) {\n        segmentInfo = this.generateSegmentInfo_(segmentInfo.playlist, segmentInfo.mediaIndex + 1, segmentInfo.startOfSegment + segmentInfo.duration, segmentInfo.isSyncRequest);\n      }\n      return segmentInfo;\n    }\n\n    /**\n     * append a decrypted segement to the SourceBuffer through a SourceUpdater\n     *\n     * @private\n     */\n  }, {\n    key: 'handleSegment_',\n    value: function handleSegment_() {\n      var _this2 = this;\n\n      if (!this.pendingSegment_ || !this.subtitlesTrack_) {\n        this.state = 'READY';\n        return;\n      }\n\n      this.state = 'APPENDING';\n\n      var segmentInfo = this.pendingSegment_;\n      var segment = segmentInfo.segment;\n\n      // Make sure that vttjs has loaded, otherwise, wait till it finished loading\n      if (typeof _globalWindow2['default'].WebVTT !== 'function' && this.subtitlesTrack_ && this.subtitlesTrack_.tech_) {\n        var _ret = (function () {\n\n          var loadHandler = function loadHandler() {\n            _this2.handleSegment_();\n          };\n\n          _this2.state = 'WAITING_ON_VTTJS';\n          _this2.subtitlesTrack_.tech_.one('vttjsloaded', loadHandler);\n          _this2.subtitlesTrack_.tech_.one('vttjserror', function () {\n            _this2.subtitlesTrack_.tech_.off('vttjsloaded', loadHandler);\n            _this2.error({\n              message: 'Error loading vtt.js'\n            });\n            _this2.state = 'READY';\n            _this2.pause();\n            _this2.trigger('error');\n          });\n\n          return {\n            v: undefined\n          };\n        })();\n\n        if (typeof _ret === 'object') return _ret.v;\n      }\n\n      segment.requested = true;\n\n      try {\n        this.parseVTTCues_(segmentInfo);\n      } catch (e) {\n        this.error({\n          message: e.message\n        });\n        this.state = 'READY';\n        this.pause();\n        return this.trigger('error');\n      }\n\n      this.updateTimeMapping_(segmentInfo, this.syncController_.timelines[segmentInfo.timeline], this.playlist_);\n\n      if (segmentInfo.isSyncRequest) {\n        this.trigger('syncinfoupdate');\n        this.pendingSegment_ = null;\n        this.state = 'READY';\n        return;\n      }\n\n      segmentInfo.byteLength = segmentInfo.bytes.byteLength;\n\n      this.mediaSecondsLoaded += segment.duration;\n\n      if (segmentInfo.cues.length) {\n        // remove any overlapping cues to prevent doubling\n        this.remove(segmentInfo.cues[0].endTime, segmentInfo.cues[segmentInfo.cues.length - 1].endTime);\n      }\n\n      segmentInfo.cues.forEach(function (cue) {\n        _this2.subtitlesTrack_.addCue(cue);\n      });\n\n      this.handleUpdateEnd_();\n    }\n\n    /**\n     * Uses the WebVTT parser to parse the segment response\n     *\n     * @param {Object} segmentInfo\n     *        a segment info object that describes the current segment\n     * @private\n     */\n  }, {\n    key: 'parseVTTCues_',\n    value: function parseVTTCues_(segmentInfo) {\n      var decoder = undefined;\n      var decodeBytesToString = false;\n\n      if (typeof _globalWindow2['default'].TextDecoder === 'function') {\n        decoder = new _globalWindow2['default'].TextDecoder('utf8');\n      } else {\n        decoder = _globalWindow2['default'].WebVTT.StringDecoder();\n        decodeBytesToString = true;\n      }\n\n      var parser = new _globalWindow2['default'].WebVTT.Parser(_globalWindow2['default'], _globalWindow2['default'].vttjs, decoder);\n\n      segmentInfo.cues = [];\n      segmentInfo.timestampmap = { MPEGTS: 0, LOCAL: 0 };\n\n      parser.oncue = segmentInfo.cues.push.bind(segmentInfo.cues);\n      parser.ontimestampmap = function (map) {\n        return segmentInfo.timestampmap = map;\n      };\n      parser.onparsingerror = function (error) {\n        _videoJs2['default'].log.warn('Error encountered when parsing cues: ' + error.message);\n      };\n\n      if (segmentInfo.segment.map) {\n        var mapData = segmentInfo.segment.map.bytes;\n\n        if (decodeBytesToString) {\n          mapData = uintToString(mapData);\n        }\n\n        parser.parse(mapData);\n      }\n\n      var segmentData = segmentInfo.bytes;\n\n      if (decodeBytesToString) {\n        segmentData = uintToString(segmentData);\n      }\n\n      parser.parse(segmentData);\n      parser.flush();\n    }\n\n    /**\n     * Updates the start and end times of any cues parsed by the WebVTT parser using\n     * the information parsed from the X-TIMESTAMP-MAP header and a TS to media time mapping\n     * from the SyncController\n     *\n     * @param {Object} segmentInfo\n     *        a segment info object that describes the current segment\n     * @param {Object} mappingObj\n     *        object containing a mapping from TS to media time\n     * @param {Object} playlist\n     *        the playlist object containing the segment\n     * @private\n     */\n  }, {\n    key: 'updateTimeMapping_',\n    value: function updateTimeMapping_(segmentInfo, mappingObj, playlist) {\n      var segment = segmentInfo.segment;\n\n      if (!mappingObj) {\n        // If the sync controller does not have a mapping of TS to Media Time for the\n        // timeline, then we don't have enough information to update the cue\n        // start/end times\n        return;\n      }\n\n      if (!segmentInfo.cues.length) {\n        // If there are no cues, we also do not have enough information to figure out\n        // segment timing. Mark that the segment contains no cues so we don't re-request\n        // an empty segment.\n        segment.empty = true;\n        return;\n      }\n\n      var timestampmap = segmentInfo.timestampmap;\n      var diff = timestampmap.MPEGTS / 90000 - timestampmap.LOCAL + mappingObj.mapping;\n\n      segmentInfo.cues.forEach(function (cue) {\n        // First convert cue time to TS time using the timestamp-map provided within the vtt\n        cue.startTime += diff;\n        cue.endTime += diff;\n      });\n\n      if (!playlist.syncInfo) {\n        var firstStart = segmentInfo.cues[0].startTime;\n        var lastStart = segmentInfo.cues[segmentInfo.cues.length - 1].startTime;\n\n        playlist.syncInfo = {\n          mediaSequence: playlist.mediaSequence + segmentInfo.mediaIndex,\n          time: Math.min(firstStart, lastStart - segment.duration)\n        };\n      }\n    }\n  }]);\n\n  return VTTSegmentLoader;\n})(_segmentLoader2['default']);\n\nexports['default'] = VTTSegmentLoader;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/vtt-segment-loader.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-hls/es5/xhr.js":
/*!*****************************************************!*\
  !*** ./node_modules/videojs-contrib-hls/es5/xhr.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file xhr.js\n */\n\n/**\n * A wrapper for videojs.xhr that tracks bandwidth.\n *\n * @param {Object} options options for the XHR\n * @param {Function} callback the callback to call when done\n * @return {Request} the xhr request that is going to be made\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar xhrFactory = function xhrFactory() {\n  var xhr = function XhrFunction(options, callback) {\n    // Add a default timeout for all hls requests\n    options = (0, _videoJs.mergeOptions)({\n      timeout: 45e3\n    }, options);\n\n    // Allow an optional user-specified function to modify the option\n    // object before we construct the xhr request\n    var beforeRequest = XhrFunction.beforeRequest || _videoJs2['default'].Hls.xhr.beforeRequest;\n\n    if (beforeRequest && typeof beforeRequest === 'function') {\n      var newOptions = beforeRequest(options);\n\n      if (newOptions) {\n        options = newOptions;\n      }\n    }\n\n    var request = (0, _videoJs.xhr)(options, function (error, response) {\n      var reqResponse = request.response;\n\n      if (!error && reqResponse) {\n        request.responseTime = Date.now();\n        request.roundTripTime = request.responseTime - request.requestTime;\n        request.bytesReceived = reqResponse.byteLength || reqResponse.length;\n        if (!request.bandwidth) {\n          request.bandwidth = Math.floor(request.bytesReceived / request.roundTripTime * 8 * 1000);\n        }\n      }\n\n      // videojs.xhr now uses a specific code on the error\n      // object to signal that a request has timed out instead\n      // of setting a boolean on the request object\n      if (error && error.code === 'ETIMEDOUT') {\n        request.timedout = true;\n      }\n\n      // videojs.xhr no longer considers status codes outside of 200 and 0\n      // (for file uris) to be errors, but the old XHR did, so emulate that\n      // behavior. Status 206 may be used in response to byterange requests.\n      if (!error && !request.aborted && response.statusCode !== 200 && response.statusCode !== 206 && response.statusCode !== 0) {\n        error = new Error('XHR Failed with a response of: ' + (request && (reqResponse || request.responseText)));\n      }\n\n      callback(error, request);\n    });\n    var originalAbort = request.abort;\n\n    request.abort = function () {\n      request.aborted = true;\n      return originalAbort.apply(request, arguments);\n    };\n    request.uri = options.uri;\n    request.requestTime = Date.now();\n    return request;\n  };\n\n  return xhr;\n};\n\nexports['default'] = xhrFactory;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-hls/es5/xhr.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/add-text-track-data.js":
/*!*******************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/add-text-track-data.js ***!
  \*******************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file add-text-track-data.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\n/**\n * Define properties on a cue for backwards compatability,\n * but warn the user that the way that they are using it\n * is depricated and will be removed at a later date.\n *\n * @param {Cue} cue the cue to add the properties on\n * @private\n */\nvar deprecateOldCue = function deprecateOldCue(cue) {\n  Object.defineProperties(cue.frame, {\n    id: {\n      get: function get() {\n        _videoJs2['default'].log.warn('cue.frame.id is deprecated. Use cue.value.key instead.');\n        return cue.value.key;\n      }\n    },\n    value: {\n      get: function get() {\n        _videoJs2['default'].log.warn('cue.frame.value is deprecated. Use cue.value.data instead.');\n        return cue.value.data;\n      }\n    },\n    privateData: {\n      get: function get() {\n        _videoJs2['default'].log.warn('cue.frame.privateData is deprecated. Use cue.value.data instead.');\n        return cue.value.data;\n      }\n    }\n  });\n};\n\nvar durationOfVideo = function durationOfVideo(duration) {\n  var dur = undefined;\n\n  if (isNaN(duration) || Math.abs(duration) === Infinity) {\n    dur = Number.MAX_VALUE;\n  } else {\n    dur = duration;\n  }\n  return dur;\n};\n/**\n * Add text track data to a source handler given the captions and\n * metadata from the buffer.\n *\n * @param {Object} sourceHandler the flash or virtual source buffer\n * @param {Array} captionArray an array of caption data\n * @param {Array} metadataArray an array of meta data\n * @private\n */\nvar addTextTrackData = function addTextTrackData(sourceHandler, captionArray, metadataArray) {\n  var Cue = _globalWindow2['default'].WebKitDataCue || _globalWindow2['default'].VTTCue;\n\n  if (captionArray) {\n    captionArray.forEach(function (caption) {\n      var track = caption.stream;\n\n      this.inbandTextTracks_[track].addCue(new Cue(caption.startTime + this.timestampOffset, caption.endTime + this.timestampOffset, caption.text));\n    }, sourceHandler);\n  }\n\n  if (metadataArray) {\n    (function () {\n      var videoDuration = durationOfVideo(sourceHandler.mediaSource_.duration);\n\n      metadataArray.forEach(function (metadata) {\n        var time = metadata.cueTime + this.timestampOffset;\n\n        metadata.frames.forEach(function (frame) {\n          var cue = new Cue(time, time, frame.value || frame.url || frame.data || '');\n\n          cue.frame = frame;\n          cue.value = frame;\n          deprecateOldCue(cue);\n\n          this.metadataTrack_.addCue(cue);\n        }, this);\n      }, sourceHandler);\n\n      // Updating the metadeta cues so that\n      // the endTime of each cue is the startTime of the next cue\n      // the endTime of last cue is the duration of the video\n      if (sourceHandler.metadataTrack_ && sourceHandler.metadataTrack_.cues && sourceHandler.metadataTrack_.cues.length) {\n        (function () {\n          var cues = sourceHandler.metadataTrack_.cues;\n          var cuesArray = [];\n\n          // Create a copy of the TextTrackCueList...\n          // ...disregarding cues with a falsey value\n          for (var i = 0; i < cues.length; i++) {\n            if (cues[i]) {\n              cuesArray.push(cues[i]);\n            }\n          }\n\n          // Group cues by their startTime value\n          var cuesGroupedByStartTime = cuesArray.reduce(function (obj, cue) {\n            var timeSlot = obj[cue.startTime] || [];\n\n            timeSlot.push(cue);\n            obj[cue.startTime] = timeSlot;\n\n            return obj;\n          }, {});\n\n          // Sort startTimes by ascending order\n          var sortedStartTimes = Object.keys(cuesGroupedByStartTime).sort(function (a, b) {\n            return Number(a) - Number(b);\n          });\n\n          // Map each cue group's endTime to the next group's startTime\n          sortedStartTimes.forEach(function (startTime, idx) {\n            var cueGroup = cuesGroupedByStartTime[startTime];\n            var nextTime = Number(sortedStartTimes[idx + 1]) || videoDuration;\n\n            // Map each cue's endTime the next group's startTime\n            cueGroup.forEach(function (cue) {\n              cue.endTime = nextTime;\n            });\n          });\n        })();\n      }\n    })();\n  }\n};\n\nexports['default'] = {\n  addTextTrackData: addTextTrackData,\n  durationOfVideo: durationOfVideo\n};\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/add-text-track-data.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/codec-utils.js":
/*!***********************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/codec-utils.js ***!
  \***********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file codec-utils.js\n */\n\n/**\n * Check if a codec string refers to an audio codec.\n *\n * @param {String} codec codec string to check\n * @return {Boolean} if this is an audio codec\n * @private\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\nvar isAudioCodec = function isAudioCodec(codec) {\n  return (/mp4a\\.\\d+.\\d+/i.test(codec)\n  );\n};\n\n/**\n * Check if a codec string refers to a video codec.\n *\n * @param {String} codec codec string to check\n * @return {Boolean} if this is a video codec\n * @private\n */\nvar isVideoCodec = function isVideoCodec(codec) {\n  return (/avc1\\.[\\da-f]+/i.test(codec)\n  );\n};\n\n/**\n * Parse a content type header into a type and parameters\n * object\n *\n * @param {String} type the content type header\n * @return {Object} the parsed content-type\n * @private\n */\nvar parseContentType = function parseContentType(type) {\n  var object = { type: '', parameters: {} };\n  var parameters = type.trim().split(';');\n\n  // first parameter should always be content-type\n  object.type = parameters.shift().trim();\n  parameters.forEach(function (parameter) {\n    var pair = parameter.trim().split('=');\n\n    if (pair.length > 1) {\n      var _name = pair[0].replace(/\"/g, '').trim();\n      var value = pair[1].replace(/\"/g, '').trim();\n\n      object.parameters[_name] = value;\n    }\n  });\n\n  return object;\n};\n\n/**\n * Replace the old apple-style `avc1.<dd>.<dd>` codec string with the standard\n * `avc1.<hhhhhh>`\n *\n * @param {Array} codecs an array of codec strings to fix\n * @return {Array} the translated codec array\n * @private\n */\nvar translateLegacyCodecs = function translateLegacyCodecs(codecs) {\n  return codecs.map(function (codec) {\n    return codec.replace(/avc1\\.(\\d+)\\.(\\d+)/i, function (orig, profile, avcLevel) {\n      var profileHex = ('00' + Number(profile).toString(16)).slice(-2);\n      var avcLevelHex = ('00' + Number(avcLevel).toString(16)).slice(-2);\n\n      return 'avc1.' + profileHex + '00' + avcLevelHex;\n    });\n  });\n};\n\nexports['default'] = {\n  isAudioCodec: isAudioCodec,\n  parseContentType: parseContentType,\n  isVideoCodec: isVideoCodec,\n  translateLegacyCodecs: translateLegacyCodecs\n};\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/codec-utils.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/create-text-tracks-if-necessary.js":
/*!*******************************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/create-text-tracks-if-necessary.js ***!
  \*******************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file create-text-tracks-if-necessary.js\n */\n\n/**\n * Create text tracks on video.js if they exist on a segment.\n *\n * @param {Object} sourceBuffer the VSB or FSB\n * @param {Object} mediaSource the HTML or Flash media source\n * @param {Object} segment the segment that may contain the text track\n * @private\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\nvar createTextTracksIfNecessary = function createTextTracksIfNecessary(sourceBuffer, mediaSource, segment) {\n  var player = mediaSource.player_;\n\n  // create an in-band caption track if one is present in the segment\n  if (segment.captions && segment.captions.length) {\n    if (!sourceBuffer.inbandTextTracks_) {\n      sourceBuffer.inbandTextTracks_ = {};\n    }\n\n    for (var trackId in segment.captionStreams) {\n      if (!sourceBuffer.inbandTextTracks_[trackId]) {\n        player.tech_.trigger({ type: 'usage', name: 'hls-608' });\n        var track = player.textTracks().getTrackById(trackId);\n\n        if (track) {\n          // Resuse an existing track with a CC# id because this was\n          // very likely created by videojs-contrib-hls from information\n          // in the m3u8 for us to use\n          sourceBuffer.inbandTextTracks_[trackId] = track;\n        } else {\n          // Otherwise, create a track with the default `CC#` label and\n          // without a language\n          sourceBuffer.inbandTextTracks_[trackId] = player.addRemoteTextTrack({\n            kind: 'captions',\n            id: trackId,\n            label: trackId\n          }, false).track;\n        }\n      }\n    }\n  }\n\n  if (segment.metadata && segment.metadata.length && !sourceBuffer.metadataTrack_) {\n    sourceBuffer.metadataTrack_ = player.addRemoteTextTrack({\n      kind: 'metadata',\n      label: 'Timed Metadata'\n    }, false).track;\n    sourceBuffer.metadataTrack_.inBandMetadataTrackDispatchType = segment.metadata.dispatchType;\n  }\n};\n\nexports['default'] = createTextTracksIfNecessary;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/create-text-tracks-if-necessary.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/flash-constants.js":
/*!***************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/flash-constants.js ***!
  \***************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file flash-constants.js\n */\n/**\n * The maximum size in bytes for append operations to the video.js\n * SWF. Calling through to Flash blocks and can be expensive so\n * we chunk data and pass through 4KB at a time, yielding to the\n * browser between chunks. This gives a theoretical maximum rate of\n * 1MB/s into Flash. Any higher and we begin to drop frames and UI\n * responsiveness suffers.\n *\n * @private\n */\n\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nvar flashConstants = {\n  // times in milliseconds\n  TIME_BETWEEN_CHUNKS: 1,\n  BYTES_PER_CHUNK: 1024 * 32\n};\n\nexports[\"default\"] = flashConstants;\nmodule.exports = exports[\"default\"];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/flash-constants.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/flash-media-source.js":
/*!******************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/flash-media-source.js ***!
  \******************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file flash-media-source.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _globalDocument = __webpack_require__(/*! global/document */ \"./node_modules/global/document.js\");\n\nvar _globalDocument2 = _interopRequireDefault(_globalDocument);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _flashSourceBuffer = __webpack_require__(/*! ./flash-source-buffer */ \"./node_modules/videojs-contrib-media-sources/es5/flash-source-buffer.js\");\n\nvar _flashSourceBuffer2 = _interopRequireDefault(_flashSourceBuffer);\n\nvar _flashConstants = __webpack_require__(/*! ./flash-constants */ \"./node_modules/videojs-contrib-media-sources/es5/flash-constants.js\");\n\nvar _flashConstants2 = _interopRequireDefault(_flashConstants);\n\nvar _codecUtils = __webpack_require__(/*! ./codec-utils */ \"./node_modules/videojs-contrib-media-sources/es5/codec-utils.js\");\n\n/**\n * A flash implmentation of HTML MediaSources and a polyfill\n * for browsers that don't support native or HTML MediaSources..\n *\n * @link https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\n * @class FlashMediaSource\n * @extends videojs.EventTarget\n */\n\nvar FlashMediaSource = (function (_videojs$EventTarget) {\n  _inherits(FlashMediaSource, _videojs$EventTarget);\n\n  function FlashMediaSource() {\n    var _this = this;\n\n    _classCallCheck(this, FlashMediaSource);\n\n    _get(Object.getPrototypeOf(FlashMediaSource.prototype), 'constructor', this).call(this);\n    this.sourceBuffers = [];\n    this.readyState = 'closed';\n\n    this.on(['sourceopen', 'webkitsourceopen'], function (event) {\n      // find the swf where we will push media data\n      _this.swfObj = _globalDocument2['default'].getElementById(event.swfId);\n      _this.player_ = (0, _videoJs2['default'])(_this.swfObj.parentNode);\n      _this.tech_ = _this.swfObj.tech;\n      _this.readyState = 'open';\n\n      _this.tech_.on('seeking', function () {\n        var i = _this.sourceBuffers.length;\n\n        while (i--) {\n          _this.sourceBuffers[i].abort();\n        }\n      });\n\n      // trigger load events\n      if (_this.swfObj) {\n        _this.swfObj.vjs_load();\n      }\n    });\n  }\n\n  /**\n    * Set or return the presentation duration.\n    *\n    * @param {Double} value the duration of the media in seconds\n    * @param {Double} the current presentation duration\n    * @link http://www.w3.org/TR/media-source/#widl-MediaSource-duration\n    */\n\n  /**\n   * We have this function so that the html and flash interfaces\n   * are the same.\n   *\n   * @private\n   */\n\n  _createClass(FlashMediaSource, [{\n    key: 'addSeekableRange_',\n    value: function addSeekableRange_() {}\n    // intentional no-op\n\n    /**\n     * Create a new flash source buffer and add it to our flash media source.\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/MediaSource/addSourceBuffer\n     * @param {String} type the content-type of the source\n     * @return {Object} the flash source buffer\n     */\n\n  }, {\n    key: 'addSourceBuffer',\n    value: function addSourceBuffer(type) {\n      var parsedType = (0, _codecUtils.parseContentType)(type);\n      var sourceBuffer = undefined;\n\n      // if this is an FLV type, we'll push data to flash\n      if (parsedType.type === 'video/mp2t' || parsedType.type === 'audio/mp2t') {\n        // Flash source buffers\n        sourceBuffer = new _flashSourceBuffer2['default'](this);\n      } else {\n        throw new Error('NotSupportedError (Video.js)');\n      }\n\n      this.sourceBuffers.push(sourceBuffer);\n      return sourceBuffer;\n    }\n\n    /**\n     * Signals the end of the stream.\n     *\n     * @link https://w3c.github.io/media-source/#widl-MediaSource-endOfStream-void-EndOfStreamError-error\n     * @param {String=} error Signals that a playback error\n     * has occurred. If specified, it must be either \"network\" or\n     * \"decode\".\n     */\n  }, {\n    key: 'endOfStream',\n    value: function endOfStream(error) {\n      if (error === 'network') {\n        // MEDIA_ERR_NETWORK\n        this.tech_.error(2);\n      } else if (error === 'decode') {\n        // MEDIA_ERR_DECODE\n        this.tech_.error(3);\n      }\n      if (this.readyState !== 'ended') {\n        this.readyState = 'ended';\n        this.swfObj.vjs_endOfStream();\n      }\n    }\n  }]);\n\n  return FlashMediaSource;\n})(_videoJs2['default'].EventTarget);\n\nexports['default'] = FlashMediaSource;\ntry {\n  Object.defineProperty(FlashMediaSource.prototype, 'duration', {\n    /**\n     * Return the presentation duration.\n     *\n     * @return {Double} the duration of the media in seconds\n     * @link http://www.w3.org/TR/media-source/#widl-MediaSource-duration\n     */\n    get: function get() {\n      if (!this.swfObj) {\n        return NaN;\n      }\n      // get the current duration from the SWF\n      return this.swfObj.vjs_getProperty('duration');\n    },\n    /**\n     * Set the presentation duration.\n     *\n     * @param {Double} value the duration of the media in seconds\n     * @return {Double} the duration of the media in seconds\n     * @link http://www.w3.org/TR/media-source/#widl-MediaSource-duration\n     */\n    set: function set(value) {\n      var i = undefined;\n      var oldDuration = this.swfObj.vjs_getProperty('duration');\n\n      this.swfObj.vjs_setProperty('duration', value);\n\n      if (value < oldDuration) {\n        // In MSE, this triggers the range removal algorithm which causes\n        // an update to occur\n        for (i = 0; i < this.sourceBuffers.length; i++) {\n          this.sourceBuffers[i].remove(value, oldDuration);\n        }\n      }\n\n      return value;\n    }\n  });\n} catch (e) {\n  // IE8 throws if defineProperty is called on a non-DOM node. We\n  // don't support IE8 but we shouldn't throw an error if loaded\n  // there.\n  FlashMediaSource.prototype.duration = NaN;\n}\n\nfor (var property in _flashConstants2['default']) {\n  FlashMediaSource[property] = _flashConstants2['default'][property];\n}\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/flash-media-source.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/flash-source-buffer.js":
/*!*******************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/flash-source-buffer.js ***!
  \*******************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file flash-source-buffer.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _muxJsLibFlv = __webpack_require__(/*! mux.js/lib/flv */ \"./node_modules/mux.js/lib/flv/index.js\");\n\nvar _muxJsLibFlv2 = _interopRequireDefault(_muxJsLibFlv);\n\nvar _removeCuesFromTrack = __webpack_require__(/*! ./remove-cues-from-track */ \"./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js\");\n\nvar _removeCuesFromTrack2 = _interopRequireDefault(_removeCuesFromTrack);\n\nvar _createTextTracksIfNecessary = __webpack_require__(/*! ./create-text-tracks-if-necessary */ \"./node_modules/videojs-contrib-media-sources/es5/create-text-tracks-if-necessary.js\");\n\nvar _createTextTracksIfNecessary2 = _interopRequireDefault(_createTextTracksIfNecessary);\n\nvar _addTextTrackData = __webpack_require__(/*! ./add-text-track-data */ \"./node_modules/videojs-contrib-media-sources/es5/add-text-track-data.js\");\n\nvar _flashTransmuxerWorker = __webpack_require__(/*! ./flash-transmuxer-worker */ \"./node_modules/videojs-contrib-media-sources/es5/flash-transmuxer-worker.js\");\n\nvar _flashTransmuxerWorker2 = _interopRequireDefault(_flashTransmuxerWorker);\n\nvar _webwackify = __webpack_require__(/*! webwackify */ \"./node_modules/webwackify/index.js\");\n\nvar _webwackify2 = _interopRequireDefault(_webwackify);\n\nvar _flashConstants = __webpack_require__(/*! ./flash-constants */ \"./node_modules/videojs-contrib-media-sources/es5/flash-constants.js\");\n\nvar _flashConstants2 = _interopRequireDefault(_flashConstants);\n\nvar resolveFlashTransmuxWorker = function resolveFlashTransmuxWorker() {\n  var result = undefined;\n\n  try {\n    result = /*require.resolve*/(/*! ./flash-transmuxer-worker */ \"./node_modules/videojs-contrib-media-sources/es5/flash-transmuxer-worker.js\");\n  } catch (e) {\n    // no result\n  }\n\n  return result;\n};\n\n/**\n * A wrapper around the setTimeout function that uses\n * the flash constant time between ticks value.\n *\n * @param {Function} func the function callback to run\n * @private\n */\nvar scheduleTick = function scheduleTick(func) {\n  // Chrome doesn't invoke requestAnimationFrame callbacks\n  // in background tabs, so use setTimeout.\n  _globalWindow2['default'].setTimeout(func, _flashConstants2['default'].TIME_BETWEEN_CHUNKS);\n};\n\n/**\n * Generates a random string of max length 6\n *\n * @return {String} the randomly generated string\n * @function generateRandomString\n * @private\n */\nvar generateRandomString = function generateRandomString() {\n  return Math.random().toString(36).slice(2, 8);\n};\n\n/**\n * Round a number to a specified number of places much like\n * toFixed but return a number instead of a string representation.\n *\n * @param {Number} num A number\n * @param {Number} places The number of decimal places which to\n * round\n * @private\n */\nvar toDecimalPlaces = function toDecimalPlaces(num, places) {\n  if (typeof places !== 'number' || places < 0) {\n    places = 0;\n  }\n\n  var scale = Math.pow(10, places);\n\n  return Math.round(num * scale) / scale;\n};\n\n/**\n * A SourceBuffer implementation for Flash rather than HTML.\n *\n * @link https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\n * @param {Object} mediaSource the flash media source\n * @class FlashSourceBuffer\n * @extends videojs.EventTarget\n */\n\nvar FlashSourceBuffer = (function (_videojs$EventTarget) {\n  _inherits(FlashSourceBuffer, _videojs$EventTarget);\n\n  function FlashSourceBuffer(mediaSource) {\n    var _this = this;\n\n    _classCallCheck(this, FlashSourceBuffer);\n\n    _get(Object.getPrototypeOf(FlashSourceBuffer.prototype), 'constructor', this).call(this);\n    var encodedHeader = undefined;\n\n    // Start off using the globally defined value but refine\n    // as we append data into flash\n    this.chunkSize_ = _flashConstants2['default'].BYTES_PER_CHUNK;\n\n    // byte arrays queued to be appended\n    this.buffer_ = [];\n\n    // the total number of queued bytes\n    this.bufferSize_ = 0;\n\n    // to be able to determine the correct position to seek to, we\n    // need to retain information about the mapping between the\n    // media timeline and PTS values\n    this.basePtsOffset_ = NaN;\n\n    this.mediaSource_ = mediaSource;\n\n    this.audioBufferEnd_ = NaN;\n    this.videoBufferEnd_ = NaN;\n\n    // indicates whether the asynchronous continuation of an operation\n    // is still being processed\n    // see https://w3c.github.io/media-source/#widl-SourceBuffer-updating\n    this.updating = false;\n    this.timestampOffset_ = 0;\n\n    encodedHeader = _globalWindow2['default'].btoa(String.fromCharCode.apply(null, Array.prototype.slice.call(_muxJsLibFlv2['default'].getFlvHeader())));\n\n    // create function names with added randomness for the global callbacks flash will use\n    // to get data from javascript into the swf. Random strings are added as a safety\n    // measure for pages with multiple players since these functions will be global\n    // instead of per instance. When making a call to the swf, the browser generates a\n    // try catch code snippet, but just takes the function name and writes out an unquoted\n    // call to that function. If the player id has any special characters, this will result\n    // in an error, so safePlayerId replaces all special characters to '_'\n    var safePlayerId = this.mediaSource_.player_.id().replace(/[^a-zA-Z0-9]/g, '_');\n\n    this.flashEncodedHeaderName_ = 'vjs_flashEncodedHeader_' + safePlayerId + generateRandomString();\n    this.flashEncodedDataName_ = 'vjs_flashEncodedData_' + safePlayerId + generateRandomString();\n\n    _globalWindow2['default'][this.flashEncodedHeaderName_] = function () {\n      delete _globalWindow2['default'][_this.flashEncodedHeaderName_];\n      return encodedHeader;\n    };\n\n    this.mediaSource_.swfObj.vjs_appendChunkReady(this.flashEncodedHeaderName_);\n\n    this.transmuxer_ = (0, _webwackify2['default'])(_flashTransmuxerWorker2['default'], resolveFlashTransmuxWorker());\n    this.transmuxer_.postMessage({ action: 'init', options: {} });\n    this.transmuxer_.onmessage = function (event) {\n      if (event.data.action === 'data') {\n        _this.receiveBuffer_(event.data.segment);\n      }\n    };\n\n    this.one('updateend', function () {\n      _this.mediaSource_.tech_.trigger('loadedmetadata');\n    });\n\n    Object.defineProperty(this, 'timestampOffset', {\n      get: function get() {\n        return this.timestampOffset_;\n      },\n      set: function set(val) {\n        if (typeof val === 'number' && val >= 0) {\n          this.timestampOffset_ = val;\n          // We have to tell flash to expect a discontinuity\n          this.mediaSource_.swfObj.vjs_discontinuity();\n          // the media <-> PTS mapping must be re-established after\n          // the discontinuity\n          this.basePtsOffset_ = NaN;\n          this.audioBufferEnd_ = NaN;\n          this.videoBufferEnd_ = NaN;\n\n          this.transmuxer_.postMessage({ action: 'reset' });\n        }\n      }\n    });\n\n    Object.defineProperty(this, 'buffered', {\n      get: function get() {\n        if (!this.mediaSource_ || !this.mediaSource_.swfObj || !('vjs_getProperty' in this.mediaSource_.swfObj)) {\n          return _videoJs2['default'].createTimeRange();\n        }\n\n        var buffered = this.mediaSource_.swfObj.vjs_getProperty('buffered');\n\n        if (buffered && buffered.length) {\n          buffered[0][0] = toDecimalPlaces(buffered[0][0], 3);\n          buffered[0][1] = toDecimalPlaces(buffered[0][1], 3);\n        }\n        return _videoJs2['default'].createTimeRanges(buffered);\n      }\n    });\n\n    // On a seek we remove all text track data since flash has no concept\n    // of a buffered-range and everything else is reset on seek\n    this.mediaSource_.player_.on('seeked', function () {\n      (0, _removeCuesFromTrack2['default'])(0, Infinity, _this.metadataTrack_);\n      if (_this.inbandTextTracks_) {\n        for (var track in _this.inbandTextTracks_) {\n          (0, _removeCuesFromTrack2['default'])(0, Infinity, _this.inbandTextTracks_[track]);\n        }\n      }\n    });\n\n    var onHlsReset = this.onHlsReset_.bind(this);\n\n    // hls-reset is fired by videojs.Hls on to the tech after the main SegmentLoader\n    // resets its state and flushes the buffer\n    this.mediaSource_.player_.tech_.on('hls-reset', onHlsReset);\n\n    this.mediaSource_.player_.tech_.hls.on('dispose', function () {\n      _this.transmuxer_.terminate();\n      _this.mediaSource_.player_.tech_.off('hls-reset', onHlsReset);\n    });\n  }\n\n  /**\n   * Append bytes to the sourcebuffers buffer, in this case we\n   * have to append it to swf object.\n   *\n   * @link https://developer.mozilla.org/en-US/docs/Web/API/SourceBuffer/appendBuffer\n   * @param {Array} bytes\n   */\n\n  _createClass(FlashSourceBuffer, [{\n    key: 'appendBuffer',\n    value: function appendBuffer(bytes) {\n      var error = undefined;\n\n      if (this.updating) {\n        error = new Error('SourceBuffer.append() cannot be called ' + 'while an update is in progress');\n        error.name = 'InvalidStateError';\n        error.code = 11;\n        throw error;\n      }\n      this.updating = true;\n      this.mediaSource_.readyState = 'open';\n      this.trigger({ type: 'update' });\n\n      this.transmuxer_.postMessage({\n        action: 'push',\n        data: bytes.buffer,\n        byteOffset: bytes.byteOffset,\n        byteLength: bytes.byteLength\n      }, [bytes.buffer]);\n      this.transmuxer_.postMessage({ action: 'flush' });\n    }\n\n    /**\n     * Reset the parser and remove any data queued to be sent to the SWF.\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/SourceBuffer/abort\n     */\n  }, {\n    key: 'abort',\n    value: function abort() {\n      this.buffer_ = [];\n      this.bufferSize_ = 0;\n      this.mediaSource_.swfObj.vjs_abort();\n\n      // report any outstanding updates have ended\n      if (this.updating) {\n        this.updating = false;\n        this.trigger({ type: 'updateend' });\n      }\n    }\n\n    /**\n     * Flash cannot remove ranges already buffered in the NetStream\n     * but seeking clears the buffer entirely. For most purposes,\n     * having this operation act as a no-op is acceptable.\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/SourceBuffer/remove\n     * @param {Double} start start of the section to remove\n     * @param {Double} end end of the section to remove\n     */\n  }, {\n    key: 'remove',\n    value: function remove(start, end) {\n      (0, _removeCuesFromTrack2['default'])(start, end, this.metadataTrack_);\n      if (this.inbandTextTracks_) {\n        for (var track in this.inbandTextTracks_) {\n          (0, _removeCuesFromTrack2['default'])(start, end, this.inbandTextTracks_[track]);\n        }\n      }\n      this.trigger({ type: 'update' });\n      this.trigger({ type: 'updateend' });\n    }\n\n    /**\n     * Receive a buffer from the flv.\n     *\n     * @param {Object} segment\n     * @private\n     */\n  }, {\n    key: 'receiveBuffer_',\n    value: function receiveBuffer_(segment) {\n      var _this2 = this;\n\n      // create an in-band caption track if one is present in the segment\n      (0, _createTextTracksIfNecessary2['default'])(this, this.mediaSource_, segment);\n      (0, _addTextTrackData.addTextTrackData)(this, segment.captions, segment.metadata);\n\n      // Do this asynchronously since convertTagsToData_ can be time consuming\n      scheduleTick(function () {\n        var flvBytes = _this2.convertTagsToData_(segment);\n\n        if (_this2.buffer_.length === 0) {\n          scheduleTick(_this2.processBuffer_.bind(_this2));\n        }\n\n        if (flvBytes) {\n          _this2.buffer_.push(flvBytes);\n          _this2.bufferSize_ += flvBytes.byteLength;\n        }\n      });\n    }\n\n    /**\n     * Append a portion of the current buffer to the SWF.\n     *\n     * @private\n     */\n  }, {\n    key: 'processBuffer_',\n    value: function processBuffer_() {\n      var _this3 = this;\n\n      var chunkSize = _flashConstants2['default'].BYTES_PER_CHUNK;\n\n      if (!this.buffer_.length) {\n        if (this.updating !== false) {\n          this.updating = false;\n          this.trigger({ type: 'updateend' });\n        }\n        // do nothing if the buffer is empty\n        return;\n      }\n\n      // concatenate appends up to the max append size\n      var chunk = this.buffer_[0].subarray(0, chunkSize);\n\n      // requeue any bytes that won't make it this round\n      if (chunk.byteLength < chunkSize || this.buffer_[0].byteLength === chunkSize) {\n        this.buffer_.shift();\n      } else {\n        this.buffer_[0] = this.buffer_[0].subarray(chunkSize);\n      }\n\n      this.bufferSize_ -= chunk.byteLength;\n\n      // base64 encode the bytes\n      var binary = [];\n      var length = chunk.byteLength;\n\n      for (var i = 0; i < length; i++) {\n        binary.push(String.fromCharCode(chunk[i]));\n      }\n      var b64str = _globalWindow2['default'].btoa(binary.join(''));\n\n      _globalWindow2['default'][this.flashEncodedDataName_] = function () {\n        // schedule another processBuffer to process any left over data or to\n        // trigger updateend\n        scheduleTick(_this3.processBuffer_.bind(_this3));\n        delete _globalWindow2['default'][_this3.flashEncodedDataName_];\n        return b64str;\n      };\n\n      // Notify the swf that segment data is ready to be appended\n      this.mediaSource_.swfObj.vjs_appendChunkReady(this.flashEncodedDataName_);\n    }\n\n    /**\n     * Turns an array of flv tags into a Uint8Array representing the\n     * flv data. Also removes any tags that are before the current\n     * time so that playback begins at or slightly after the right\n     * place on a seek\n     *\n     * @private\n     * @param {Object} segmentData object of segment data\n     */\n  }, {\n    key: 'convertTagsToData_',\n    value: function convertTagsToData_(segmentData) {\n      var segmentByteLength = 0;\n      var tech = this.mediaSource_.tech_;\n      var videoTargetPts = 0;\n      var segment = undefined;\n      var videoTags = segmentData.tags.videoTags;\n      var audioTags = segmentData.tags.audioTags;\n\n      // Establish the media timeline to PTS translation if we don't\n      // have one already\n      if (isNaN(this.basePtsOffset_) && (videoTags.length || audioTags.length)) {\n        // We know there is at least one video or audio tag, but since we may not have both,\n        // we use pts: Infinity for the missing tag. The will force the following Math.min\n        // call will to use the proper pts value since it will always be less than Infinity\n        var firstVideoTag = videoTags[0] || { pts: Infinity };\n        var firstAudioTag = audioTags[0] || { pts: Infinity };\n\n        this.basePtsOffset_ = Math.min(firstAudioTag.pts, firstVideoTag.pts);\n      }\n\n      if (tech.seeking()) {\n        // Do not use previously saved buffer end values while seeking since buffer\n        // is cleared on all seeks\n        this.videoBufferEnd_ = NaN;\n        this.audioBufferEnd_ = NaN;\n      }\n\n      if (isNaN(this.videoBufferEnd_)) {\n        if (tech.buffered().length) {\n          videoTargetPts = tech.buffered().end(0) - this.timestampOffset;\n        }\n\n        // Trim to currentTime if seeking\n        if (tech.seeking()) {\n          videoTargetPts = Math.max(videoTargetPts, tech.currentTime() - this.timestampOffset);\n        }\n\n        // PTS values are represented in milliseconds\n        videoTargetPts *= 1e3;\n        videoTargetPts += this.basePtsOffset_;\n      } else {\n        // Add a fudge factor of 0.1 to the last video pts appended since a rendition change\n        // could append an overlapping segment, in which case there is a high likelyhood\n        // a tag could have a matching pts to videoBufferEnd_, which would cause\n        // that tag to get appended by the tag.pts >= targetPts check below even though it\n        // is a duplicate of what was previously appended\n        videoTargetPts = this.videoBufferEnd_ + 0.1;\n      }\n\n      // filter complete GOPs with a presentation time less than the seek target/end of buffer\n      var currentIndex = videoTags.length;\n\n      // if the last tag is beyond videoTargetPts, then do not search the list for a GOP\n      // since our videoTargetPts lies in a future segment\n      if (currentIndex && videoTags[currentIndex - 1].pts >= videoTargetPts) {\n        // Start by walking backwards from the end of the list until we reach a tag that\n        // is equal to or less than videoTargetPts\n        while (--currentIndex) {\n          var currentTag = videoTags[currentIndex];\n\n          if (currentTag.pts > videoTargetPts) {\n            continue;\n          }\n\n          // if we see a keyFrame or metadata tag once we've gone below videoTargetPts,\n          // exit the loop as this is the start of the GOP that we want to append\n          if (currentTag.keyFrame || currentTag.metaDataTag) {\n            break;\n          }\n        }\n\n        // We need to check if there are any metadata tags that come before currentIndex\n        // as those will be metadata tags associated with the GOP we are appending\n        // There could be 0 to 2 metadata tags that come before the currentIndex depending\n        // on what videoTargetPts is and whether the transmuxer prepended metadata tags to this\n        // key frame\n        while (currentIndex) {\n          var nextTag = videoTags[currentIndex - 1];\n\n          if (!nextTag.metaDataTag) {\n            break;\n          }\n\n          currentIndex--;\n        }\n      }\n\n      var filteredVideoTags = videoTags.slice(currentIndex);\n\n      var audioTargetPts = undefined;\n\n      if (isNaN(this.audioBufferEnd_)) {\n        audioTargetPts = videoTargetPts;\n      } else {\n        // Add a fudge factor of 0.1 to the last video pts appended since a rendition change\n        // could append an overlapping segment, in which case there is a high likelyhood\n        // a tag could have a matching pts to videoBufferEnd_, which would cause\n        // that tag to get appended by the tag.pts >= targetPts check below even though it\n        // is a duplicate of what was previously appended\n        audioTargetPts = this.audioBufferEnd_ + 0.1;\n      }\n\n      if (filteredVideoTags.length) {\n        // If targetPts intersects a GOP and we appended the tags for the GOP that came\n        // before targetPts, we want to make sure to trim audio tags at the pts\n        // of the first video tag to avoid brief moments of silence\n        audioTargetPts = Math.min(audioTargetPts, filteredVideoTags[0].pts);\n      }\n\n      // skip tags with a presentation time less than the seek target/end of buffer\n      currentIndex = 0;\n\n      while (currentIndex < audioTags.length) {\n        if (audioTags[currentIndex].pts >= audioTargetPts) {\n          break;\n        }\n\n        currentIndex++;\n      }\n\n      var filteredAudioTags = audioTags.slice(currentIndex);\n\n      // update the audio and video buffer ends\n      if (filteredAudioTags.length) {\n        this.audioBufferEnd_ = filteredAudioTags[filteredAudioTags.length - 1].pts;\n      }\n      if (filteredVideoTags.length) {\n        this.videoBufferEnd_ = filteredVideoTags[filteredVideoTags.length - 1].pts;\n      }\n\n      var tags = this.getOrderedTags_(filteredVideoTags, filteredAudioTags);\n\n      if (tags.length === 0) {\n        return;\n      }\n\n      // If we are appending data that comes before our target pts, we want to tell\n      // the swf to adjust its notion of current time to account for the extra tags\n      // we are appending to complete the GOP that intersects with targetPts\n      if (tags[0].pts < videoTargetPts && tech.seeking()) {\n        var fudgeFactor = 1 / 30;\n        var currentTime = tech.currentTime();\n        var diff = (videoTargetPts - tags[0].pts) / 1e3;\n        var adjustedTime = currentTime - diff;\n\n        if (adjustedTime < fudgeFactor) {\n          adjustedTime = 0;\n        }\n\n        try {\n          this.mediaSource_.swfObj.vjs_adjustCurrentTime(adjustedTime);\n        } catch (e) {\n          // no-op for backwards compatability of swf. If adjustCurrentTime fails,\n          // the swf may incorrectly report currentTime and buffered ranges\n          // but should not affect playback over than the time displayed on the\n          // progress bar is inaccurate\n        }\n      }\n\n      // concatenate the bytes into a single segment\n      for (var i = 0; i < tags.length; i++) {\n        segmentByteLength += tags[i].bytes.byteLength;\n      }\n      segment = new Uint8Array(segmentByteLength);\n      for (var i = 0, j = 0; i < tags.length; i++) {\n        segment.set(tags[i].bytes, j);\n        j += tags[i].bytes.byteLength;\n      }\n\n      return segment;\n    }\n\n    /**\n     * Assemble the FLV tags in decoder order.\n     *\n     * @private\n     * @param {Array} videoTags list of video tags\n     * @param {Array} audioTags list of audio tags\n     */\n  }, {\n    key: 'getOrderedTags_',\n    value: function getOrderedTags_(videoTags, audioTags) {\n      var tag = undefined;\n      var tags = [];\n\n      while (videoTags.length || audioTags.length) {\n        if (!videoTags.length) {\n          // only audio tags remain\n          tag = audioTags.shift();\n        } else if (!audioTags.length) {\n          // only video tags remain\n          tag = videoTags.shift();\n        } else if (audioTags[0].dts < videoTags[0].dts) {\n          // audio should be decoded next\n          tag = audioTags.shift();\n        } else {\n          // video should be decoded next\n          tag = videoTags.shift();\n        }\n\n        tags.push(tag);\n      }\n\n      return tags;\n    }\n  }, {\n    key: 'onHlsReset_',\n    value: function onHlsReset_() {\n      this.transmuxer_.postMessage({ action: 'resetCaptions' });\n    }\n  }]);\n\n  return FlashSourceBuffer;\n})(_videoJs2['default'].EventTarget);\n\nexports['default'] = FlashSourceBuffer;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/flash-source-buffer.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/flash-transmuxer-worker.js":
/*!***********************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/flash-transmuxer-worker.js ***!
  \***********************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file flash-transmuxer-worker.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _muxJsLibFlv = __webpack_require__(/*! mux.js/lib/flv */ \"./node_modules/mux.js/lib/flv/index.js\");\n\nvar _muxJsLibFlv2 = _interopRequireDefault(_muxJsLibFlv);\n\n/**\n * Re-emits transmuxer events by converting them into messages to the\n * world outside the worker.\n *\n * @param {Object} transmuxer the transmuxer to wire events on\n * @private\n */\nvar wireTransmuxerEvents = function wireTransmuxerEvents(transmuxer) {\n  transmuxer.on('data', function (segment) {\n    _globalWindow2['default'].postMessage({\n      action: 'data',\n      segment: segment\n    });\n  });\n\n  transmuxer.on('done', function (data) {\n    _globalWindow2['default'].postMessage({ action: 'done' });\n  });\n};\n\n/**\n * All incoming messages route through this hash. If no function exists\n * to handle an incoming message, then we ignore the message.\n *\n * @class MessageHandlers\n * @param {Object} options the options to initialize with\n */\n\nvar MessageHandlers = (function () {\n  function MessageHandlers(options) {\n    _classCallCheck(this, MessageHandlers);\n\n    this.options = options || {};\n    this.init();\n  }\n\n  /**\n   * Our web wroker interface so that things can talk to mux.js\n   * that will be running in a web worker. The scope is passed to this by\n   * webworkify.\n   *\n   * @param {Object} self the scope for the web worker\n   */\n\n  /**\n   * initialize our web worker and wire all the events.\n   */\n\n  _createClass(MessageHandlers, [{\n    key: 'init',\n    value: function init() {\n      if (this.transmuxer) {\n        this.transmuxer.dispose();\n      }\n      this.transmuxer = new _muxJsLibFlv2['default'].Transmuxer(this.options);\n      wireTransmuxerEvents(this.transmuxer);\n    }\n\n    /**\n     * Adds data (a ts segment) to the start of the transmuxer pipeline for\n     * processing.\n     *\n     * @param {ArrayBuffer} data data to push into the muxer\n     */\n  }, {\n    key: 'push',\n    value: function push(data) {\n      // Cast array buffer to correct type for transmuxer\n      var segment = new Uint8Array(data.data, data.byteOffset, data.byteLength);\n\n      this.transmuxer.push(segment);\n    }\n\n    /**\n     * Recreate the transmuxer so that the next segment added via `push`\n     * start with a fresh transmuxer.\n     */\n  }, {\n    key: 'reset',\n    value: function reset() {\n      this.init();\n    }\n\n    /**\n     * Forces the pipeline to finish processing the last segment and emit its\n     * results.\n     */\n  }, {\n    key: 'flush',\n    value: function flush() {\n      this.transmuxer.flush();\n    }\n  }, {\n    key: 'resetCaptions',\n    value: function resetCaptions() {\n      this.transmuxer.resetCaptions();\n    }\n  }]);\n\n  return MessageHandlers;\n})();\n\nvar FlashTransmuxerWorker = function FlashTransmuxerWorker(self) {\n  self.onmessage = function (event) {\n    if (event.data.action === 'init' && event.data.options) {\n      this.messageHandlers = new MessageHandlers(event.data.options);\n      return;\n    }\n\n    if (!this.messageHandlers) {\n      this.messageHandlers = new MessageHandlers();\n    }\n\n    if (event.data && event.data.action && event.data.action !== 'init') {\n      if (this.messageHandlers[event.data.action]) {\n        this.messageHandlers[event.data.action](event.data);\n      }\n    }\n  };\n};\n\nexports['default'] = function (self) {\n  return new FlashTransmuxerWorker(self);\n};\n\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/flash-transmuxer-worker.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/html-media-source.js":
/*!*****************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/html-media-source.js ***!
  \*****************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file html-media-source.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _globalDocument = __webpack_require__(/*! global/document */ \"./node_modules/global/document.js\");\n\nvar _globalDocument2 = _interopRequireDefault(_globalDocument);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _virtualSourceBuffer = __webpack_require__(/*! ./virtual-source-buffer */ \"./node_modules/videojs-contrib-media-sources/es5/virtual-source-buffer.js\");\n\nvar _virtualSourceBuffer2 = _interopRequireDefault(_virtualSourceBuffer);\n\nvar _addTextTrackData = __webpack_require__(/*! ./add-text-track-data */ \"./node_modules/videojs-contrib-media-sources/es5/add-text-track-data.js\");\n\nvar _codecUtils = __webpack_require__(/*! ./codec-utils */ \"./node_modules/videojs-contrib-media-sources/es5/codec-utils.js\");\n\n/**\n * Our MediaSource implementation in HTML, mimics native\n * MediaSource where/if possible.\n *\n * @link https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\n * @class HtmlMediaSource\n * @extends videojs.EventTarget\n */\n\nvar HtmlMediaSource = (function (_videojs$EventTarget) {\n  _inherits(HtmlMediaSource, _videojs$EventTarget);\n\n  function HtmlMediaSource() {\n    var _this = this;\n\n    _classCallCheck(this, HtmlMediaSource);\n\n    _get(Object.getPrototypeOf(HtmlMediaSource.prototype), 'constructor', this).call(this);\n    var property = undefined;\n\n    this.nativeMediaSource_ = new _globalWindow2['default'].MediaSource();\n    // delegate to the native MediaSource's methods by default\n    for (property in this.nativeMediaSource_) {\n      if (!(property in HtmlMediaSource.prototype) && typeof this.nativeMediaSource_[property] === 'function') {\n        this[property] = this.nativeMediaSource_[property].bind(this.nativeMediaSource_);\n      }\n    }\n\n    // emulate `duration` and `seekable` until seeking can be\n    // handled uniformly for live streams\n    // see https://github.com/w3c/media-source/issues/5\n    this.duration_ = NaN;\n    Object.defineProperty(this, 'duration', {\n      get: function get() {\n        if (this.duration_ === Infinity) {\n          return this.duration_;\n        }\n        return this.nativeMediaSource_.duration;\n      },\n      set: function set(duration) {\n        this.duration_ = duration;\n        if (duration !== Infinity) {\n          this.nativeMediaSource_.duration = duration;\n          return;\n        }\n      }\n    });\n    Object.defineProperty(this, 'seekable', {\n      get: function get() {\n        if (this.duration_ === Infinity) {\n          return _videoJs2['default'].createTimeRanges([[0, this.nativeMediaSource_.duration]]);\n        }\n        return this.nativeMediaSource_.seekable;\n      }\n    });\n\n    Object.defineProperty(this, 'readyState', {\n      get: function get() {\n        return this.nativeMediaSource_.readyState;\n      }\n    });\n\n    Object.defineProperty(this, 'activeSourceBuffers', {\n      get: function get() {\n        return this.activeSourceBuffers_;\n      }\n    });\n\n    // the list of virtual and native SourceBuffers created by this\n    // MediaSource\n    this.sourceBuffers = [];\n\n    this.activeSourceBuffers_ = [];\n\n    /**\n     * update the list of active source buffers based upon various\n     * imformation from HLS and video.js\n     *\n     * @private\n     */\n    this.updateActiveSourceBuffers_ = function () {\n      // Retain the reference but empty the array\n      _this.activeSourceBuffers_.length = 0;\n\n      // If there is only one source buffer, then it will always be active and audio will\n      // be disabled based on the codec of the source buffer\n      if (_this.sourceBuffers.length === 1) {\n        var sourceBuffer = _this.sourceBuffers[0];\n\n        sourceBuffer.appendAudioInitSegment_ = true;\n        sourceBuffer.audioDisabled_ = !sourceBuffer.audioCodec_;\n        _this.activeSourceBuffers_.push(sourceBuffer);\n        return;\n      }\n\n      // There are 2 source buffers, a combined (possibly video only) source buffer and\n      // and an audio only source buffer.\n      // By default, the audio in the combined virtual source buffer is enabled\n      // and the audio-only source buffer (if it exists) is disabled.\n      var disableCombined = false;\n      var disableAudioOnly = true;\n\n      // TODO: maybe we can store the sourcebuffers on the track objects?\n      // safari may do something like this\n      for (var i = 0; i < _this.player_.audioTracks().length; i++) {\n        var track = _this.player_.audioTracks()[i];\n\n        if (track.enabled && track.kind !== 'main') {\n          // The enabled track is an alternate audio track so disable the audio in\n          // the combined source buffer and enable the audio-only source buffer.\n          disableCombined = true;\n          disableAudioOnly = false;\n          break;\n        }\n      }\n\n      _this.sourceBuffers.forEach(function (sourceBuffer) {\n        /* eslinst-disable */\n        // TODO once codecs are required, we can switch to using the codecs to determine\n        //      what stream is the video stream, rather than relying on videoTracks\n        /* eslinst-enable */\n\n        sourceBuffer.appendAudioInitSegment_ = true;\n\n        if (sourceBuffer.videoCodec_ && sourceBuffer.audioCodec_) {\n          // combined\n          sourceBuffer.audioDisabled_ = disableCombined;\n        } else if (sourceBuffer.videoCodec_ && !sourceBuffer.audioCodec_) {\n          // If the \"combined\" source buffer is video only, then we do not want\n          // disable the audio-only source buffer (this is mostly for demuxed\n          // audio and video hls)\n          sourceBuffer.audioDisabled_ = true;\n          disableAudioOnly = false;\n        } else if (!sourceBuffer.videoCodec_ && sourceBuffer.audioCodec_) {\n          // audio only\n          sourceBuffer.audioDisabled_ = disableAudioOnly;\n          if (disableAudioOnly) {\n            return;\n          }\n        }\n\n        _this.activeSourceBuffers_.push(sourceBuffer);\n      });\n    };\n\n    this.onPlayerMediachange_ = function () {\n      _this.sourceBuffers.forEach(function (sourceBuffer) {\n        sourceBuffer.appendAudioInitSegment_ = true;\n      });\n    };\n\n    this.onHlsReset_ = function () {\n      _this.sourceBuffers.forEach(function (sourceBuffer) {\n        if (sourceBuffer.transmuxer_) {\n          sourceBuffer.transmuxer_.postMessage({ action: 'resetCaptions' });\n        }\n      });\n    };\n\n    this.onHlsSegmentTimeMapping_ = function (event) {\n      _this.sourceBuffers.forEach(function (buffer) {\n        return buffer.timeMapping_ = event.mapping;\n      });\n    };\n\n    // Re-emit MediaSource events on the polyfill\n    ['sourceopen', 'sourceclose', 'sourceended'].forEach(function (eventName) {\n      this.nativeMediaSource_.addEventListener(eventName, this.trigger.bind(this));\n    }, this);\n\n    // capture the associated player when the MediaSource is\n    // successfully attached\n    this.on('sourceopen', function (event) {\n      // Get the player this MediaSource is attached to\n      var video = _globalDocument2['default'].querySelector('[src=\"' + _this.url_ + '\"]');\n\n      if (!video) {\n        return;\n      }\n\n      _this.player_ = (0, _videoJs2['default'])(video.parentNode);\n\n      // hls-reset is fired by videojs.Hls on to the tech after the main SegmentLoader\n      // resets its state and flushes the buffer\n      _this.player_.tech_.on('hls-reset', _this.onHlsReset_);\n      // hls-segment-time-mapping is fired by videojs.Hls on to the tech after the main\n      // SegmentLoader inspects an MTS segment and has an accurate stream to display\n      // time mapping\n      _this.player_.tech_.on('hls-segment-time-mapping', _this.onHlsSegmentTimeMapping_);\n\n      if (_this.player_.audioTracks && _this.player_.audioTracks()) {\n        _this.player_.audioTracks().on('change', _this.updateActiveSourceBuffers_);\n        _this.player_.audioTracks().on('addtrack', _this.updateActiveSourceBuffers_);\n        _this.player_.audioTracks().on('removetrack', _this.updateActiveSourceBuffers_);\n      }\n\n      _this.player_.on('mediachange', _this.onPlayerMediachange_);\n    });\n\n    this.on('sourceended', function (event) {\n      var duration = (0, _addTextTrackData.durationOfVideo)(_this.duration);\n\n      for (var i = 0; i < _this.sourceBuffers.length; i++) {\n        var sourcebuffer = _this.sourceBuffers[i];\n        var cues = sourcebuffer.metadataTrack_ && sourcebuffer.metadataTrack_.cues;\n\n        if (cues && cues.length) {\n          cues[cues.length - 1].endTime = duration;\n        }\n      }\n    });\n\n    // explicitly terminate any WebWorkers that were created\n    // by SourceHandlers\n    this.on('sourceclose', function (event) {\n      this.sourceBuffers.forEach(function (sourceBuffer) {\n        if (sourceBuffer.transmuxer_) {\n          sourceBuffer.transmuxer_.terminate();\n        }\n      });\n\n      this.sourceBuffers.length = 0;\n      if (!this.player_) {\n        return;\n      }\n\n      if (this.player_.audioTracks && this.player_.audioTracks()) {\n        this.player_.audioTracks().off('change', this.updateActiveSourceBuffers_);\n        this.player_.audioTracks().off('addtrack', this.updateActiveSourceBuffers_);\n        this.player_.audioTracks().off('removetrack', this.updateActiveSourceBuffers_);\n      }\n\n      // We can only change this if the player hasn't been disposed of yet\n      // because `off` eventually tries to use the el_ property. If it has\n      // been disposed of, then don't worry about it because there are no\n      // event handlers left to unbind anyway\n      if (this.player_.el_) {\n        this.player_.off('mediachange', this.onPlayerMediachange_);\n        this.player_.tech_.off('hls-reset', this.onHlsReset_);\n        this.player_.tech_.off('hls-segment-time-mapping', this.onHlsSegmentTimeMapping_);\n      }\n    });\n  }\n\n  /**\n   * Add a range that that can now be seeked to.\n   *\n   * @param {Double} start where to start the addition\n   * @param {Double} end where to end the addition\n   * @private\n   */\n\n  _createClass(HtmlMediaSource, [{\n    key: 'addSeekableRange_',\n    value: function addSeekableRange_(start, end) {\n      var error = undefined;\n\n      if (this.duration !== Infinity) {\n        error = new Error('MediaSource.addSeekableRange() can only be invoked ' + 'when the duration is Infinity');\n        error.name = 'InvalidStateError';\n        error.code = 11;\n        throw error;\n      }\n\n      if (end > this.nativeMediaSource_.duration || isNaN(this.nativeMediaSource_.duration)) {\n        this.nativeMediaSource_.duration = end;\n      }\n    }\n\n    /**\n     * Add a source buffer to the media source.\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/MediaSource/addSourceBuffer\n     * @param {String} type the content-type of the content\n     * @return {Object} the created source buffer\n     */\n  }, {\n    key: 'addSourceBuffer',\n    value: function addSourceBuffer(type) {\n      var buffer = undefined;\n      var parsedType = (0, _codecUtils.parseContentType)(type);\n\n      // Create a VirtualSourceBuffer to transmux MPEG-2 transport\n      // stream segments into fragmented MP4s\n      if (/^(video|audio)\\/mp2t$/i.test(parsedType.type)) {\n        var codecs = [];\n\n        if (parsedType.parameters && parsedType.parameters.codecs) {\n          codecs = parsedType.parameters.codecs.split(',');\n          codecs = (0, _codecUtils.translateLegacyCodecs)(codecs);\n          codecs = codecs.filter(function (codec) {\n            return (0, _codecUtils.isAudioCodec)(codec) || (0, _codecUtils.isVideoCodec)(codec);\n          });\n        }\n\n        if (codecs.length === 0) {\n          codecs = ['avc1.4d400d', 'mp4a.40.2'];\n        }\n\n        buffer = new _virtualSourceBuffer2['default'](this, codecs);\n\n        if (this.sourceBuffers.length !== 0) {\n          // If another VirtualSourceBuffer already exists, then we are creating a\n          // SourceBuffer for an alternate audio track and therefore we know that\n          // the source has both an audio and video track.\n          // That means we should trigger the manual creation of the real\n          // SourceBuffers instead of waiting for the transmuxer to return data\n          this.sourceBuffers[0].createRealSourceBuffers_();\n          buffer.createRealSourceBuffers_();\n\n          // Automatically disable the audio on the first source buffer if\n          // a second source buffer is ever created\n          this.sourceBuffers[0].audioDisabled_ = true;\n        }\n      } else {\n        // delegate to the native implementation\n        buffer = this.nativeMediaSource_.addSourceBuffer(type);\n      }\n\n      this.sourceBuffers.push(buffer);\n      return buffer;\n    }\n  }]);\n\n  return HtmlMediaSource;\n})(_videoJs2['default'].EventTarget);\n\nexports['default'] = HtmlMediaSource;\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/html-media-source.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js":
/*!**********************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js ***!
  \**********************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file remove-cues-from-track.js\n */\n\n/**\n * Remove cues from a track on video.js.\n *\n * @param {Double} start start of where we should remove the cue\n * @param {Double} end end of where the we should remove the cue\n * @param {Object} track the text track to remove the cues from\n * @private\n */\n\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nvar removeCuesFromTrack = function removeCuesFromTrack(start, end, track) {\n  var i = undefined;\n  var cue = undefined;\n\n  if (!track) {\n    return;\n  }\n\n  if (!track.cues) {\n    return;\n  }\n\n  i = track.cues.length;\n\n  while (i--) {\n    cue = track.cues[i];\n\n    // Remove any overlapping cue\n    if (cue.startTime <= end && cue.endTime >= start) {\n      track.removeCue(cue);\n    }\n  }\n};\n\nexports[\"default\"] = removeCuesFromTrack;\nmodule.exports = exports[\"default\"];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/transmuxer-worker.js":
/*!*****************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/transmuxer-worker.js ***!
  \*****************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file transmuxer-worker.js\n */\n\n/**\n * videojs-contrib-media-sources\n *\n * Copyright (c) 2015 Brightcove\n * All rights reserved.\n *\n * Handles communication between the browser-world and the mux.js\n * transmuxer running inside of a WebWorker by exposing a simple\n * message-based interface to a Transmuxer object.\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _muxJsLibMp4 = __webpack_require__(/*! mux.js/lib/mp4 */ \"./node_modules/mux.js/lib/mp4/index.js\");\n\nvar _muxJsLibMp42 = _interopRequireDefault(_muxJsLibMp4);\n\n/**\n * Re-emits transmuxer events by converting them into messages to the\n * world outside the worker.\n *\n * @param {Object} transmuxer the transmuxer to wire events on\n * @private\n */\nvar wireTransmuxerEvents = function wireTransmuxerEvents(transmuxer) {\n  transmuxer.on('data', function (segment) {\n    // transfer ownership of the underlying ArrayBuffer\n    // instead of doing a copy to save memory\n    // ArrayBuffers are transferable but generic TypedArrays are not\n    // @link https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers#Passing_data_by_transferring_ownership_(transferable_objects)\n    var initArray = segment.initSegment;\n\n    segment.initSegment = {\n      data: initArray.buffer,\n      byteOffset: initArray.byteOffset,\n      byteLength: initArray.byteLength\n    };\n\n    var typedArray = segment.data;\n\n    segment.data = typedArray.buffer;\n    _globalWindow2['default'].postMessage({\n      action: 'data',\n      segment: segment,\n      byteOffset: typedArray.byteOffset,\n      byteLength: typedArray.byteLength\n    }, [segment.data]);\n  });\n\n  if (transmuxer.captionStream) {\n    transmuxer.captionStream.on('data', function (caption) {\n      _globalWindow2['default'].postMessage({\n        action: 'caption',\n        data: caption\n      });\n    });\n  }\n\n  transmuxer.on('done', function (data) {\n    _globalWindow2['default'].postMessage({ action: 'done' });\n  });\n\n  transmuxer.on('gopInfo', function (gopInfo) {\n    _globalWindow2['default'].postMessage({\n      action: 'gopInfo',\n      gopInfo: gopInfo\n    });\n  });\n};\n\n/**\n * All incoming messages route through this hash. If no function exists\n * to handle an incoming message, then we ignore the message.\n *\n * @class MessageHandlers\n * @param {Object} options the options to initialize with\n */\n\nvar MessageHandlers = (function () {\n  function MessageHandlers(options) {\n    _classCallCheck(this, MessageHandlers);\n\n    this.options = options || {};\n    this.init();\n  }\n\n  /**\n   * Our web wroker interface so that things can talk to mux.js\n   * that will be running in a web worker. the scope is passed to this by\n   * webworkify.\n   *\n   * @param {Object} self the scope for the web worker\n   */\n\n  /**\n   * initialize our web worker and wire all the events.\n   */\n\n  _createClass(MessageHandlers, [{\n    key: 'init',\n    value: function init() {\n      if (this.transmuxer) {\n        this.transmuxer.dispose();\n      }\n      this.transmuxer = new _muxJsLibMp42['default'].Transmuxer(this.options);\n      wireTransmuxerEvents(this.transmuxer);\n    }\n\n    /**\n     * Adds data (a ts segment) to the start of the transmuxer pipeline for\n     * processing.\n     *\n     * @param {ArrayBuffer} data data to push into the muxer\n     */\n  }, {\n    key: 'push',\n    value: function push(data) {\n      // Cast array buffer to correct type for transmuxer\n      var segment = new Uint8Array(data.data, data.byteOffset, data.byteLength);\n\n      this.transmuxer.push(segment);\n    }\n\n    /**\n     * Recreate the transmuxer so that the next segment added via `push`\n     * start with a fresh transmuxer.\n     */\n  }, {\n    key: 'reset',\n    value: function reset() {\n      this.init();\n    }\n\n    /**\n     * Set the value that will be used as the `baseMediaDecodeTime` time for the\n     * next segment pushed in. Subsequent segments will have their `baseMediaDecodeTime`\n     * set relative to the first based on the PTS values.\n     *\n     * @param {Object} data used to set the timestamp offset in the muxer\n     */\n  }, {\n    key: 'setTimestampOffset',\n    value: function setTimestampOffset(data) {\n      var timestampOffset = data.timestampOffset || 0;\n\n      this.transmuxer.setBaseMediaDecodeTime(Math.round(timestampOffset * 90000));\n    }\n  }, {\n    key: 'setAudioAppendStart',\n    value: function setAudioAppendStart(data) {\n      this.transmuxer.setAudioAppendStart(Math.ceil(data.appendStart * 90000));\n    }\n\n    /**\n     * Forces the pipeline to finish processing the last segment and emit it's\n     * results.\n     *\n     * @param {Object} data event data, not really used\n     */\n  }, {\n    key: 'flush',\n    value: function flush(data) {\n      this.transmuxer.flush();\n    }\n  }, {\n    key: 'resetCaptions',\n    value: function resetCaptions() {\n      this.transmuxer.resetCaptions();\n    }\n  }, {\n    key: 'alignGopsWith',\n    value: function alignGopsWith(data) {\n      this.transmuxer.alignGopsWith(data.gopsToAlignWith.slice());\n    }\n  }]);\n\n  return MessageHandlers;\n})();\n\nvar TransmuxerWorker = function TransmuxerWorker(self) {\n  self.onmessage = function (event) {\n    if (event.data.action === 'init' && event.data.options) {\n      this.messageHandlers = new MessageHandlers(event.data.options);\n      return;\n    }\n\n    if (!this.messageHandlers) {\n      this.messageHandlers = new MessageHandlers();\n    }\n\n    if (event.data && event.data.action && event.data.action !== 'init') {\n      if (this.messageHandlers[event.data.action]) {\n        this.messageHandlers[event.data.action](event.data);\n      }\n    }\n  };\n};\n\nexports['default'] = function (self) {\n  return new TransmuxerWorker(self);\n};\n\nmodule.exports = exports['default'];\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/transmuxer-worker.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/videojs-contrib-media-sources.js":
/*!*****************************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/videojs-contrib-media-sources.js ***!
  \*****************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file videojs-contrib-media-sources.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nvar _globalWindow = __webpack_require__(/*! global/window */ \"./node_modules/global/window.js\");\n\nvar _globalWindow2 = _interopRequireDefault(_globalWindow);\n\nvar _flashMediaSource = __webpack_require__(/*! ./flash-media-source */ \"./node_modules/videojs-contrib-media-sources/es5/flash-media-source.js\");\n\nvar _flashMediaSource2 = _interopRequireDefault(_flashMediaSource);\n\nvar _htmlMediaSource = __webpack_require__(/*! ./html-media-source */ \"./node_modules/videojs-contrib-media-sources/es5/html-media-source.js\");\n\nvar _htmlMediaSource2 = _interopRequireDefault(_htmlMediaSource);\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar urlCount = 0;\n\n// ------------\n// Media Source\n// ------------\n\nvar defaults = {\n  // how to determine the MediaSource implementation to use. There\n  // are three available modes:\n  // - auto: use native MediaSources where available and Flash\n  //   everywhere else\n  // - html5: always use native MediaSources\n  // - flash: always use the Flash MediaSource polyfill\n  mode: 'auto'\n};\n\n// store references to the media sources so they can be connected\n// to a video element (a swf object)\n// TODO: can we store this somewhere local to this module?\n_videoJs2['default'].mediaSources = {};\n\n/**\n * Provide a method for a swf object to notify JS that a\n * media source is now open.\n *\n * @param {String} msObjectURL string referencing the MSE Object URL\n * @param {String} swfId the swf id\n */\nvar open = function open(msObjectURL, swfId) {\n  var mediaSource = _videoJs2['default'].mediaSources[msObjectURL];\n\n  if (mediaSource) {\n    mediaSource.trigger({ type: 'sourceopen', swfId: swfId });\n  } else {\n    throw new Error('Media Source not found (Video.js)');\n  }\n};\n\n/**\n * Check to see if the native MediaSource object exists and supports\n * an MP4 container with both H.264 video and AAC-LC audio.\n *\n * @return {Boolean} if  native media sources are supported\n */\nvar supportsNativeMediaSources = function supportsNativeMediaSources() {\n  return !!_globalWindow2['default'].MediaSource && !!_globalWindow2['default'].MediaSource.isTypeSupported && _globalWindow2['default'].MediaSource.isTypeSupported('video/mp4;codecs=\"avc1.4d400d,mp4a.40.2\"');\n};\n\n/**\n * An emulation of the MediaSource API so that we can support\n * native and non-native functionality such as flash and\n * video/mp2t videos. returns an instance of HtmlMediaSource or\n * FlashMediaSource depending on what is supported and what options\n * are passed in.\n *\n * @link https://developer.mozilla.org/en-US/docs/Web/API/MediaSource/MediaSource\n * @param {Object} options options to use during setup.\n */\nvar MediaSource = function MediaSource(options) {\n  var settings = _videoJs2['default'].mergeOptions(defaults, options);\n\n  this.MediaSource = {\n    open: open,\n    supportsNativeMediaSources: supportsNativeMediaSources\n  };\n\n  // determine whether HTML MediaSources should be used\n  if (settings.mode === 'html5' || settings.mode === 'auto' && supportsNativeMediaSources()) {\n    return new _htmlMediaSource2['default']();\n  } else if (_videoJs2['default'].getTech('Flash')) {\n    return new _flashMediaSource2['default']();\n  }\n\n  throw new Error('Cannot use Flash or Html5 to create a MediaSource for this video');\n};\n\nexports.MediaSource = MediaSource;\nMediaSource.open = open;\nMediaSource.supportsNativeMediaSources = supportsNativeMediaSources;\n\n/**\n * A wrapper around the native URL for our MSE object\n * implementation, this object is exposed under videojs.URL\n *\n * @link https://developer.mozilla.org/en-US/docs/Web/API/URL/URL\n */\nvar URL = {\n  /**\n   * A wrapper around the native createObjectURL for our objects.\n   * This function maps a native or emulated mediaSource to a blob\n   * url so that it can be loaded into video.js\n   *\n   * @link https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL\n   * @param {MediaSource} object the object to create a blob url to\n   */\n  createObjectURL: function createObjectURL(object) {\n    var objectUrlPrefix = 'blob:vjs-media-source/';\n    var url = undefined;\n\n    // use the native MediaSource to generate an object URL\n    if (object instanceof _htmlMediaSource2['default']) {\n      url = _globalWindow2['default'].URL.createObjectURL(object.nativeMediaSource_);\n      object.url_ = url;\n      return url;\n    }\n    // if the object isn't an emulated MediaSource, delegate to the\n    // native implementation\n    if (!(object instanceof _flashMediaSource2['default'])) {\n      url = _globalWindow2['default'].URL.createObjectURL(object);\n      object.url_ = url;\n      return url;\n    }\n\n    // build a URL that can be used to map back to the emulated\n    // MediaSource\n    url = objectUrlPrefix + urlCount;\n\n    urlCount++;\n\n    // setup the mapping back to object\n    _videoJs2['default'].mediaSources[url] = object;\n\n    return url;\n  }\n};\n\nexports.URL = URL;\n_videoJs2['default'].MediaSource = MediaSource;\n_videoJs2['default'].URL = URL;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/videojs-contrib-media-sources.js?");

/***/ }),

/***/ "./node_modules/videojs-contrib-media-sources/es5/virtual-source-buffer.js":
/*!*********************************************************************************!*\
  !*** ./node_modules/videojs-contrib-media-sources/es5/virtual-source-buffer.js ***!
  \*********************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/**\n * @file virtual-source-buffer.js\n */\n\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\n\nvar _createClass = (function () { function defineProperties(target, props) { for (var i = 0; i < props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if ('value' in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; })();\n\nvar _get = function get(_x, _x2, _x3) { var _again = true; _function: while (_again) { var object = _x, property = _x2, receiver = _x3; _again = false; if (object === null) object = Function.prototype; var desc = Object.getOwnPropertyDescriptor(object, property); if (desc === undefined) { var parent = Object.getPrototypeOf(object); if (parent === null) { return undefined; } else { _x = parent; _x2 = property; _x3 = receiver; _again = true; desc = parent = undefined; continue _function; } } else if ('value' in desc) { return desc.value; } else { var getter = desc.get; if (getter === undefined) { return undefined; } return getter.call(receiver); } } };\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { 'default': obj }; }\n\nfunction _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError('Cannot call a class as a function'); } }\n\nfunction _inherits(subClass, superClass) { if (typeof superClass !== 'function' && superClass !== null) { throw new TypeError('Super expression must either be null or a function, not ' + typeof superClass); } subClass.prototype = Object.create(superClass && superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }\n\nvar _videoJs = __webpack_require__(/*! video.js */ \"./node_modules/video.js/dist/video.cjs.js\");\n\nvar _videoJs2 = _interopRequireDefault(_videoJs);\n\nvar _createTextTracksIfNecessary = __webpack_require__(/*! ./create-text-tracks-if-necessary */ \"./node_modules/videojs-contrib-media-sources/es5/create-text-tracks-if-necessary.js\");\n\nvar _createTextTracksIfNecessary2 = _interopRequireDefault(_createTextTracksIfNecessary);\n\nvar _removeCuesFromTrack = __webpack_require__(/*! ./remove-cues-from-track */ \"./node_modules/videojs-contrib-media-sources/es5/remove-cues-from-track.js\");\n\nvar _removeCuesFromTrack2 = _interopRequireDefault(_removeCuesFromTrack);\n\nvar _addTextTrackData = __webpack_require__(/*! ./add-text-track-data */ \"./node_modules/videojs-contrib-media-sources/es5/add-text-track-data.js\");\n\nvar _webwackify = __webpack_require__(/*! webwackify */ \"./node_modules/webwackify/index.js\");\n\nvar _webwackify2 = _interopRequireDefault(_webwackify);\n\nvar _transmuxerWorker = __webpack_require__(/*! ./transmuxer-worker */ \"./node_modules/videojs-contrib-media-sources/es5/transmuxer-worker.js\");\n\nvar _transmuxerWorker2 = _interopRequireDefault(_transmuxerWorker);\n\nvar _codecUtils = __webpack_require__(/*! ./codec-utils */ \"./node_modules/videojs-contrib-media-sources/es5/codec-utils.js\");\n\nvar resolveTransmuxWorker = function resolveTransmuxWorker() {\n  var result = undefined;\n\n  try {\n    result = /*require.resolve*/(/*! ./transmuxer-worker */ \"./node_modules/videojs-contrib-media-sources/es5/transmuxer-worker.js\");\n  } catch (e) {\n    // no result\n  }\n\n  return result;\n};\n\n// We create a wrapper around the SourceBuffer so that we can manage the\n// state of the `updating` property manually. We have to do this because\n// Firefox changes `updating` to false long before triggering `updateend`\n// events and that was causing strange problems in videojs-contrib-hls\nvar makeWrappedSourceBuffer = function makeWrappedSourceBuffer(mediaSource, mimeType) {\n  var sourceBuffer = mediaSource.addSourceBuffer(mimeType);\n  var wrapper = Object.create(null);\n\n  wrapper.updating = false;\n  wrapper.realBuffer_ = sourceBuffer;\n\n  var _loop = function (key) {\n    if (typeof sourceBuffer[key] === 'function') {\n      wrapper[key] = function () {\n        return sourceBuffer[key].apply(sourceBuffer, arguments);\n      };\n    } else if (typeof wrapper[key] === 'undefined') {\n      Object.defineProperty(wrapper, key, {\n        get: function get() {\n          return sourceBuffer[key];\n        },\n        set: function set(v) {\n          return sourceBuffer[key] = v;\n        }\n      });\n    }\n  };\n\n  for (var key in sourceBuffer) {\n    _loop(key);\n  }\n\n  return wrapper;\n};\n\n/**\n * Returns a list of gops in the buffer that have a pts value of 3 seconds or more in\n * front of current time.\n *\n * @param {Array} buffer\n *        The current buffer of gop information\n * @param {Player} player\n *        The player instance\n * @param {Double} mapping\n *        Offset to map display time to stream presentation time\n * @return {Array}\n *         List of gops considered safe to append over\n */\nvar gopsSafeToAlignWith = function gopsSafeToAlignWith(buffer, player, mapping) {\n  if (!player || !buffer.length) {\n    return [];\n  }\n\n  // pts value for current time + 3 seconds to give a bit more wiggle room\n  var currentTimePts = Math.ceil((player.currentTime() - mapping + 3) * 90000);\n\n  var i = undefined;\n\n  for (i = 0; i < buffer.length; i++) {\n    if (buffer[i].pts > currentTimePts) {\n      break;\n    }\n  }\n\n  return buffer.slice(i);\n};\n\nexports.gopsSafeToAlignWith = gopsSafeToAlignWith;\n/**\n * Appends gop information (timing and byteLength) received by the transmuxer for the\n * gops appended in the last call to appendBuffer\n *\n * @param {Array} buffer\n *        The current buffer of gop information\n * @param {Array} gops\n *        List of new gop information\n * @param {boolean} replace\n *        If true, replace the buffer with the new gop information. If false, append the\n *        new gop information to the buffer in the right location of time.\n * @return {Array}\n *         Updated list of gop information\n */\nvar updateGopBuffer = function updateGopBuffer(buffer, gops, replace) {\n  if (!gops.length) {\n    return buffer;\n  }\n\n  if (replace) {\n    // If we are in safe append mode, then completely overwrite the gop buffer\n    // with the most recent appeneded data. This will make sure that when appending\n    // future segments, we only try to align with gops that are both ahead of current\n    // time and in the last segment appended.\n    return gops.slice();\n  }\n\n  var start = gops[0].pts;\n\n  var i = 0;\n\n  for (i; i < buffer.length; i++) {\n    if (buffer[i].pts >= start) {\n      break;\n    }\n  }\n\n  return buffer.slice(0, i).concat(gops);\n};\n\nexports.updateGopBuffer = updateGopBuffer;\n/**\n * Removes gop information in buffer that overlaps with provided start and end\n *\n * @param {Array} buffer\n *        The current buffer of gop information\n * @param {Double} start\n *        position to start the remove at\n * @param {Double} end\n *        position to end the remove at\n * @param {Double} mapping\n *        Offset to map display time to stream presentation time\n */\nvar removeGopBuffer = function removeGopBuffer(buffer, start, end, mapping) {\n  var startPts = Math.ceil((start - mapping) * 90000);\n  var endPts = Math.ceil((end - mapping) * 90000);\n  var updatedBuffer = buffer.slice();\n\n  var i = buffer.length;\n\n  while (i--) {\n    if (buffer[i].pts <= endPts) {\n      break;\n    }\n  }\n\n  if (i === -1) {\n    // no removal because end of remove range is before start of buffer\n    return updatedBuffer;\n  }\n\n  var j = i + 1;\n\n  while (j--) {\n    if (buffer[j].pts <= startPts) {\n      break;\n    }\n  }\n\n  // clamp remove range start to 0 index\n  j = Math.max(j, 0);\n\n  updatedBuffer.splice(j, i - j + 1);\n\n  return updatedBuffer;\n};\n\nexports.removeGopBuffer = removeGopBuffer;\n/**\n * VirtualSourceBuffers exist so that we can transmux non native formats\n * into a native format, but keep the same api as a native source buffer.\n * It creates a transmuxer, that works in its own thread (a web worker) and\n * that transmuxer muxes the data into a native format. VirtualSourceBuffer will\n * then send all of that data to the naive sourcebuffer so that it is\n * indestinguishable from a natively supported format.\n *\n * @param {HtmlMediaSource} mediaSource the parent mediaSource\n * @param {Array} codecs array of codecs that we will be dealing with\n * @class VirtualSourceBuffer\n * @extends video.js.EventTarget\n */\n\nvar VirtualSourceBuffer = (function (_videojs$EventTarget) {\n  _inherits(VirtualSourceBuffer, _videojs$EventTarget);\n\n  function VirtualSourceBuffer(mediaSource, codecs) {\n    var _this = this;\n\n    _classCallCheck(this, VirtualSourceBuffer);\n\n    _get(Object.getPrototypeOf(VirtualSourceBuffer.prototype), 'constructor', this).call(this, _videoJs2['default'].EventTarget);\n    this.timestampOffset_ = 0;\n    this.pendingBuffers_ = [];\n    this.bufferUpdating_ = false;\n\n    this.mediaSource_ = mediaSource;\n    this.codecs_ = codecs;\n    this.audioCodec_ = null;\n    this.videoCodec_ = null;\n    this.audioDisabled_ = false;\n    this.appendAudioInitSegment_ = true;\n    this.gopBuffer_ = [];\n    this.timeMapping_ = 0;\n    this.safeAppend_ = _videoJs2['default'].browser.IE_VERSION >= 11;\n\n    var options = {\n      remux: false,\n      alignGopsAtEnd: this.safeAppend_\n    };\n\n    this.codecs_.forEach(function (codec) {\n      if ((0, _codecUtils.isAudioCodec)(codec)) {\n        _this.audioCodec_ = codec;\n      } else if ((0, _codecUtils.isVideoCodec)(codec)) {\n        _this.videoCodec_ = codec;\n      }\n    });\n\n    // append muxed segments to their respective native buffers as\n    // soon as they are available\n    this.transmuxer_ = (0, _webwackify2['default'])(_transmuxerWorker2['default'], resolveTransmuxWorker());\n    this.transmuxer_.postMessage({ action: 'init', options: options });\n\n    this.transmuxer_.onmessage = function (event) {\n      if (event.data.action === 'data') {\n        return _this.data_(event);\n      }\n\n      if (event.data.action === 'done') {\n        return _this.done_(event);\n      }\n\n      if (event.data.action === 'gopInfo') {\n        return _this.appendGopInfo_(event);\n      }\n    };\n\n    // this timestampOffset is a property with the side-effect of resetting\n    // baseMediaDecodeTime in the transmuxer on the setter\n    Object.defineProperty(this, 'timestampOffset', {\n      get: function get() {\n        return this.timestampOffset_;\n      },\n      set: function set(val) {\n        if (typeof val === 'number' && val >= 0) {\n          this.timestampOffset_ = val;\n          this.appendAudioInitSegment_ = true;\n\n          // reset gop buffer on timestampoffset as this signals a change in timeline\n          this.gopBuffer_.length = 0;\n          this.timeMapping_ = 0;\n\n          // We have to tell the transmuxer to set the baseMediaDecodeTime to\n          // the desired timestampOffset for the next segment\n          this.transmuxer_.postMessage({\n            action: 'setTimestampOffset',\n            timestampOffset: val\n          });\n        }\n      }\n    });\n\n    // setting the append window affects both source buffers\n    Object.defineProperty(this, 'appendWindowStart', {\n      get: function get() {\n        return (this.videoBuffer_ || this.audioBuffer_).appendWindowStart;\n      },\n      set: function set(start) {\n        if (this.videoBuffer_) {\n          this.videoBuffer_.appendWindowStart = start;\n        }\n        if (this.audioBuffer_) {\n          this.audioBuffer_.appendWindowStart = start;\n        }\n      }\n    });\n\n    // this buffer is \"updating\" if either of its native buffers are\n    Object.defineProperty(this, 'updating', {\n      get: function get() {\n        return !!(this.bufferUpdating_ || !this.audioDisabled_ && this.audioBuffer_ && this.audioBuffer_.updating || this.videoBuffer_ && this.videoBuffer_.updating);\n      }\n    });\n\n    // the buffered property is the intersection of the buffered\n    // ranges of the native source buffers\n    Object.defineProperty(this, 'buffered', {\n      get: function get() {\n        var start = null;\n        var end = null;\n        var arity = 0;\n        var extents = [];\n        var ranges = [];\n\n        // neither buffer has been created yet\n        if (!this.videoBuffer_ && !this.audioBuffer_) {\n          return _videoJs2['default'].createTimeRange();\n        }\n\n        // only one buffer is configured\n        if (!this.videoBuffer_) {\n          return this.audioBuffer_.buffered;\n        }\n        if (!this.audioBuffer_) {\n          return this.videoBuffer_.buffered;\n        }\n\n        // both buffers are configured\n        if (this.audioDisabled_) {\n          return this.videoBuffer_.buffered;\n        }\n\n        // both buffers are empty\n        if (this.videoBuffer_.buffered.length === 0 && this.audioBuffer_.buffered.length === 0) {\n          return _videoJs2['default'].createTimeRange();\n        }\n\n        // Handle the case where we have both buffers and create an\n        // intersection of the two\n        var videoBuffered = this.videoBuffer_.buffered;\n        var audioBuffered = this.audioBuffer_.buffered;\n        var count = videoBuffered.length;\n\n        // A) Gather up all start and end times\n        while (count--) {\n          extents.push({ time: videoBuffered.start(count), type: 'start' });\n          extents.push({ time: videoBuffered.end(count), type: 'end' });\n        }\n        count = audioBuffered.length;\n        while (count--) {\n          extents.push({ time: audioBuffered.start(count), type: 'start' });\n          extents.push({ time: audioBuffered.end(count), type: 'end' });\n        }\n        // B) Sort them by time\n        extents.sort(function (a, b) {\n          return a.time - b.time;\n        });\n\n        // C) Go along one by one incrementing arity for start and decrementing\n        //    arity for ends\n        for (count = 0; count < extents.length; count++) {\n          if (extents[count].type === 'start') {\n            arity++;\n\n            // D) If arity is ever incremented to 2 we are entering an\n            //    overlapping range\n            if (arity === 2) {\n              start = extents[count].time;\n            }\n          } else if (extents[count].type === 'end') {\n            arity--;\n\n            // E) If arity is ever decremented to 1 we leaving an\n            //    overlapping range\n            if (arity === 1) {\n              end = extents[count].time;\n            }\n          }\n\n          // F) Record overlapping ranges\n          if (start !== null && end !== null) {\n            ranges.push([start, end]);\n            start = null;\n            end = null;\n          }\n        }\n\n        return _videoJs2['default'].createTimeRanges(ranges);\n      }\n    });\n  }\n\n  /**\n   * When we get a data event from the transmuxer\n   * we call this function and handle the data that\n   * was sent to us\n   *\n   * @private\n   * @param {Event} event the data event from the transmuxer\n   */\n\n  _createClass(VirtualSourceBuffer, [{\n    key: 'data_',\n    value: function data_(event) {\n      var segment = event.data.segment;\n\n      // Cast ArrayBuffer to TypedArray\n      segment.data = new Uint8Array(segment.data, event.data.byteOffset, event.data.byteLength);\n\n      segment.initSegment = new Uint8Array(segment.initSegment.data, segment.initSegment.byteOffset, segment.initSegment.byteLength);\n\n      (0, _createTextTracksIfNecessary2['default'])(this, this.mediaSource_, segment);\n\n      // Add the segments to the pendingBuffers array\n      this.pendingBuffers_.push(segment);\n      return;\n    }\n\n    /**\n     * When we get a done event from the transmuxer\n     * we call this function and we process all\n     * of the pending data that we have been saving in the\n     * data_ function\n     *\n     * @private\n     * @param {Event} event the done event from the transmuxer\n     */\n  }, {\n    key: 'done_',\n    value: function done_(event) {\n      // Don't process and append data if the mediaSource is closed\n      if (this.mediaSource_.readyState === 'closed') {\n        this.pendingBuffers_.length = 0;\n        return;\n      }\n\n      // All buffers should have been flushed from the muxer\n      // start processing anything we have received\n      this.processPendingSegments_();\n      return;\n    }\n\n    /**\n     * Create our internal native audio/video source buffers and add\n     * event handlers to them with the following conditions:\n     * 1. they do not already exist on the mediaSource\n     * 2. this VSB has a codec for them\n     *\n     * @private\n     */\n  }, {\n    key: 'createRealSourceBuffers_',\n    value: function createRealSourceBuffers_() {\n      var _this2 = this;\n\n      var types = ['audio', 'video'];\n\n      types.forEach(function (type) {\n        // Don't create a SourceBuffer of this type if we don't have a\n        // codec for it\n        if (!_this2[type + 'Codec_']) {\n          return;\n        }\n\n        // Do nothing if a SourceBuffer of this type already exists\n        if (_this2[type + 'Buffer_']) {\n          return;\n        }\n\n        var buffer = null;\n\n        // If the mediasource already has a SourceBuffer for the codec\n        // use that\n        if (_this2.mediaSource_[type + 'Buffer_']) {\n          buffer = _this2.mediaSource_[type + 'Buffer_'];\n          // In multiple audio track cases, the audio source buffer is disabled\n          // on the main VirtualSourceBuffer by the HTMLMediaSource much earlier\n          // than createRealSourceBuffers_ is called to create the second\n          // VirtualSourceBuffer because that happens as a side-effect of\n          // videojs-contrib-hls starting the audioSegmentLoader. As a result,\n          // the audioBuffer is essentially \"ownerless\" and no one will toggle\n          // the `updating` state back to false once the `updateend` event is received\n          //\n          // Setting `updating` to false manually will work around this\n          // situation and allow work to continue\n          buffer.updating = false;\n        } else {\n          var codecProperty = type + 'Codec_';\n          var mimeType = type + '/mp4;codecs=\"' + _this2[codecProperty] + '\"';\n\n          buffer = makeWrappedSourceBuffer(_this2.mediaSource_.nativeMediaSource_, mimeType);\n\n          _this2.mediaSource_[type + 'Buffer_'] = buffer;\n        }\n\n        _this2[type + 'Buffer_'] = buffer;\n\n        // Wire up the events to the SourceBuffer\n        ['update', 'updatestart', 'updateend'].forEach(function (event) {\n          buffer.addEventListener(event, function () {\n            // if audio is disabled\n            if (type === 'audio' && _this2.audioDisabled_) {\n              return;\n            }\n\n            if (event === 'updateend') {\n              _this2[type + 'Buffer_'].updating = false;\n            }\n\n            var shouldTrigger = types.every(function (t) {\n              // skip checking audio's updating status if audio\n              // is not enabled\n              if (t === 'audio' && _this2.audioDisabled_) {\n                return true;\n              }\n              // if the other type if updating we don't trigger\n              if (type !== t && _this2[t + 'Buffer_'] && _this2[t + 'Buffer_'].updating) {\n                return false;\n              }\n              return true;\n            });\n\n            if (shouldTrigger) {\n              return _this2.trigger(event);\n            }\n          });\n        });\n      });\n    }\n\n    /**\n     * Emulate the native mediasource function, but our function will\n     * send all of the proposed segments to the transmuxer so that we\n     * can transmux them before we append them to our internal\n     * native source buffers in the correct format.\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/SourceBuffer/appendBuffer\n     * @param {Uint8Array} segment the segment to append to the buffer\n     */\n  }, {\n    key: 'appendBuffer',\n    value: function appendBuffer(segment) {\n      // Start the internal \"updating\" state\n      this.bufferUpdating_ = true;\n\n      if (this.audioBuffer_ && this.audioBuffer_.buffered.length) {\n        var audioBuffered = this.audioBuffer_.buffered;\n\n        this.transmuxer_.postMessage({\n          action: 'setAudioAppendStart',\n          appendStart: audioBuffered.end(audioBuffered.length - 1)\n        });\n      }\n\n      if (this.videoBuffer_) {\n        this.transmuxer_.postMessage({\n          action: 'alignGopsWith',\n          gopsToAlignWith: gopsSafeToAlignWith(this.gopBuffer_, this.mediaSource_.player_, this.timeMapping_)\n        });\n      }\n\n      this.transmuxer_.postMessage({\n        action: 'push',\n        // Send the typed-array of data as an ArrayBuffer so that\n        // it can be sent as a \"Transferable\" and avoid the costly\n        // memory copy\n        data: segment.buffer,\n\n        // To recreate the original typed-array, we need information\n        // about what portion of the ArrayBuffer it was a view into\n        byteOffset: segment.byteOffset,\n        byteLength: segment.byteLength\n      }, [segment.buffer]);\n      this.transmuxer_.postMessage({ action: 'flush' });\n    }\n\n    /**\n     * Appends gop information (timing and byteLength) received by the transmuxer for the\n     * gops appended in the last call to appendBuffer\n     *\n     * @param {Event} event\n     *        The gopInfo event from the transmuxer\n     * @param {Array} event.data.gopInfo\n     *        List of gop info to append\n     */\n  }, {\n    key: 'appendGopInfo_',\n    value: function appendGopInfo_(event) {\n      this.gopBuffer_ = updateGopBuffer(this.gopBuffer_, event.data.gopInfo, this.safeAppend_);\n    }\n\n    /**\n     * Emulate the native mediasource function and remove parts\n     * of the buffer from any of our internal buffers that exist\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/SourceBuffer/remove\n     * @param {Double} start position to start the remove at\n     * @param {Double} end position to end the remove at\n     */\n  }, {\n    key: 'remove',\n    value: function remove(start, end) {\n      if (this.videoBuffer_) {\n        this.videoBuffer_.updating = true;\n        this.videoBuffer_.remove(start, end);\n        this.gopBuffer_ = removeGopBuffer(this.gopBuffer_, start, end, this.timeMapping_);\n      }\n      if (!this.audioDisabled_ && this.audioBuffer_) {\n        this.audioBuffer_.updating = true;\n        this.audioBuffer_.remove(start, end);\n      }\n\n      // Remove Metadata Cues (id3)\n      (0, _removeCuesFromTrack2['default'])(start, end, this.metadataTrack_);\n\n      // Remove Any Captions\n      if (this.inbandTextTracks_) {\n        for (var track in this.inbandTextTracks_) {\n          (0, _removeCuesFromTrack2['default'])(start, end, this.inbandTextTracks_[track]);\n        }\n      }\n    }\n\n    /**\n     * Process any segments that the muxer has output\n     * Concatenate segments together based on type and append them into\n     * their respective sourceBuffers\n     *\n     * @private\n     */\n  }, {\n    key: 'processPendingSegments_',\n    value: function processPendingSegments_() {\n      var sortedSegments = {\n        video: {\n          segments: [],\n          bytes: 0\n        },\n        audio: {\n          segments: [],\n          bytes: 0\n        },\n        captions: [],\n        metadata: []\n      };\n\n      // Sort segments into separate video/audio arrays and\n      // keep track of their total byte lengths\n      sortedSegments = this.pendingBuffers_.reduce(function (segmentObj, segment) {\n        var type = segment.type;\n        var data = segment.data;\n        var initSegment = segment.initSegment;\n\n        segmentObj[type].segments.push(data);\n        segmentObj[type].bytes += data.byteLength;\n\n        segmentObj[type].initSegment = initSegment;\n\n        // Gather any captions into a single array\n        if (segment.captions) {\n          segmentObj.captions = segmentObj.captions.concat(segment.captions);\n        }\n\n        if (segment.info) {\n          segmentObj[type].info = segment.info;\n        }\n\n        // Gather any metadata into a single array\n        if (segment.metadata) {\n          segmentObj.metadata = segmentObj.metadata.concat(segment.metadata);\n        }\n\n        return segmentObj;\n      }, sortedSegments);\n\n      // Create the real source buffers if they don't exist by now since we\n      // finally are sure what tracks are contained in the source\n      if (!this.videoBuffer_ && !this.audioBuffer_) {\n        // Remove any codecs that may have been specified by default but\n        // are no longer applicable now\n        if (sortedSegments.video.bytes === 0) {\n          this.videoCodec_ = null;\n        }\n        if (sortedSegments.audio.bytes === 0) {\n          this.audioCodec_ = null;\n        }\n\n        this.createRealSourceBuffers_();\n      }\n\n      if (sortedSegments.audio.info) {\n        this.mediaSource_.trigger({ type: 'audioinfo', info: sortedSegments.audio.info });\n      }\n      if (sortedSegments.video.info) {\n        this.mediaSource_.trigger({ type: 'videoinfo', info: sortedSegments.video.info });\n      }\n\n      if (this.appendAudioInitSegment_) {\n        if (!this.audioDisabled_ && this.audioBuffer_) {\n          sortedSegments.audio.segments.unshift(sortedSegments.audio.initSegment);\n          sortedSegments.audio.bytes += sortedSegments.audio.initSegment.byteLength;\n        }\n        this.appendAudioInitSegment_ = false;\n      }\n\n      var triggerUpdateend = false;\n\n      // Merge multiple video and audio segments into one and append\n      if (this.videoBuffer_ && sortedSegments.video.bytes) {\n        sortedSegments.video.segments.unshift(sortedSegments.video.initSegment);\n        sortedSegments.video.bytes += sortedSegments.video.initSegment.byteLength;\n        this.concatAndAppendSegments_(sortedSegments.video, this.videoBuffer_);\n        // TODO: are video tracks the only ones with text tracks?\n        (0, _addTextTrackData.addTextTrackData)(this, sortedSegments.captions, sortedSegments.metadata);\n      } else if (this.videoBuffer_ && (this.audioDisabled_ || !this.audioBuffer_)) {\n        // The transmuxer did not return any bytes of video, meaning it was all trimmed\n        // for gop alignment. Since we have a video buffer and audio is disabled, updateend\n        // will never be triggered by this source buffer, which will cause contrib-hls\n        // to be stuck forever waiting for updateend. If audio is not disabled, updateend\n        // will be triggered by the audio buffer, which will be sent upwards since the video\n        // buffer will not be in an updating state.\n        triggerUpdateend = true;\n      }\n\n      if (!this.audioDisabled_ && this.audioBuffer_) {\n        this.concatAndAppendSegments_(sortedSegments.audio, this.audioBuffer_);\n      }\n\n      this.pendingBuffers_.length = 0;\n\n      if (triggerUpdateend) {\n        this.trigger('updateend');\n      }\n\n      // We are no longer in the internal \"updating\" state\n      this.bufferUpdating_ = false;\n    }\n\n    /**\n     * Combine all segments into a single Uint8Array and then append them\n     * to the destination buffer\n     *\n     * @param {Object} segmentObj\n     * @param {SourceBuffer} destinationBuffer native source buffer to append data to\n     * @private\n     */\n  }, {\n    key: 'concatAndAppendSegments_',\n    value: function concatAndAppendSegments_(segmentObj, destinationBuffer) {\n      var offset = 0;\n      var tempBuffer = undefined;\n\n      if (segmentObj.bytes) {\n        tempBuffer = new Uint8Array(segmentObj.bytes);\n\n        // Combine the individual segments into one large typed-array\n        segmentObj.segments.forEach(function (segment) {\n          tempBuffer.set(segment, offset);\n          offset += segment.byteLength;\n        });\n\n        try {\n          destinationBuffer.updating = true;\n          destinationBuffer.appendBuffer(tempBuffer);\n        } catch (error) {\n          if (this.mediaSource_.player_) {\n            this.mediaSource_.player_.error({\n              code: -3,\n              type: 'APPEND_BUFFER_ERR',\n              message: error.message,\n              originalError: error\n            });\n          }\n        }\n      }\n    }\n\n    /**\n     * Emulate the native mediasource function. abort any soureBuffer\n     * actions and throw out any un-appended data.\n     *\n     * @link https://developer.mozilla.org/en-US/docs/Web/API/SourceBuffer/abort\n     */\n  }, {\n    key: 'abort',\n    value: function abort() {\n      if (this.videoBuffer_) {\n        this.videoBuffer_.abort();\n      }\n      if (!this.audioDisabled_ && this.audioBuffer_) {\n        this.audioBuffer_.abort();\n      }\n      if (this.transmuxer_) {\n        this.transmuxer_.postMessage({ action: 'reset' });\n      }\n      this.pendingBuffers_.length = 0;\n      this.bufferUpdating_ = false;\n    }\n  }]);\n\n  return VirtualSourceBuffer;\n})(_videoJs2['default'].EventTarget);\n\nexports['default'] = VirtualSourceBuffer;\n\n//# sourceURL=webpack:///./node_modules/videojs-contrib-media-sources/es5/virtual-source-buffer.js?");

/***/ }),

/***/ "./node_modules/vue-style-loader/index.js?!./node_modules/css-loader/dist/cjs.js?!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src/index.js?!./node_modules/sass-loader/dist/cjs.js?!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true&":
/*!**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************!*\
  !*** ./node_modules/vue-style-loader??ref--8-oneOf-1-0!./node_modules/css-loader/dist/cjs.js??ref--8-oneOf-1-1!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src??ref--8-oneOf-1-2!./node_modules/sass-loader/dist/cjs.js??ref--8-oneOf-1-3!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options!./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true& ***!
  \**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("// style-loader: Adds some css to the DOM by adding a <style> tag\n\n// load the styles\nvar content = __webpack_require__(/*! !../../node_modules/css-loader/dist/cjs.js??ref--8-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src??ref--8-oneOf-1-2!../../node_modules/sass-loader/dist/cjs.js??ref--8-oneOf-1-3!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib??vue-loader-options!./DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true& */ \"./node_modules/css-loader/dist/cjs.js?!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src/index.js?!./node_modules/sass-loader/dist/cjs.js?!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true&\");\nif(typeof content === 'string') content = [[module.i, content, '']];\nif(content.locals) module.exports = content.locals;\n// add the styles to the DOM\nvar add = __webpack_require__(/*! ../../node_modules/vue-style-loader/lib/addStylesClient.js */ \"./node_modules/vue-style-loader/lib/addStylesClient.js\").default\nvar update = add(\"2b0cbddc\", content, false, {\"sourceMap\":false,\"shadowMode\":false});\n// Hot Module Replacement\nif(false) {}\n\n//# sourceURL=webpack:///./src/views/DKZb.vue?./node_modules/vue-style-loader??ref--8-oneOf-1-0!./node_modules/css-loader/dist/cjs.js??ref--8-oneOf-1-1!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src??ref--8-oneOf-1-2!./node_modules/sass-loader/dist/cjs.js??ref--8-oneOf-1-3!./node_modules/cache-loader/dist/cjs.js??ref--0-0!./node_modules/vue-loader/lib??vue-loader-options");

/***/ }),

/***/ "./node_modules/webwackify/index.js":
/*!******************************************!*\
  !*** ./node_modules/webwackify/index.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("// By default assume browserify was used to bundle app. These arguments are passed to\n// the module by browserify.\nvar bundleFn = arguments[3];\nvar sources = arguments[4];\nvar cache = arguments[5];\nvar stringify = JSON.stringify;\nvar webpack = false;\n\n// webpackBootstrap\nvar webpackBootstrapFn = function(modules) {\n  // The module cache\n  var installedModules = {};\n\n  // The require function\n  function __webpack_require__(moduleId) {\n\n    // Check if module is in cache\n    if(installedModules[moduleId]) {\n      return installedModules[moduleId].exports;\n    }\n    // Create a new module (and put it into the cache)\n    var module = installedModules[moduleId] = {\n      i: moduleId,\n      l: false,\n      exports: {}\n    };\n\n    // Execute the module function\n    modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n    // Flag the module as loaded\n    module.l = true;\n\n    // Return the exports of the module\n    return module.exports;\n  }\n\n\n  // expose the modules object (__webpack_modules__)\n  __webpack_require__.m = modules;\n\n  // expose the module cache\n  __webpack_require__.c = installedModules;\n\n  // define getter function for harmony exports\n  __webpack_require__.d = function(exports, name, getter) {\n    if(!__webpack_require__.o(exports, name)) {\n      Object.defineProperty(exports, name, {\n        configurable: false,\n        enumerable: true,\n        get: getter\n      });\n    }\n  };\n\n  // getDefaultExport function for compatibility with non-harmony modules\n  __webpack_require__.n = function(module) {\n    var getter = module && module.__esModule ?\n      function getDefault() { return module['default']; } :\n      function getModuleExports() { return module; };\n\n    __webpack_require__.d(getter, 'a', getter);\n    return getter;\n  };\n\n  // Object.prototype.hasOwnProperty.call\n  __webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n  // __webpack_public_path__\n  __webpack_require__.p = \"\";\n\n  // Load entry module and return exports\n  return __webpack_require__(__webpack_require__.s = entryModule);\n}\n\nif (typeof bundleFn === 'undefined') {\n  // Assume this was bundled with webpack and not browserify\n  webpack = true;\n  bundleFn = webpackBootstrapFn;\n  sources = __webpack_require__.m;\n}\n\nvar bundleWithBrowserify = function(fn) {\n  // with browserify we must find the module key ourselves\n  var cacheKeys = Object.keys(cache);\n  var fnModuleKey;\n\n  for (var i = 0; i < cacheKeys.length; i++) {\n    var cacheKey = cacheKeys[i];\n    var cacheExports = cache[cacheKey].exports;\n\n    // Using babel as a transpiler to use esmodule, the export will always\n    // be an object with the default export as a property of it. To ensure\n    // the existing api and babel esmodule exports are both supported we\n    // check for both\n    if (cacheExports === fn || cacheExports && cacheExports.default === fn) {\n        fnModuleKey = cacheKey;\n        break;\n    }\n  }\n\n  // if we couldn't find one, lets make one\n  if (!fnModuleKey) {\n    fnModuleKey = Math.floor(Math.pow(16, 8) * Math.random()).toString(16);\n\n    var fnModuleCache = {};\n\n    for (var i = 0; i < cacheKeys.length; i++) {\n      var cacheKey = cacheKeys[i];\n\n      fnModuleCache[cacheKey] = cacheKey;\n    }\n\n    sources[fnModuleKey] = [\n      'function(require,module,exports){' + fn + '(self); }',\n      fnModuleCache\n    ];\n  }\n\n  var entryKey = Math.floor(Math.pow(16, 8) * Math.random()).toString(16);\n  var entryCache = {};\n\n  entryCache[fnModuleKey] = fnModuleKey;\n  sources[entryKey] = [\n    'function(require,module,exports){' +\n    // try to call default if defined to also support babel esmodule exports\n      'var f = require(' + stringify(fnModuleKey) + ');' +\n      '(f.default ? f.default : f)(self);' +\n    '}',\n    entryCache\n  ];\n\n  return '(' + bundleFn + ')({'\n        + Object.keys(sources).map(function(key) {\n            return stringify(key) + ':['\n                + sources[key][0] + ','\n                + stringify(sources[key][1]) + ']';\n        }).join(',')\n        + '},{},[' + stringify(entryKey) + '])';\n};\n\nvar bundleWithWebpack = function(fn, fnModuleId) {\n  var devMode = typeof fnModuleId === 'string';\n  var sourceStrings;\n\n  if (devMode) {\n    sourceStrings = {};\n  } else {\n    sourceStrings = [];\n  }\n\n  Object.keys(sources).forEach(function(sKey) {\n    if (!sources[sKey]) {\n      return;\n    }\n    sourceStrings[sKey] = sources[sKey].toString();\n  });\n\n  var fnModuleExports = __webpack_require__(fnModuleId);\n\n  // Using babel as a transpiler to use esmodule, the export will always\n  // be an object with the default export as a property of it. To ensure\n  // the existing api and babel esmodule exports are both supported we\n  // check for both\n  if (!(fnModuleExports && (fnModuleExports === fn || fnModuleExports.default === fn))) {\n    var fnSourceString = sourceStrings[fnModuleId];\n\n    sourceStrings[fnModuleId] = fnSourceString.substring(0, fnSourceString.length - 1) +\n                                '\\n' + fn.name + '();\\n}';\n  }\n\n  var modulesString;\n\n  if (devMode) {\n    // must escape quotes to support webpack loader options\n    fnModuleId = stringify(fnModuleId);\n    // dev mode in webpack4, modules are passed as an object\n    var mappedSourceStrings = Object.keys(sourceStrings).map(function(sKey) {\n      return stringify(sKey) + ':' + sourceStrings[sKey];\n    });\n\n    modulesString = '{' + mappedSourceStrings.join(',') + '}';\n  } else {\n    modulesString = '[' + sourceStrings.join(',') + ']';\n  }\n\n  return 'var fn = (' + bundleFn.toString().replace('entryModule', fnModuleId) + ')('\n        + modulesString\n        + ');\\n'\n        // not a function when calling a function from the current scope\n        + '(typeof fn === \"function\") && fn(self);';\n\n};\n\nmodule.exports = function webwackify(fn, fnModuleId) {\n  var src;\n\n  if (webpack) {\n    src = bundleWithWebpack(fn, fnModuleId);\n  } else {\n    src = bundleWithBrowserify(fn);\n  }\n\n  var blob = new Blob([src], { type: 'text/javascript' });\n  var URL = window.URL || window.webkitURL || window.mozURL || window.msURL;\n  var workerUrl = URL.createObjectURL(blob);\n  var worker = new Worker(workerUrl);\n  worker.objectURL = workerUrl;\n  return worker;\n};\n\n\n//# sourceURL=webpack:///./node_modules/webwackify/index.js?");

/***/ }),

/***/ "./src/views/DKZb.vue":
/*!****************************!*\
  !*** ./src/views/DKZb.vue ***!
  \****************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _DKZb_vue_vue_type_template_id_5211bda9_scoped_true___WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./DKZb.vue?vue&type=template&id=5211bda9&scoped=true& */ \"./src/views/DKZb.vue?vue&type=template&id=5211bda9&scoped=true&\");\n/* harmony import */ var _DKZb_vue_vue_type_script_lang_js___WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./DKZb.vue?vue&type=script&lang=js& */ \"./src/views/DKZb.vue?vue&type=script&lang=js&\");\n/* empty/unused harmony star reexport *//* harmony import */ var _DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true& */ \"./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true&\");\n/* harmony import */ var _node_modules_vue_loader_lib_runtime_componentNormalizer_js__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../node_modules/vue-loader/lib/runtime/componentNormalizer.js */ \"./node_modules/vue-loader/lib/runtime/componentNormalizer.js\");\n\n\n\n\n\n\n/* normalize component */\n\nvar component = Object(_node_modules_vue_loader_lib_runtime_componentNormalizer_js__WEBPACK_IMPORTED_MODULE_3__[\"default\"])(\n  _DKZb_vue_vue_type_script_lang_js___WEBPACK_IMPORTED_MODULE_1__[\"default\"],\n  _DKZb_vue_vue_type_template_id_5211bda9_scoped_true___WEBPACK_IMPORTED_MODULE_0__[\"render\"],\n  _DKZb_vue_vue_type_template_id_5211bda9_scoped_true___WEBPACK_IMPORTED_MODULE_0__[\"staticRenderFns\"],\n  false,\n  null,\n  \"5211bda9\",\n  null\n  \n)\n\n/* hot reload */\nif (false) { var api; }\ncomponent.options.__file = \"src/views/DKZb.vue\"\n/* harmony default export */ __webpack_exports__[\"default\"] = (component.exports);\n\n//# sourceURL=webpack:///./src/views/DKZb.vue?");

/***/ }),

/***/ "./src/views/DKZb.vue?vue&type=script&lang=js&":
/*!*****************************************************!*\
  !*** ./src/views/DKZb.vue?vue&type=script&lang=js& ***!
  \*****************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _node_modules_cache_loader_dist_cjs_js_ref_12_0_node_modules_babel_loader_lib_index_js_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_script_lang_js___WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! -!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/babel-loader/lib!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib??vue-loader-options!./DKZb.vue?vue&type=script&lang=js& */ \"./node_modules/cache-loader/dist/cjs.js?!./node_modules/babel-loader/lib/index.js!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=script&lang=js&\");\n/* empty/unused harmony star reexport */ /* harmony default export */ __webpack_exports__[\"default\"] = (_node_modules_cache_loader_dist_cjs_js_ref_12_0_node_modules_babel_loader_lib_index_js_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_script_lang_js___WEBPACK_IMPORTED_MODULE_0__[\"default\"]); \n\n//# sourceURL=webpack:///./src/views/DKZb.vue?");

/***/ }),

/***/ "./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true&":
/*!**************************************************************************************!*\
  !*** ./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true& ***!
  \**************************************************************************************/
/*! no static exports found */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _node_modules_vue_style_loader_index_js_ref_8_oneOf_1_0_node_modules_css_loader_dist_cjs_js_ref_8_oneOf_1_1_node_modules_vue_loader_lib_loaders_stylePostLoader_js_node_modules_postcss_loader_src_index_js_ref_8_oneOf_1_2_node_modules_sass_loader_dist_cjs_js_ref_8_oneOf_1_3_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! -!../../node_modules/vue-style-loader??ref--8-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--8-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src??ref--8-oneOf-1-2!../../node_modules/sass-loader/dist/cjs.js??ref--8-oneOf-1-3!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib??vue-loader-options!./DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true& */ \"./node_modules/vue-style-loader/index.js?!./node_modules/css-loader/dist/cjs.js?!./node_modules/vue-loader/lib/loaders/stylePostLoader.js!./node_modules/postcss-loader/src/index.js?!./node_modules/sass-loader/dist/cjs.js?!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=style&index=0&id=5211bda9&lang=scss&scoped=true&\");\n/* harmony import */ var _node_modules_vue_style_loader_index_js_ref_8_oneOf_1_0_node_modules_css_loader_dist_cjs_js_ref_8_oneOf_1_1_node_modules_vue_loader_lib_loaders_stylePostLoader_js_node_modules_postcss_loader_src_index_js_ref_8_oneOf_1_2_node_modules_sass_loader_dist_cjs_js_ref_8_oneOf_1_3_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_node_modules_vue_style_loader_index_js_ref_8_oneOf_1_0_node_modules_css_loader_dist_cjs_js_ref_8_oneOf_1_1_node_modules_vue_loader_lib_loaders_stylePostLoader_js_node_modules_postcss_loader_src_index_js_ref_8_oneOf_1_2_node_modules_sass_loader_dist_cjs_js_ref_8_oneOf_1_3_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_0__);\n/* harmony reexport (unknown) */ for(var __WEBPACK_IMPORT_KEY__ in _node_modules_vue_style_loader_index_js_ref_8_oneOf_1_0_node_modules_css_loader_dist_cjs_js_ref_8_oneOf_1_1_node_modules_vue_loader_lib_loaders_stylePostLoader_js_node_modules_postcss_loader_src_index_js_ref_8_oneOf_1_2_node_modules_sass_loader_dist_cjs_js_ref_8_oneOf_1_3_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_0__) if(__WEBPACK_IMPORT_KEY__ !== 'default') (function(key) { __webpack_require__.d(__webpack_exports__, key, function() { return _node_modules_vue_style_loader_index_js_ref_8_oneOf_1_0_node_modules_css_loader_dist_cjs_js_ref_8_oneOf_1_1_node_modules_vue_loader_lib_loaders_stylePostLoader_js_node_modules_postcss_loader_src_index_js_ref_8_oneOf_1_2_node_modules_sass_loader_dist_cjs_js_ref_8_oneOf_1_3_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_0__[key]; }) }(__WEBPACK_IMPORT_KEY__));\n /* harmony default export */ __webpack_exports__[\"default\"] = (_node_modules_vue_style_loader_index_js_ref_8_oneOf_1_0_node_modules_css_loader_dist_cjs_js_ref_8_oneOf_1_1_node_modules_vue_loader_lib_loaders_stylePostLoader_js_node_modules_postcss_loader_src_index_js_ref_8_oneOf_1_2_node_modules_sass_loader_dist_cjs_js_ref_8_oneOf_1_3_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_style_index_0_id_5211bda9_lang_scss_scoped_true___WEBPACK_IMPORTED_MODULE_0___default.a); \n\n//# sourceURL=webpack:///./src/views/DKZb.vue?");

/***/ }),

/***/ "./src/views/DKZb.vue?vue&type=template&id=5211bda9&scoped=true&":
/*!***********************************************************************!*\
  !*** ./src/views/DKZb.vue?vue&type=template&id=5211bda9&scoped=true& ***!
  \***********************************************************************/
/*! exports provided: render, staticRenderFns */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var _node_modules_cache_loader_dist_cjs_js_cacheDirectory_node_modules_cache_vue_loader_cacheIdentifier_74741eaf_vue_loader_template_node_modules_vue_loader_lib_loaders_templateLoader_js_vue_loader_options_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_template_id_5211bda9_scoped_true___WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! -!../../node_modules/cache-loader/dist/cjs.js?{\"cacheDirectory\":\"node_modules/.cache/vue-loader\",\"cacheIdentifier\":\"74741eaf-vue-loader-template\"}!../../node_modules/vue-loader/lib/loaders/templateLoader.js??vue-loader-options!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib??vue-loader-options!./DKZb.vue?vue&type=template&id=5211bda9&scoped=true& */ \"./node_modules/cache-loader/dist/cjs.js?{\\\"cacheDirectory\\\":\\\"node_modules/.cache/vue-loader\\\",\\\"cacheIdentifier\\\":\\\"74741eaf-vue-loader-template\\\"}!./node_modules/vue-loader/lib/loaders/templateLoader.js?!./node_modules/cache-loader/dist/cjs.js?!./node_modules/vue-loader/lib/index.js?!./src/views/DKZb.vue?vue&type=template&id=5211bda9&scoped=true&\");\n/* harmony reexport (safe) */ __webpack_require__.d(__webpack_exports__, \"render\", function() { return _node_modules_cache_loader_dist_cjs_js_cacheDirectory_node_modules_cache_vue_loader_cacheIdentifier_74741eaf_vue_loader_template_node_modules_vue_loader_lib_loaders_templateLoader_js_vue_loader_options_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_template_id_5211bda9_scoped_true___WEBPACK_IMPORTED_MODULE_0__[\"render\"]; });\n\n/* harmony reexport (safe) */ __webpack_require__.d(__webpack_exports__, \"staticRenderFns\", function() { return _node_modules_cache_loader_dist_cjs_js_cacheDirectory_node_modules_cache_vue_loader_cacheIdentifier_74741eaf_vue_loader_template_node_modules_vue_loader_lib_loaders_templateLoader_js_vue_loader_options_node_modules_cache_loader_dist_cjs_js_ref_0_0_node_modules_vue_loader_lib_index_js_vue_loader_options_DKZb_vue_vue_type_template_id_5211bda9_scoped_true___WEBPACK_IMPORTED_MODULE_0__[\"staticRenderFns\"]; });\n\n\n\n//# sourceURL=webpack:///./src/views/DKZb.vue?");

/***/ })

}]);